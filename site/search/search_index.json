{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FFA Framework Wiki Welcome to the FFA Framework wiki! The FFA Framework is an open-source project for Flood Frequency Analysis (FFA) developed by researchers at the University of Calgary, Canada 1 . The framework is currently available as an R package and a command line interface (CLI). To get started with the R package, please read the documentation . To get started with the CLI, please read the installation instructions and the user manual . You may also be interested in our goals , roadmap , or release notes . Warning : This version of the FFA Framework is incomplete. The original version of this software was written in MATLAB and published as A practice-oriented framework for stationary and nonstationary flood frequency analysis (Vidrio-Sahag\u00fan et al. 2024). For a list of changes from the MATLAB version, see here . \u21a9","title":"Home"},{"location":"#ffa-framework-wiki","text":"Welcome to the FFA Framework wiki! The FFA Framework is an open-source project for Flood Frequency Analysis (FFA) developed by researchers at the University of Calgary, Canada 1 . The framework is currently available as an R package and a command line interface (CLI). To get started with the R package, please read the documentation . To get started with the CLI, please read the installation instructions and the user manual . You may also be interested in our goals , roadmap , or release notes . Warning : This version of the FFA Framework is incomplete. The original version of this software was written in MATLAB and published as A practice-oriented framework for stationary and nonstationary flood frequency analysis (Vidrio-Sahag\u00fan et al. 2024). For a list of changes from the MATLAB version, see here . \u21a9","title":"FFA Framework Wiki"},{"location":"changelog/","text":"Changelog v0.0.3 May 22nd, 2025 Return information about non-stationary structure(s) at the end of EDA Use the /data and /reports directories as defaults in config.yml Refactor code for batching EDA in stats.R , eda.R Implement support for PDF reports using a custom \\(\\LaTeX\\) template v0.0.2 May 21st, 2025 Host documentation at https://rileywheadon.github.io/ffa-framework/ Implement splitting (for change points) in stats.R and eda.R Run EDA on multiple files by setting csv_files to a list in config.yml v0.0.1 May 16th, 2025 Execute individual statistical tests using stats.R Run a suite of unit tests using tests.R Run the entire EDA pipeline (without data spliting) using eda.R","title":"Release Notes"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v003","text":"May 22nd, 2025 Return information about non-stationary structure(s) at the end of EDA Use the /data and /reports directories as defaults in config.yml Refactor code for batching EDA in stats.R , eda.R Implement support for PDF reports using a custom \\(\\LaTeX\\) template","title":"v0.0.3"},{"location":"changelog/#v002","text":"May 21st, 2025 Host documentation at https://rileywheadon.github.io/ffa-framework/ Implement splitting (for change points) in stats.R and eda.R Run EDA on multiple files by setting csv_files to a list in config.yml","title":"v0.0.2"},{"location":"changelog/#v001","text":"May 16th, 2025 Execute individual statistical tests using stats.R Run a suite of unit tests using tests.R Run the entire EDA pipeline (without data spliting) using eda.R","title":"v0.0.1"},{"location":"cli-installation/","text":"Installation Instructions The FFA framework is currently available as a collection of scripts for the R programming environment. Therefore, installation and usage of the framework will require modellers to be familiar with the shell on their respective operating system. Prerequisites Install Git from https://git-scm.com/downloads. Check that Git is installed by executing git --version in a shell. Install R from https://www.r-project.org/. Version 4.5.0 is recommended. Check that R is installed by executing R --version in a shell. Install the latest version of Pandoc from https://pandoc.org/. Check that Pandoc is installed by executing pandoc --version in a shell. A Note for Windows Users You may need to add Git, R and Pandoc to your path in order to run them from the command line. To do this, you will need to edit your system environment variables from the settings menu. The default paths for Git, R and Pandoc are: C:\\Program Files\\Git\\bin C:\\Program Files\\R\\R-4.5.0\\bin C:\\Program Files\\Pandoc Installing the FFA Framework Open a shell and navigate to the directory where you would like to install the framework. Clone the Github repository by copying the following command into a shell: git clone https://github.com/rileywheadon/ffa-framework.git Navigate to the source directory. This directory contains a renv.lock file. Open a command prompt with R . The renv package should install automatically. Load libraries from the renv.lock file by executing renv::restore() . Exit the command prompt with q() . The installation is complete. This guide has been tested on Linux, Windows, and MacOS. Please submit a Github Issue if you have any problems.","title":"Installation Instructions"},{"location":"cli-installation/#installation-instructions","text":"The FFA framework is currently available as a collection of scripts for the R programming environment. Therefore, installation and usage of the framework will require modellers to be familiar with the shell on their respective operating system.","title":"Installation Instructions"},{"location":"cli-installation/#prerequisites","text":"Install Git from https://git-scm.com/downloads. Check that Git is installed by executing git --version in a shell. Install R from https://www.r-project.org/. Version 4.5.0 is recommended. Check that R is installed by executing R --version in a shell. Install the latest version of Pandoc from https://pandoc.org/. Check that Pandoc is installed by executing pandoc --version in a shell.","title":"Prerequisites"},{"location":"cli-installation/#a-note-for-windows-users","text":"You may need to add Git, R and Pandoc to your path in order to run them from the command line. To do this, you will need to edit your system environment variables from the settings menu. The default paths for Git, R and Pandoc are: C:\\Program Files\\Git\\bin C:\\Program Files\\R\\R-4.5.0\\bin C:\\Program Files\\Pandoc","title":"A Note for Windows Users"},{"location":"cli-installation/#installing-the-ffa-framework","text":"Open a shell and navigate to the directory where you would like to install the framework. Clone the Github repository by copying the following command into a shell: git clone https://github.com/rileywheadon/ffa-framework.git Navigate to the source directory. This directory contains a renv.lock file. Open a command prompt with R . The renv package should install automatically. Load libraries from the renv.lock file by executing renv::restore() . Exit the command prompt with q() . The installation is complete. This guide has been tested on Linux, Windows, and MacOS. Please submit a Github Issue if you have any problems.","title":"Installing the FFA Framework"},{"location":"cli-manual/","text":"User Manual All configuration is done through YAML files. The default configuration file is source/config.yml , although it is possible to create additional configuration files and pass them to the scripts using the --config command line argument. Folder Configuration data_folder : This argument should be an absolute path to a directory containing the CSV files you would like to run the framework on. The framework may not work if you use a relative path . Alternatively, data_folder: null will use the default ffa-framework/data folder. csv_files : A list of 1 or more CSV files located in data_folder . Alternatively, csv_files: null will run the framework on every file in data_folder . report_folder : An absolute path to a directory which will be used to store EDA and FFA reports. The directory must exist. Again, the framework may not work if you use a relative path . Alternatively, report_folder: null will use the default ffa-framework/reports folder. Example Configuration (1) data_folder: \"~/Desktop/ffa-data\" csv_files: - \"dataset-1.csv\" - \"dataset-2.csv\" report_folder: \"~/Desktop/ffa-reports\" Example Configuration (2) data_folder: null csv_files: null report_folder: null Exploratory Data Analysis (EDA) For more information about the statistical tests used for EDA, see here . The configuration options for the EDA portion of the framework are as follows: mode : The mode in which to run EDA (complete framework only). Must be one of: \"preset\" : Specify split points ahead of time using the split_points option. \"automatic\" : Automatically determine split points using change point tests. \"manual\" : Manually confirm the split points identified using change point tests. split_points : Used with mode: \"preset\" to set split points ahead of time (as a list of years). Alternatively, split_points: null will disable splitting entirely. alpha : The confidence level used for statistical tests (must be between \\(0.01\\) and \\(0.10\\) ). bbmk_repetitions : The number of boostrap samples to use for the BB-MK Test . window_length : The size of the moving window used to compute the streamflow variance. window_step : The size of the steps used to compute the streamflow variance. show_trend : Whether or not to draw a trend line between data points ( true or false ). generate_report : Whether or not to generate a report ( true or false ). report_format : A list of report formats ( \"md_document\" , \"html_document\" , or \"pdf_document\" ). Example Configuration mode: \"manual\" split_points: null alpha: 0.05 bbmk_repetitions: 10000 window_length: 10 window_step: 5 show_trend: false generate_report: true report_format: \"html_document\" Example (Disable Splitting) mode: \"preset\" split_points: null Example (Preset Split Points) mode: \"preset\" split_points: - 1950 - 1980 Running the Complete EDA Framework The source/eda.R script is used to run the complete EDA framework. The optional command line argument --config can be used to specify a custom configuration file. If --config is not specified, the framework will use config.yml as a default. Example Usage (1) Rscript eda.R Example Usage (2) Rscript eda.R --config='custom-config.yml' Running Individual Statistical Tests The source/stats.R script is used to run individual statistical tests. --name (required) is used to configure the name of the statistical test. It must be one of \"bbmk\", \"kpss\", \"mk\", \"mks\", \"mwmk\", \"pettitt\", \"pp\", \"sens-mean\", \"sens-variance\", \"spearman\", and \"white\". --config (optional) can be used to specify a custom configuration file. Example Usage (1) Rscript stats.R --name='mks' Example Usage (2) Rscript stats.R --name='white' --config='custom-config.yml' Frequency Analysis TBD","title":"User Manual"},{"location":"cli-manual/#user-manual","text":"All configuration is done through YAML files. The default configuration file is source/config.yml , although it is possible to create additional configuration files and pass them to the scripts using the --config command line argument.","title":"User Manual"},{"location":"cli-manual/#folder-configuration","text":"data_folder : This argument should be an absolute path to a directory containing the CSV files you would like to run the framework on. The framework may not work if you use a relative path . Alternatively, data_folder: null will use the default ffa-framework/data folder. csv_files : A list of 1 or more CSV files located in data_folder . Alternatively, csv_files: null will run the framework on every file in data_folder . report_folder : An absolute path to a directory which will be used to store EDA and FFA reports. The directory must exist. Again, the framework may not work if you use a relative path . Alternatively, report_folder: null will use the default ffa-framework/reports folder. Example Configuration (1) data_folder: \"~/Desktop/ffa-data\" csv_files: - \"dataset-1.csv\" - \"dataset-2.csv\" report_folder: \"~/Desktop/ffa-reports\" Example Configuration (2) data_folder: null csv_files: null report_folder: null","title":"Folder Configuration"},{"location":"cli-manual/#exploratory-data-analysis-eda","text":"For more information about the statistical tests used for EDA, see here . The configuration options for the EDA portion of the framework are as follows: mode : The mode in which to run EDA (complete framework only). Must be one of: \"preset\" : Specify split points ahead of time using the split_points option. \"automatic\" : Automatically determine split points using change point tests. \"manual\" : Manually confirm the split points identified using change point tests. split_points : Used with mode: \"preset\" to set split points ahead of time (as a list of years). Alternatively, split_points: null will disable splitting entirely. alpha : The confidence level used for statistical tests (must be between \\(0.01\\) and \\(0.10\\) ). bbmk_repetitions : The number of boostrap samples to use for the BB-MK Test . window_length : The size of the moving window used to compute the streamflow variance. window_step : The size of the steps used to compute the streamflow variance. show_trend : Whether or not to draw a trend line between data points ( true or false ). generate_report : Whether or not to generate a report ( true or false ). report_format : A list of report formats ( \"md_document\" , \"html_document\" , or \"pdf_document\" ). Example Configuration mode: \"manual\" split_points: null alpha: 0.05 bbmk_repetitions: 10000 window_length: 10 window_step: 5 show_trend: false generate_report: true report_format: \"html_document\" Example (Disable Splitting) mode: \"preset\" split_points: null Example (Preset Split Points) mode: \"preset\" split_points: - 1950 - 1980","title":"Exploratory Data Analysis (EDA)"},{"location":"cli-manual/#running-the-complete-eda-framework","text":"The source/eda.R script is used to run the complete EDA framework. The optional command line argument --config can be used to specify a custom configuration file. If --config is not specified, the framework will use config.yml as a default. Example Usage (1) Rscript eda.R Example Usage (2) Rscript eda.R --config='custom-config.yml'","title":"Running the Complete EDA Framework"},{"location":"cli-manual/#running-individual-statistical-tests","text":"The source/stats.R script is used to run individual statistical tests. --name (required) is used to configure the name of the statistical test. It must be one of \"bbmk\", \"kpss\", \"mk\", \"mks\", \"mwmk\", \"pettitt\", \"pp\", \"sens-mean\", \"sens-variance\", \"spearman\", and \"white\". --config (optional) can be used to specify a custom configuration file. Example Usage (1) Rscript stats.R --name='mks' Example Usage (2) Rscript stats.R --name='white' --config='custom-config.yml'","title":"Running Individual Statistical Tests"},{"location":"cli-manual/#frequency-analysis","text":"TBD","title":"Frequency Analysis"},{"location":"eda/","text":"EDA Framework The exploratory data analysis (EDA) module of the flood frequency analysis (FFA) framework allows users to run statistical tests on annual maximum streamflow (AMS) data. These statistical tests have four purposes: Identify change points (\"jumps\" or \"kinks\") in the AMS data. Identify serial correlation in the AMS data. Identify trends in the mean value of the AMS data. Identify trends in the variability of the AMS data. A diagram showing the current EDA framework is shown below: List of Statistical Tests BB-MK Test The Block Bootstrap Mann-Kendall (BB-MK) Test is used to assess whether there is a statistically significant monotonic trend in a time series. Unlike the MK test, the BB-MK test is insensitive to autocorrelation. Null hypothesis: There is no monotonic trend. Alternative hypothesis: There is a monotonic upwards or downwards trend. To carry out the BB-MK test, we rely on the results of the MK test and Spearman test. Compute the MK test statistic. Find the least significant lag \\(k\\) using the Spearman test. Resample from the original time series in blocks of size \\(k+1\\) , without replacement. Estimate the MK test statistic for each bootstrapped sample. Derive the empirical distribution of the MK test statistic from the bootstrapped statistics. Estimate the significance of the observed test statistic using the empirical distribution. KPSS Test The KPSS Test is used to identify if an autoregressive time series has a unit root . Null hypothesis: The time series does not have a unit root and is trend-stationary . Alternative hypothesis: The time series has a unit root and is non-stationary . Precisely, the autoregressive time series shown below has unit root if \\(\\sigma_{v}^2 > 0\\) : \\[ \\begin{align} y_{t} &= \\mu_{t} + \\beta t + \\epsilon_{t} \\\\[5pt] \\mu_{t} &= \\mu_{t-1} + v_{t} \\\\[5pt] v_{t} &\\sim \\mathcal{N}(0, \\sigma_{v}^2) \\end{align} \\] Here is what each term in this formulation represents: \\(\\mu_{t}\\) is the drift , or the deviation of \\(y_{t}\\) from \\(0\\) . Under the null hypothesis, \\(\\mu_{t}\\) is constant (since \\(v_{t}\\) is constant). Under the alternative hypothesis, \\(\\mu_t\\) is a stochastic process with unit root. \\(\\beta t\\) is a linear trend , which represents deterministic non-stationarity (i.e. climate change). \\(\\epsilon_{t}\\) is stationary noise , corresponding to reversible fluctuations in \\(y_{t}\\) . In hydrology, \\(\\epsilon_{t}\\) represents fluctuations in streamflow due to random events (i.e. weather). \\(v_{t}\\) is random walk innovation , or irreversible fluctuations in \\(\\mu_{t}\\) . In hydrology, \\(v_{t}\\) could represent randomness in industrial activity causing climate change. To conduct the test, we fit a linear model to \\(y_{t}\\) and get the residuals \\(\\hat{r}_{t}\\) Then, we compute the cumulative partial-sum statistics \\(S_{k}\\) using the following formula: \\[ S_{k} = \\sum_{t=1}^{k} \\hat{r}_{t} \\] Under the null hypothesis, \\(S_{k}\\) will behave like a random walk with finite variance. If \\(y_{t}\\) has a unit root, then the sums will \"drift\" too much. Next, we estimate the long-run variance of the time series, accounting for autocovariance. To do this, we first compute the sample autocovariances \\(\\gamma_{j}\\) for up to \\(q\\) lags, where: \\[ q = \\left\\lfloor \\frac{3\\sqrt{n}}{13} \\right\\rfloor \\] The sample autocovariance \\(\\gamma_{j}\\) is a measure of the correlation between the time series \\(y_{t}\\) and the shifted time series \\(y_{t-j}\\) . Each sample autocovariance \\(\\gamma_{j}\\) for \\(j = 0, 1, \\dots, q\\) is computed as follows: \\[ \\hat{\\gamma}_{j} = \\frac{1}{n} \\sum_{t = j + 1}^{n} \\hat{r}_{t}\\hat{r}_{t-j} \\] Finally, we estimate the long-run variance \\(\\hat{\\lambda}^2\\) using a Newey-West style estimator. This estimator corrects for the additional variability in \\(\\epsilon_{t}\\) caused by autocorrelation and heteroskedasticity. \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_{0} + 2\\sum_{j=1}^{q} \\left(1 - \\frac{j}{q + 1} \\right) \\gamma_{j} \\] Then, we compute the test statistic \\(z_{K}\\) using the following formula: \\[ z_{K} = \\frac{1}{n^2\\hat{\\lambda }^2}\\sum_{k=1}^{n} S_{k}^2 \\] The test statistic \\(z_{K}\\) is not normally distributed. Instead, we compute the p-value by interpolating a table from Hobjin et al. (2004) . This table is shown below for various quantiles \\(q\\) . \\(q\\) 0.90 0.95 0.975 0.99 Statistic 0.119 0.146 0.176 0.216 Warning : The interpolation procedure discussed above only works for \\(0.01 < p < 0.10\\) . Therefore, p-values below \\(0.01\\) and above \\(0.10\\) will be truncated and it is required that \\(0.01 < \\alpha < 0.1\\) . Mann-Kendall Test The Mann-Kendall Test is used to assess whether there is a statistically significant monotonic trend in a time series. The test requires that when no trend is present, the data is independent and identically distributed. Null hypothesis: There is no monotonic trend. Alternative hypothesis: There is a monotonic upwards or downwards trend. Define \\(\\text{sign} (x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. The test statistic \\(S\\) is defined as follows: \\[ S = \\sum_{k-1}^{n-1} \\sum_{j - k + 1}^{n} \\text{sign} (y_{j} - y_{k}) \\] Next, we need to compute \\(\\text{Var}(S)\\) , which depends on the number of tied groups in the data. Let \\(g\\) be the number of tied groups and \\(t_{p}\\) be the number of observations in the \\(p\\) -th group. \\[\\text{Var}(S) = \\frac{1}{18} \\left[n(n-1)(2n + 1) - \\sum_{p-1}^{g} t_{p}(t_{p} - 1)(2t_{p} + 5) \\right]\\] Then, compute the MK test statistic, \\(Z_{MK}\\) , as follows: \\[ Z_{MK} = \\begin{cases} \\frac{S-1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S > 0 \\\\ 0 &\\text{if } S = 0 \\\\ \\frac{S+1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S < 0 \\end{cases} \\] For a two-sided test, we reject the null hypothesis if \\(|Z_{MK}| \\geq Z_{1 - (\\alpha/2) }\\) and conclude that there is a statistically significant monotonic trend in the data. For more information, see here . Mann-Kendall-Sneyers Test The Mann-Kendall-Sneyers (MKS) Test is used to identify the beginning of a trend in a time series: Null hypothesis: There are no change points in the time series. Alternative hypothesis: There are one or more change points in the time series. Define \\(\\mathbb{I}(y_{i} > y_{j})\\) to be \\(1\\) if \\(y_{i} > y_{j}\\) and \\(0\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , we compute the following test statistic: \\[ S_{t} = \\sum_{i=i}^{t} \\sum_{j=1}^{i-1} \\mathbb{I}(y_{i} > y_{j}) \\] Then, we compute the the progressive series \\(UF_{t}\\) : \\[ UF_{t} = \\frac{S_{t} - \\mathbb{E}[S_{t}]}{\\sqrt{\\text{Var}\\,(S_{t})}} \\] Next, we reverse the time series to create a new time series \\(y'\\) such that \\(y_{i}' = y_{n+1-i}\\) . Then, we use formula (1) to compute \\(S_{t}'\\) for \\(y'\\) and reverse \\(S_{t}'\\) to get \\(S_{t}''\\) . The regressive series \\(UB_{t}\\) is defined as follows: \\[ UB_{t} = \\frac{S_{t}'' - \\mathbb{E}[S_{t}'']}{\\sqrt{\\text{Var}\\,(S_{t}'')}} \\] For both the progressive and regressive series, the expectation and variance is as follows: \\[ \\mathbb{E}[S_{t}] = \\mathbb{E}[S_{t}''] = \\frac{t(t-1)}{4}, \\quad \\text{Var}(S_{t}) = \\text{Var}(S_{t}'') = \\frac{t(t-1)(2t+5)}{72} \\] Finally, we plot \\(UF_{t}\\) and \\(UB_{t}\\) with confidence bounds at \\(\\pm z_{1 - (\\alpha/2)}\\) , where \\(\\alpha\\) is the chosen significance level. A crossing point between \\(UF_{t}\\) and \\(UB_{t}\\) that lies outside the confidence bounds indicates the start of the trend. MW-MK Test The Moving Window Mann-Kendall (MW-MK) Test is used to identify a statistically significant monotonic trend in the variances of an AMS time series. Null hypothesis: There is no significant trend in the variance of the AMS. Alternative hypothesis: There is a significant trend in the variance of the AMS. To compute the AMS variances we use a moving window: Set the length of the moving window \\(w\\) and the step size \\(s\\) . Compute the standard deviation over indices \\([1, w]\\) . Move the window forward by \\(s\\) . Check if the right boundary is beyond the end of the data. If not, go to (5). Compute the standard deviation again. Return to (3). Then, we perform the Mann-Kendall Test on the time series of variances. For more information about the Mann-Kendall test, see here . Pettitt Test The Pettitt Test is used to identify abrupt changes in the mean of a time series. Null hypothesis: There are no abrupt changes in the time series mean. Alternative hypothesis: There is one abrupt change in the time series mean. Define \\(\\text{sign}(x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , compute the following test statistic: \\[ U_{t} = \\sum_{i=1}^{t} \\sum_{j=t+1}^{n} \\text{sign} (y_{j} - y_{i}), \\quad K = \\max_{t}|U_{t}| \\] The value of \\(t\\) such that \\(U_{t} = K\\) is a potential change point . The p-value of the potential change point can be approximated using the following formula for a one-sided test: \\[ p \\approx \\exp \\left(-\\frac{6K^2}{n^3 + n^2}\\right) \\] If the p-value is less than the significance level \\(\\alpha\\) , we reject the null hypothesis and conclude that there is evidence for an abrupt change in the mean at the potential change point. Phillips-Perron Test The Phillips-Perron (PP) Test is used to identify if an autoregressive time series \\(y_t\\) has a unit root. Null hypothesis: \\(y_{t}\\) has a unit root and is thus non-stationary . Alternative hypothesis: \\(y_{t}\\) does not have a unit root and is trend-stationary . Precisely, let \\(x_{t}\\) be an AR(1) model. Let \\(y_{t}\\) be a function of \\(x_{t}\\) with drift \\(\\beta_{0}\\) and trend \\(\\beta_{1} t\\) . \\[ \\begin{align} y_{t} &= \\beta_{0} + \\beta_{1} t + x_{t} \\\\[5pt] x_{t} &= \\rho x_{t-1} + \\epsilon_{t} \\end{align} \\] If \\(\\rho = 1\\) , then \\(x_t\\) and hence \\(y_t\\) has a unit root (null hypothesis). If \\(\\rho < 1\\) , then \\(y_t\\) is trend stationary (alternative hypothesis). To perform the test, we begin by fitting an autoregressive linear model to \\(y_{t}\\) . Let \\(\\hat{r}_{t}\\) be the residuals of this model. From this model, we can determine \\(\\hat{\\rho}\\) (the estimated coefficient on \\(y_{t-1}\\) ) and \\(\\text{SE}(\\hat{\\rho})\\) (its standard error). Let \\(\\hat{\\sigma}^2\\) be the variance of the residuals, computed using the following formula: \\[ \\hat{\\sigma^2} = \\frac{1}{n - 3} \\sum_{t=1}^{n} \\hat{r}_{t}^2 \\] In the equation above, \\(n\\) is the number of data points in the sample. We have \\(n-3\\) degrees of freedom since there are three parameters in the autoregressive model ( \\(\\beta_{0}\\) , \\(\\beta_{1}\\) , and \\(\\rho\\) ). Next, we compute the sample autocovariances \\(\\gamma_{j}\\) for up to \\(q\\) lags, where: \\[ q = \\left\\lfloor \\sqrt[4]{\\frac{n}{25}}\\right\\rfloor \\] The sample autocovariance \\(\\gamma_{j}\\) is a measure of the correlation between the time series \\(y_{t}\\) and the shifted time series \\(y_{t-j}\\) . Each sample autocovariance \\(\\gamma_{j}\\) for \\(j = 0, 1, \\dots, q\\) is computed as follows: \\[ \\hat{\\gamma}_{j} = \\frac{1}{n} \\sum_{t = j + 1}^{n} \\hat{r}_{t}\\hat{r}_{t-j} \\] Finally, we estimate the long-run variance \\(\\hat{\\lambda}^2\\) using a Newey-West style estimator. This estimator corrects for the additional variability in \\(\\epsilon_{t}\\) caused by autocorrelation and heteroskedasticity. \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_{0} + 2\\sum_{j=1}^{q} \\left(1 - \\frac{j}{q + 1} \\right) \\gamma_{j} \\] Then, we compute the test statistic \\(z_{\\rho}\\) using the following formula: \\[ z_{\\rho } = n(\\hat{\\rho} - 1) - \\frac{n^2 \\text{SE}(\\hat{\\rho})^2}{2 \\hat{\\sigma}^2}(\\hat{\\lambda }^2 - \\hat{\\gamma}_{0}) \\] The test statistic \\(z_{\\rho}\\) is not normally distributed. Instead, we compute the p-value by interpolating a table from Fuller, W. A. (1996). This table is shown below for sample sizes \\(n\\) and quantiles \\(q\\) : \\(n\\) \\ \\(q\\) 0.01 0.025 0.05 0.10 0.50 0.90 0.95 0.975 0.99 25 -22.5 -20.0 -17.9 -15.6 -8.49 -3.65 -2.51 -1.53 -0.46 50 -25.8 -22.4 -19.7 -16.8 -8.80 -3.71 -2.60 -1.67 -0.67 100 -27.4 -23.7 -20.6 -17.5 -8.96 -3.74 -2.63 -1.74 -0.76 250 -28.5 -24.4 -21.3 -17.9 -9.05 -3.76 -2.65 -1.79 -0.83 500 -28.9 -24.7 -21.5 -18.1 -9.08 -3.76 -2.66 -1.80 -0.86 1000 -29.4 -25.0 -21.7 -18.3 -9.11 -3.77 -2.67 -1.81 -0.88 Warning : The interpolation procedure discussed above only works for \\(0.01 < p\\) . Therefore, p-values below \\(0.01\\) will be truncated and it is required that \\(0.01 < \\alpha\\) . Sen's Trend Estimator Sen's Trend Estimator is used to estimate the slope of a regression line. Unlike Least Squares , Sen's trend estimator uses a non-parametric approach which makes it robust to outliers. To compute Sen's trend estimator we use the following procedure: Iterate over all pairs of data points \\((x_{i}, y_{i})\\) and \\((x_{j}, y_{j})\\) . If \\(x_{i} \\neq x_{j}\\) , compute the slope \\((y_{j} - y_{i})/(x_{j} - x_{i})\\) and add it to a list \\(S\\) . Sen's trend estimator \\(\\hat{m}\\) is the median of \\(S\\) . After computing \\(\\hat{m}\\) , we can estimate the \\(y\\) -intercept \\(b\\) by the median of \\(y_{i} - \\hat{m}x_{i}\\) for all \\(i\\) . Runs Test After computing the regression line using Sen's trend estimator , we use the Runs Test to determine whether the residuals from the regression are random. If the Runs test identifies non-randomness in the residuals, it is a strong indication that the non-stationarity in the data is non-linear. Null hypothesis: The residuals are distributed randomly. Alternative hypothesis: The residuals are not distributed randomly. Prior to applying the Runs test, the data is categorized based on whether it is above or below the median. Then, we compute the number of contiguous blocks of \\(+\\) or \\(-\\) (or runs ) in the data. Example : Suppose that after categorization, the sequence of data is as follows: \\[ +++--+++-+- \\] This sequence has six runs with length \\((3, 2, 3, 1,1, 1)\\) . Let \\(R\\) be the number of runs in \\(N\\) data points (with category counts \\(N_{+}\\) and \\(N_{-}\\) ). Then, under the null hypothesis, \\(R\\) is asymptotically normal with: \\[ \\mathbb{E}[R] = \\frac{2N_{+}N_{-}}{N} + 1, \\quad \\text{Var}(R) = \\frac{2N_{+}N_{-}(2N_{+}N_{-} - N)}{N^2(N - 1)} \\] For more information, see the Wikipedia entry or the R Documentation . Spearman Test The Spearman Test is used to identify autocorrelation in a time series \\(y_{t}\\) . A significant lag is a number \\(i\\) such that the correlation between \\(y_{t}\\) and \\(y_{t-i}\\) is statistically significant. The least insignificant lag is the largest \\(i\\) such that all \\(j < i\\) are significant lags. Null hypothesis: The least insignificant lag is \\(0\\) . Alternative hypothesis: The least insignificant lag is greater than \\(0\\) . To carry out the Spearman test, we use the following procedure: Compute Spearman's correlation coefficient \\(\\rho_{i}\\) for \\(y_{t}\\) and \\(y_{t-i}\\) for all \\(0 \\leq i < n\\) . Determine the \\(p\\) -value \\(p_{i}\\) for each correlation coefficient \\(\\rho _{i}\\) . Iterate through \\(p_{i}\\) to find the largest \\(i\\) such that \\(p_{j} \\leq \\alpha\\) for all \\(j \\leq i\\) . The value of \\(i\\) found in (3) is the least insignificant lag at confidence level \\(\\alpha\\) . Remark : To compute the \\(p\\) -value of a correlation coefficient \\(\\rho _{i}\\) , first compute: \\[ t_{i}= \\rho_{i} \\sqrt{\\frac{n-2}{1 - \\rho _{i}^2}} \\] Then, the test statistic \\(t_{i}\\) has the \\(t\\) -distribution with \\(n-2\\) degrees of freedom. For more information, see the Wikipedia pages on Autocorrelation and Spearman's Rho . White Test The White Test is used to detect changes in the variance of a time series. Null hypothesis: The variance of the time series is constant (homoskedasticity). Alternative hypothesis: The variance of the time series is time-dependent (heteroskedasticity). Consider a simple linear regression model: \\[y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i}\\] Use ordinary least squares to fit the model. Then compute the squared residuals: \\[{\\hat{r}}_{i}^{2} = (y_{i} - \\hat{y}_{i})^{2}\\] Next, fit an auxillary regression model to the squared residuals. This model should include each regressor, the square of each regressor, and the cross products between all regressors. Since \\(x\\) is our only regressor, \\[{\\hat{r}}_{i}^{2} = \\alpha_{0} + \\alpha_{1}x_{i} + \\alpha_{2}x_{i}^{2} + u_{i}\\] Next, we compute the coefficient of determination \\(R^2\\) for the auxillary model. The test statistic is \\(nR^2 \\sim \\chi_{d}^2\\) , where \\(n\\) is the number of observations and \\(d = 2\\) is the number of regressors, excluding the intercept. If \\(nR^2 > \\chi^2_{1-\\alpha, d}\\) , we reject the null hypothesis and conclude that the time series exhibits heteroskedasticity. For more information, the following sources may be useful: White Test Deep Dive Marno Verbeek, A Guide to Modern Econometrics (2004) William H. Greene, Econometric Analysis, 5th Edition (2002)","title":"Exploratory Data Analysis"},{"location":"eda/#eda-framework","text":"The exploratory data analysis (EDA) module of the flood frequency analysis (FFA) framework allows users to run statistical tests on annual maximum streamflow (AMS) data. These statistical tests have four purposes: Identify change points (\"jumps\" or \"kinks\") in the AMS data. Identify serial correlation in the AMS data. Identify trends in the mean value of the AMS data. Identify trends in the variability of the AMS data. A diagram showing the current EDA framework is shown below:","title":"EDA Framework"},{"location":"eda/#list-of-statistical-tests","text":"","title":"List of Statistical Tests"},{"location":"eda/#bb-mk-test","text":"The Block Bootstrap Mann-Kendall (BB-MK) Test is used to assess whether there is a statistically significant monotonic trend in a time series. Unlike the MK test, the BB-MK test is insensitive to autocorrelation. Null hypothesis: There is no monotonic trend. Alternative hypothesis: There is a monotonic upwards or downwards trend. To carry out the BB-MK test, we rely on the results of the MK test and Spearman test. Compute the MK test statistic. Find the least significant lag \\(k\\) using the Spearman test. Resample from the original time series in blocks of size \\(k+1\\) , without replacement. Estimate the MK test statistic for each bootstrapped sample. Derive the empirical distribution of the MK test statistic from the bootstrapped statistics. Estimate the significance of the observed test statistic using the empirical distribution.","title":"BB-MK Test"},{"location":"eda/#kpss-test","text":"The KPSS Test is used to identify if an autoregressive time series has a unit root . Null hypothesis: The time series does not have a unit root and is trend-stationary . Alternative hypothesis: The time series has a unit root and is non-stationary . Precisely, the autoregressive time series shown below has unit root if \\(\\sigma_{v}^2 > 0\\) : \\[ \\begin{align} y_{t} &= \\mu_{t} + \\beta t + \\epsilon_{t} \\\\[5pt] \\mu_{t} &= \\mu_{t-1} + v_{t} \\\\[5pt] v_{t} &\\sim \\mathcal{N}(0, \\sigma_{v}^2) \\end{align} \\] Here is what each term in this formulation represents: \\(\\mu_{t}\\) is the drift , or the deviation of \\(y_{t}\\) from \\(0\\) . Under the null hypothesis, \\(\\mu_{t}\\) is constant (since \\(v_{t}\\) is constant). Under the alternative hypothesis, \\(\\mu_t\\) is a stochastic process with unit root. \\(\\beta t\\) is a linear trend , which represents deterministic non-stationarity (i.e. climate change). \\(\\epsilon_{t}\\) is stationary noise , corresponding to reversible fluctuations in \\(y_{t}\\) . In hydrology, \\(\\epsilon_{t}\\) represents fluctuations in streamflow due to random events (i.e. weather). \\(v_{t}\\) is random walk innovation , or irreversible fluctuations in \\(\\mu_{t}\\) . In hydrology, \\(v_{t}\\) could represent randomness in industrial activity causing climate change. To conduct the test, we fit a linear model to \\(y_{t}\\) and get the residuals \\(\\hat{r}_{t}\\) Then, we compute the cumulative partial-sum statistics \\(S_{k}\\) using the following formula: \\[ S_{k} = \\sum_{t=1}^{k} \\hat{r}_{t} \\] Under the null hypothesis, \\(S_{k}\\) will behave like a random walk with finite variance. If \\(y_{t}\\) has a unit root, then the sums will \"drift\" too much. Next, we estimate the long-run variance of the time series, accounting for autocovariance. To do this, we first compute the sample autocovariances \\(\\gamma_{j}\\) for up to \\(q\\) lags, where: \\[ q = \\left\\lfloor \\frac{3\\sqrt{n}}{13} \\right\\rfloor \\] The sample autocovariance \\(\\gamma_{j}\\) is a measure of the correlation between the time series \\(y_{t}\\) and the shifted time series \\(y_{t-j}\\) . Each sample autocovariance \\(\\gamma_{j}\\) for \\(j = 0, 1, \\dots, q\\) is computed as follows: \\[ \\hat{\\gamma}_{j} = \\frac{1}{n} \\sum_{t = j + 1}^{n} \\hat{r}_{t}\\hat{r}_{t-j} \\] Finally, we estimate the long-run variance \\(\\hat{\\lambda}^2\\) using a Newey-West style estimator. This estimator corrects for the additional variability in \\(\\epsilon_{t}\\) caused by autocorrelation and heteroskedasticity. \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_{0} + 2\\sum_{j=1}^{q} \\left(1 - \\frac{j}{q + 1} \\right) \\gamma_{j} \\] Then, we compute the test statistic \\(z_{K}\\) using the following formula: \\[ z_{K} = \\frac{1}{n^2\\hat{\\lambda }^2}\\sum_{k=1}^{n} S_{k}^2 \\] The test statistic \\(z_{K}\\) is not normally distributed. Instead, we compute the p-value by interpolating a table from Hobjin et al. (2004) . This table is shown below for various quantiles \\(q\\) . \\(q\\) 0.90 0.95 0.975 0.99 Statistic 0.119 0.146 0.176 0.216 Warning : The interpolation procedure discussed above only works for \\(0.01 < p < 0.10\\) . Therefore, p-values below \\(0.01\\) and above \\(0.10\\) will be truncated and it is required that \\(0.01 < \\alpha < 0.1\\) .","title":"KPSS Test"},{"location":"eda/#mann-kendall-test","text":"The Mann-Kendall Test is used to assess whether there is a statistically significant monotonic trend in a time series. The test requires that when no trend is present, the data is independent and identically distributed. Null hypothesis: There is no monotonic trend. Alternative hypothesis: There is a monotonic upwards or downwards trend. Define \\(\\text{sign} (x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. The test statistic \\(S\\) is defined as follows: \\[ S = \\sum_{k-1}^{n-1} \\sum_{j - k + 1}^{n} \\text{sign} (y_{j} - y_{k}) \\] Next, we need to compute \\(\\text{Var}(S)\\) , which depends on the number of tied groups in the data. Let \\(g\\) be the number of tied groups and \\(t_{p}\\) be the number of observations in the \\(p\\) -th group. \\[\\text{Var}(S) = \\frac{1}{18} \\left[n(n-1)(2n + 1) - \\sum_{p-1}^{g} t_{p}(t_{p} - 1)(2t_{p} + 5) \\right]\\] Then, compute the MK test statistic, \\(Z_{MK}\\) , as follows: \\[ Z_{MK} = \\begin{cases} \\frac{S-1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S > 0 \\\\ 0 &\\text{if } S = 0 \\\\ \\frac{S+1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S < 0 \\end{cases} \\] For a two-sided test, we reject the null hypothesis if \\(|Z_{MK}| \\geq Z_{1 - (\\alpha/2) }\\) and conclude that there is a statistically significant monotonic trend in the data. For more information, see here .","title":"Mann-Kendall Test"},{"location":"eda/#mann-kendall-sneyers-test","text":"The Mann-Kendall-Sneyers (MKS) Test is used to identify the beginning of a trend in a time series: Null hypothesis: There are no change points in the time series. Alternative hypothesis: There are one or more change points in the time series. Define \\(\\mathbb{I}(y_{i} > y_{j})\\) to be \\(1\\) if \\(y_{i} > y_{j}\\) and \\(0\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , we compute the following test statistic: \\[ S_{t} = \\sum_{i=i}^{t} \\sum_{j=1}^{i-1} \\mathbb{I}(y_{i} > y_{j}) \\] Then, we compute the the progressive series \\(UF_{t}\\) : \\[ UF_{t} = \\frac{S_{t} - \\mathbb{E}[S_{t}]}{\\sqrt{\\text{Var}\\,(S_{t})}} \\] Next, we reverse the time series to create a new time series \\(y'\\) such that \\(y_{i}' = y_{n+1-i}\\) . Then, we use formula (1) to compute \\(S_{t}'\\) for \\(y'\\) and reverse \\(S_{t}'\\) to get \\(S_{t}''\\) . The regressive series \\(UB_{t}\\) is defined as follows: \\[ UB_{t} = \\frac{S_{t}'' - \\mathbb{E}[S_{t}'']}{\\sqrt{\\text{Var}\\,(S_{t}'')}} \\] For both the progressive and regressive series, the expectation and variance is as follows: \\[ \\mathbb{E}[S_{t}] = \\mathbb{E}[S_{t}''] = \\frac{t(t-1)}{4}, \\quad \\text{Var}(S_{t}) = \\text{Var}(S_{t}'') = \\frac{t(t-1)(2t+5)}{72} \\] Finally, we plot \\(UF_{t}\\) and \\(UB_{t}\\) with confidence bounds at \\(\\pm z_{1 - (\\alpha/2)}\\) , where \\(\\alpha\\) is the chosen significance level. A crossing point between \\(UF_{t}\\) and \\(UB_{t}\\) that lies outside the confidence bounds indicates the start of the trend.","title":"Mann-Kendall-Sneyers Test"},{"location":"eda/#mw-mk-test","text":"The Moving Window Mann-Kendall (MW-MK) Test is used to identify a statistically significant monotonic trend in the variances of an AMS time series. Null hypothesis: There is no significant trend in the variance of the AMS. Alternative hypothesis: There is a significant trend in the variance of the AMS. To compute the AMS variances we use a moving window: Set the length of the moving window \\(w\\) and the step size \\(s\\) . Compute the standard deviation over indices \\([1, w]\\) . Move the window forward by \\(s\\) . Check if the right boundary is beyond the end of the data. If not, go to (5). Compute the standard deviation again. Return to (3). Then, we perform the Mann-Kendall Test on the time series of variances. For more information about the Mann-Kendall test, see here .","title":"MW-MK Test"},{"location":"eda/#pettitt-test","text":"The Pettitt Test is used to identify abrupt changes in the mean of a time series. Null hypothesis: There are no abrupt changes in the time series mean. Alternative hypothesis: There is one abrupt change in the time series mean. Define \\(\\text{sign}(x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , compute the following test statistic: \\[ U_{t} = \\sum_{i=1}^{t} \\sum_{j=t+1}^{n} \\text{sign} (y_{j} - y_{i}), \\quad K = \\max_{t}|U_{t}| \\] The value of \\(t\\) such that \\(U_{t} = K\\) is a potential change point . The p-value of the potential change point can be approximated using the following formula for a one-sided test: \\[ p \\approx \\exp \\left(-\\frac{6K^2}{n^3 + n^2}\\right) \\] If the p-value is less than the significance level \\(\\alpha\\) , we reject the null hypothesis and conclude that there is evidence for an abrupt change in the mean at the potential change point.","title":"Pettitt Test"},{"location":"eda/#phillips-perron-test","text":"The Phillips-Perron (PP) Test is used to identify if an autoregressive time series \\(y_t\\) has a unit root. Null hypothesis: \\(y_{t}\\) has a unit root and is thus non-stationary . Alternative hypothesis: \\(y_{t}\\) does not have a unit root and is trend-stationary . Precisely, let \\(x_{t}\\) be an AR(1) model. Let \\(y_{t}\\) be a function of \\(x_{t}\\) with drift \\(\\beta_{0}\\) and trend \\(\\beta_{1} t\\) . \\[ \\begin{align} y_{t} &= \\beta_{0} + \\beta_{1} t + x_{t} \\\\[5pt] x_{t} &= \\rho x_{t-1} + \\epsilon_{t} \\end{align} \\] If \\(\\rho = 1\\) , then \\(x_t\\) and hence \\(y_t\\) has a unit root (null hypothesis). If \\(\\rho < 1\\) , then \\(y_t\\) is trend stationary (alternative hypothesis). To perform the test, we begin by fitting an autoregressive linear model to \\(y_{t}\\) . Let \\(\\hat{r}_{t}\\) be the residuals of this model. From this model, we can determine \\(\\hat{\\rho}\\) (the estimated coefficient on \\(y_{t-1}\\) ) and \\(\\text{SE}(\\hat{\\rho})\\) (its standard error). Let \\(\\hat{\\sigma}^2\\) be the variance of the residuals, computed using the following formula: \\[ \\hat{\\sigma^2} = \\frac{1}{n - 3} \\sum_{t=1}^{n} \\hat{r}_{t}^2 \\] In the equation above, \\(n\\) is the number of data points in the sample. We have \\(n-3\\) degrees of freedom since there are three parameters in the autoregressive model ( \\(\\beta_{0}\\) , \\(\\beta_{1}\\) , and \\(\\rho\\) ). Next, we compute the sample autocovariances \\(\\gamma_{j}\\) for up to \\(q\\) lags, where: \\[ q = \\left\\lfloor \\sqrt[4]{\\frac{n}{25}}\\right\\rfloor \\] The sample autocovariance \\(\\gamma_{j}\\) is a measure of the correlation between the time series \\(y_{t}\\) and the shifted time series \\(y_{t-j}\\) . Each sample autocovariance \\(\\gamma_{j}\\) for \\(j = 0, 1, \\dots, q\\) is computed as follows: \\[ \\hat{\\gamma}_{j} = \\frac{1}{n} \\sum_{t = j + 1}^{n} \\hat{r}_{t}\\hat{r}_{t-j} \\] Finally, we estimate the long-run variance \\(\\hat{\\lambda}^2\\) using a Newey-West style estimator. This estimator corrects for the additional variability in \\(\\epsilon_{t}\\) caused by autocorrelation and heteroskedasticity. \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_{0} + 2\\sum_{j=1}^{q} \\left(1 - \\frac{j}{q + 1} \\right) \\gamma_{j} \\] Then, we compute the test statistic \\(z_{\\rho}\\) using the following formula: \\[ z_{\\rho } = n(\\hat{\\rho} - 1) - \\frac{n^2 \\text{SE}(\\hat{\\rho})^2}{2 \\hat{\\sigma}^2}(\\hat{\\lambda }^2 - \\hat{\\gamma}_{0}) \\] The test statistic \\(z_{\\rho}\\) is not normally distributed. Instead, we compute the p-value by interpolating a table from Fuller, W. A. (1996). This table is shown below for sample sizes \\(n\\) and quantiles \\(q\\) : \\(n\\) \\ \\(q\\) 0.01 0.025 0.05 0.10 0.50 0.90 0.95 0.975 0.99 25 -22.5 -20.0 -17.9 -15.6 -8.49 -3.65 -2.51 -1.53 -0.46 50 -25.8 -22.4 -19.7 -16.8 -8.80 -3.71 -2.60 -1.67 -0.67 100 -27.4 -23.7 -20.6 -17.5 -8.96 -3.74 -2.63 -1.74 -0.76 250 -28.5 -24.4 -21.3 -17.9 -9.05 -3.76 -2.65 -1.79 -0.83 500 -28.9 -24.7 -21.5 -18.1 -9.08 -3.76 -2.66 -1.80 -0.86 1000 -29.4 -25.0 -21.7 -18.3 -9.11 -3.77 -2.67 -1.81 -0.88 Warning : The interpolation procedure discussed above only works for \\(0.01 < p\\) . Therefore, p-values below \\(0.01\\) will be truncated and it is required that \\(0.01 < \\alpha\\) .","title":"Phillips-Perron Test"},{"location":"eda/#sens-trend-estimator","text":"Sen's Trend Estimator is used to estimate the slope of a regression line. Unlike Least Squares , Sen's trend estimator uses a non-parametric approach which makes it robust to outliers. To compute Sen's trend estimator we use the following procedure: Iterate over all pairs of data points \\((x_{i}, y_{i})\\) and \\((x_{j}, y_{j})\\) . If \\(x_{i} \\neq x_{j}\\) , compute the slope \\((y_{j} - y_{i})/(x_{j} - x_{i})\\) and add it to a list \\(S\\) . Sen's trend estimator \\(\\hat{m}\\) is the median of \\(S\\) . After computing \\(\\hat{m}\\) , we can estimate the \\(y\\) -intercept \\(b\\) by the median of \\(y_{i} - \\hat{m}x_{i}\\) for all \\(i\\) .","title":"Sen's Trend Estimator"},{"location":"eda/#runs-test","text":"After computing the regression line using Sen's trend estimator , we use the Runs Test to determine whether the residuals from the regression are random. If the Runs test identifies non-randomness in the residuals, it is a strong indication that the non-stationarity in the data is non-linear. Null hypothesis: The residuals are distributed randomly. Alternative hypothesis: The residuals are not distributed randomly. Prior to applying the Runs test, the data is categorized based on whether it is above or below the median. Then, we compute the number of contiguous blocks of \\(+\\) or \\(-\\) (or runs ) in the data. Example : Suppose that after categorization, the sequence of data is as follows: \\[ +++--+++-+- \\] This sequence has six runs with length \\((3, 2, 3, 1,1, 1)\\) . Let \\(R\\) be the number of runs in \\(N\\) data points (with category counts \\(N_{+}\\) and \\(N_{-}\\) ). Then, under the null hypothesis, \\(R\\) is asymptotically normal with: \\[ \\mathbb{E}[R] = \\frac{2N_{+}N_{-}}{N} + 1, \\quad \\text{Var}(R) = \\frac{2N_{+}N_{-}(2N_{+}N_{-} - N)}{N^2(N - 1)} \\] For more information, see the Wikipedia entry or the R Documentation .","title":"Runs Test"},{"location":"eda/#spearman-test","text":"The Spearman Test is used to identify autocorrelation in a time series \\(y_{t}\\) . A significant lag is a number \\(i\\) such that the correlation between \\(y_{t}\\) and \\(y_{t-i}\\) is statistically significant. The least insignificant lag is the largest \\(i\\) such that all \\(j < i\\) are significant lags. Null hypothesis: The least insignificant lag is \\(0\\) . Alternative hypothesis: The least insignificant lag is greater than \\(0\\) . To carry out the Spearman test, we use the following procedure: Compute Spearman's correlation coefficient \\(\\rho_{i}\\) for \\(y_{t}\\) and \\(y_{t-i}\\) for all \\(0 \\leq i < n\\) . Determine the \\(p\\) -value \\(p_{i}\\) for each correlation coefficient \\(\\rho _{i}\\) . Iterate through \\(p_{i}\\) to find the largest \\(i\\) such that \\(p_{j} \\leq \\alpha\\) for all \\(j \\leq i\\) . The value of \\(i\\) found in (3) is the least insignificant lag at confidence level \\(\\alpha\\) . Remark : To compute the \\(p\\) -value of a correlation coefficient \\(\\rho _{i}\\) , first compute: \\[ t_{i}= \\rho_{i} \\sqrt{\\frac{n-2}{1 - \\rho _{i}^2}} \\] Then, the test statistic \\(t_{i}\\) has the \\(t\\) -distribution with \\(n-2\\) degrees of freedom. For more information, see the Wikipedia pages on Autocorrelation and Spearman's Rho .","title":"Spearman Test"},{"location":"eda/#white-test","text":"The White Test is used to detect changes in the variance of a time series. Null hypothesis: The variance of the time series is constant (homoskedasticity). Alternative hypothesis: The variance of the time series is time-dependent (heteroskedasticity). Consider a simple linear regression model: \\[y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i}\\] Use ordinary least squares to fit the model. Then compute the squared residuals: \\[{\\hat{r}}_{i}^{2} = (y_{i} - \\hat{y}_{i})^{2}\\] Next, fit an auxillary regression model to the squared residuals. This model should include each regressor, the square of each regressor, and the cross products between all regressors. Since \\(x\\) is our only regressor, \\[{\\hat{r}}_{i}^{2} = \\alpha_{0} + \\alpha_{1}x_{i} + \\alpha_{2}x_{i}^{2} + u_{i}\\] Next, we compute the coefficient of determination \\(R^2\\) for the auxillary model. The test statistic is \\(nR^2 \\sim \\chi_{d}^2\\) , where \\(n\\) is the number of observations and \\(d = 2\\) is the number of regressors, excluding the intercept. If \\(nR^2 > \\chi^2_{1-\\alpha, d}\\) , we reject the null hypothesis and conclude that the time series exhibits heteroskedasticity. For more information, the following sources may be useful: White Test Deep Dive Marno Verbeek, A Guide to Modern Econometrics (2004) William H. Greene, Econometric Analysis, 5th Edition (2002)","title":"White Test"},{"location":"frequency-analysis/","text":"Frequency Analysis Flood Frequency Analysis (FFA) is the act of using a fitted probability distribution to make predictions about the frequency of extreme streamflow events (i.e. floods). To do FFA, we require a probability model with suitably chosen parameters based on the data. This section will assume that these requirements are met. Typically, we describe the severity of floods in terms of their return period . Suppose we have a flood, which I will refer to as \\(A\\) . If we expect to see a flood at least as severe as \\(A\\) every ten years, then we say that \\(A\\) is a ten-year flood . Since our framework uses annual maximum streamflow data, a ten-year flood corresponds to an exceedance probability of \\(0.1\\) . Note that an exceedance probability of \\(0.1\\) corresponds to the \\(1 - 0.1 = 0.90\\) quantile of our probability distribution. Here is a table of the return periods, exceedance probabilities, and quantiles used in the FFA framework: Return Period Exceedance Probability Quantile \\(2\\) Years \\(0.50\\) \\(0.50\\) \\(5\\) Years \\(0.20\\) \\(0.80\\) \\(10\\) Years \\(0.10\\) \\(0.90\\) \\(20\\) Years \\(0.05\\) \\(0.95\\) \\(50\\) Years \\(0.02\\) \\(0.98\\) \\(100\\) Years \\(0.01\\) \\(0.99\\) Suppose our fitted probability distribution has cumulative distribution function \\(F(x)\\) . The function \\(F(x)\\) maps annual maximum streamflow values to quantiles. However, we want to determine the streamflow from the quantiles, so we use the inverse of the cumulative distribution \\(F^{-1}(x)\\) instead. The function \\(F^{-1}(x)\\) is also known as the Quantile Function . Note : The FFA framework uses the quantile functions implemented in the lmom package. We typically present the results of a flood frequency analysis as a graph with time on the \\(x\\) -axis and annual maximum streamflow on the \\(y\\) -axis. There are two ways to read a graph like this: Estimate the severity of a flood for a given time period. From the graph below, we could determine that every 50 years, we can expect a streamflow event of approximately \\(4500\\text{m}^3/\\text{s}\\) . Estimate the frequency of a flood for a given severity. From the graph below, we could determine that a streamflow event of \\(3000\\text{m}^3/\\text{s}\\) or higher will occur every 6-7 years. Note : For information about how the confidence bounds in this figure are computed, see here . Handling Non-Stationarity We say that a distribution is Non-Stationary if its mean or variance (or both) are changing over time. Under non-stationarity, the quantile function of our fitted probability distribution is also a function of time \\(F^{-1}(x, t)\\) . An Idea for Estimating Return Periods Suppose we want to estimate the severity of a flood with return period \\(t_{0}\\) at time \\(t^{*}\\) . Generate a sample \\(x_{1}, \\dots, x_{t_{0}}\\) where \\(x_{i} \\sim \\text{Unif} [0, 1]\\) . Compute simulated streamflow events \\(y_{1}, \\dots, y_{t_{0}}\\) where \\(y_{i} = F^{-1}(x_{i}, t^{*} + i)\\) . Let \\(z = \\max (y_{1}, \\dots, y_{t_{0}})\\) be the most severe streamflow event in the sample. Repeat steps 1-3 \\(n\\) times to generate a sequence \\((z_{1}, \\dots, z_{n})\\) . The mean \\((z_{1}, \\dots, z_{n}) / n\\) is an accurate estimate of the severity.","title":"Frequency Analysis"},{"location":"frequency-analysis/#frequency-analysis","text":"Flood Frequency Analysis (FFA) is the act of using a fitted probability distribution to make predictions about the frequency of extreme streamflow events (i.e. floods). To do FFA, we require a probability model with suitably chosen parameters based on the data. This section will assume that these requirements are met. Typically, we describe the severity of floods in terms of their return period . Suppose we have a flood, which I will refer to as \\(A\\) . If we expect to see a flood at least as severe as \\(A\\) every ten years, then we say that \\(A\\) is a ten-year flood . Since our framework uses annual maximum streamflow data, a ten-year flood corresponds to an exceedance probability of \\(0.1\\) . Note that an exceedance probability of \\(0.1\\) corresponds to the \\(1 - 0.1 = 0.90\\) quantile of our probability distribution. Here is a table of the return periods, exceedance probabilities, and quantiles used in the FFA framework: Return Period Exceedance Probability Quantile \\(2\\) Years \\(0.50\\) \\(0.50\\) \\(5\\) Years \\(0.20\\) \\(0.80\\) \\(10\\) Years \\(0.10\\) \\(0.90\\) \\(20\\) Years \\(0.05\\) \\(0.95\\) \\(50\\) Years \\(0.02\\) \\(0.98\\) \\(100\\) Years \\(0.01\\) \\(0.99\\) Suppose our fitted probability distribution has cumulative distribution function \\(F(x)\\) . The function \\(F(x)\\) maps annual maximum streamflow values to quantiles. However, we want to determine the streamflow from the quantiles, so we use the inverse of the cumulative distribution \\(F^{-1}(x)\\) instead. The function \\(F^{-1}(x)\\) is also known as the Quantile Function . Note : The FFA framework uses the quantile functions implemented in the lmom package. We typically present the results of a flood frequency analysis as a graph with time on the \\(x\\) -axis and annual maximum streamflow on the \\(y\\) -axis. There are two ways to read a graph like this: Estimate the severity of a flood for a given time period. From the graph below, we could determine that every 50 years, we can expect a streamflow event of approximately \\(4500\\text{m}^3/\\text{s}\\) . Estimate the frequency of a flood for a given severity. From the graph below, we could determine that a streamflow event of \\(3000\\text{m}^3/\\text{s}\\) or higher will occur every 6-7 years. Note : For information about how the confidence bounds in this figure are computed, see here .","title":"Frequency Analysis"},{"location":"frequency-analysis/#handling-non-stationarity","text":"We say that a distribution is Non-Stationary if its mean or variance (or both) are changing over time. Under non-stationarity, the quantile function of our fitted probability distribution is also a function of time \\(F^{-1}(x, t)\\) .","title":"Handling Non-Stationarity"},{"location":"frequency-analysis/#an-idea-for-estimating-return-periods","text":"Suppose we want to estimate the severity of a flood with return period \\(t_{0}\\) at time \\(t^{*}\\) . Generate a sample \\(x_{1}, \\dots, x_{t_{0}}\\) where \\(x_{i} \\sim \\text{Unif} [0, 1]\\) . Compute simulated streamflow events \\(y_{1}, \\dots, y_{t_{0}}\\) where \\(y_{i} = F^{-1}(x_{i}, t^{*} + i)\\) . Let \\(z = \\max (y_{1}, \\dots, y_{t_{0}})\\) be the most severe streamflow event in the sample. Repeat steps 1-3 \\(n\\) times to generate a sequence \\((z_{1}, \\dots, z_{n})\\) . The mean \\((z_{1}, \\dots, z_{n}) / n\\) is an accurate estimate of the severity.","title":"An Idea for Estimating Return Periods"},{"location":"goals/","text":"Goals Vidrio-Sahag\u00fan et al. (2024) identified a number of issues with FFA research in Canada. Our framework aims to address these issues. In particular, we hope to achieve: Standardization : The United States uses a standardized distribution (LP3) and parameter estimation method (expected moments) for FFA. However, Canada does not provide such guidance. By developing this framework, we hope to limit subjectivity in flood estimation. Reproducibility : A significant fraction of FFA studies in Canada do not provide essential information for reproduction, such as the exploratory data analysis (EDA) performed, the distribution selection mechanism, and even the probability distribution itself. Our framework will ensure all of this information is available. Statistical Rigour : Historically, many FFA studies have not performed uncertainty quantification, which makes it difficult to draw accurate conclusion from the results. Our framework automatically performs uncertainty analysis to quantify potential errors. Research-to-Practice Translation : The disorganization of FFA research in Canada makes it difficult for regulatory agencies to access cutting-edge advancements in FFA methodologies. By providing a common set of techniques for modellers to use, we hope to bridge the gap between research and practice. Development is guided by the following principles: Software Freedom : Our framework is built on free and open source software. Modularity : Users are allowed to use as much or as little of the framework as they like. Interoperability : Our framework can be seamlessly integrated with other flood models. Flexibility : Users can tailor their analysis to the nuances of individual watersheds. Clarity : The source code is easy to read and understand without confusing tricks. Robustness : The framework can handle datasets with small sample sizes or missing values. Speed : The framework is fast enough to handle batch processing of many datasets. Architecture We believe the following architecture will allow us to achieve the goals listed above: CRAN Package Our CRAN package will contain a suite of statistical tests, distribution selection mechanisms, and probability distributions for FFA. Many of the statistical tests used in our framework are sparsely documented, so providing an implementation of these tests in R will benefit the statistical community at large. Command Line Interface The command line interface (CLI) will allow technical users to programmatically generate reports and run statistical models. Additionally, the CLI will make it possible to run each component of the FFA framework separately. This will allow advanced users to design custom analysis pipelines. Plumber API An API (built with plumber.R ) will provide programmatic access to our framework. Web App Finally, we will a develop a web application on top of the Plumber API which will give new users a simple GUI interface for accessing our framework. We hope that this web application will make it easier for beginner modellers to learn about FFA. Non-Goals Warning : This list is not final and will be updated as development continues. This software will not support the following features: Explanations of FFA itself. We assume the modeller is familiar with basic hydrological concepts. Handling different time periods (i.e. monthly maximum streamflow data). Transforming the AMS data in any way (i.e. removing serial correlation, removing trends).","title":"Goals"},{"location":"goals/#goals","text":"Vidrio-Sahag\u00fan et al. (2024) identified a number of issues with FFA research in Canada. Our framework aims to address these issues. In particular, we hope to achieve: Standardization : The United States uses a standardized distribution (LP3) and parameter estimation method (expected moments) for FFA. However, Canada does not provide such guidance. By developing this framework, we hope to limit subjectivity in flood estimation. Reproducibility : A significant fraction of FFA studies in Canada do not provide essential information for reproduction, such as the exploratory data analysis (EDA) performed, the distribution selection mechanism, and even the probability distribution itself. Our framework will ensure all of this information is available. Statistical Rigour : Historically, many FFA studies have not performed uncertainty quantification, which makes it difficult to draw accurate conclusion from the results. Our framework automatically performs uncertainty analysis to quantify potential errors. Research-to-Practice Translation : The disorganization of FFA research in Canada makes it difficult for regulatory agencies to access cutting-edge advancements in FFA methodologies. By providing a common set of techniques for modellers to use, we hope to bridge the gap between research and practice. Development is guided by the following principles: Software Freedom : Our framework is built on free and open source software. Modularity : Users are allowed to use as much or as little of the framework as they like. Interoperability : Our framework can be seamlessly integrated with other flood models. Flexibility : Users can tailor their analysis to the nuances of individual watersheds. Clarity : The source code is easy to read and understand without confusing tricks. Robustness : The framework can handle datasets with small sample sizes or missing values. Speed : The framework is fast enough to handle batch processing of many datasets.","title":"Goals"},{"location":"goals/#architecture","text":"We believe the following architecture will allow us to achieve the goals listed above:","title":"Architecture"},{"location":"goals/#cran-package","text":"Our CRAN package will contain a suite of statistical tests, distribution selection mechanisms, and probability distributions for FFA. Many of the statistical tests used in our framework are sparsely documented, so providing an implementation of these tests in R will benefit the statistical community at large.","title":"CRAN Package"},{"location":"goals/#command-line-interface","text":"The command line interface (CLI) will allow technical users to programmatically generate reports and run statistical models. Additionally, the CLI will make it possible to run each component of the FFA framework separately. This will allow advanced users to design custom analysis pipelines.","title":"Command Line Interface"},{"location":"goals/#plumber-api","text":"An API (built with plumber.R ) will provide programmatic access to our framework.","title":"Plumber API"},{"location":"goals/#web-app","text":"Finally, we will a develop a web application on top of the Plumber API which will give new users a simple GUI interface for accessing our framework. We hope that this web application will make it easier for beginner modellers to learn about FFA.","title":"Web App"},{"location":"goals/#non-goals","text":"Warning : This list is not final and will be updated as development continues. This software will not support the following features: Explanations of FFA itself. We assume the modeller is familiar with basic hydrological concepts. Handling different time periods (i.e. monthly maximum streamflow data). Transforming the AMS data in any way (i.e. removing serial correlation, removing trends).","title":"Non-Goals"},{"location":"license/","text":"License FFA Framework \u00a9 2025 by Riley Wheadon, Cuauht\u00e9moc Tonatiuh Vidrio-Sahag\u00fan, and Alain Pietroniro is licensed under CC BY 4.0. To view a copy of this license, visit https://creativecommons.org/licenses/by/4.0/","title":"License"},{"location":"license/#license","text":"FFA Framework \u00a9 2025 by Riley Wheadon, Cuauht\u00e9moc Tonatiuh Vidrio-Sahag\u00fan, and Alain Pietroniro is licensed under CC BY 4.0. To view a copy of this license, visit https://creativecommons.org/licenses/by/4.0/","title":"License"},{"location":"matlab/","text":"This page documents changes from original MATLAB code . General All configuration details are now stored in YAML files. Implement a suite of unit tests using the testthat library. Use knitr and rmarkdown to generate reports with text, equations, and images. Export reports to .md (Pandoc) and .html . Run individual statistical tests using run-stats.R . Exploratory Data Analysis (EDA) Bug Fixes Only show statistically significant change points for the Pettitt and MKS plots. Fix bug where the MKS test would only identify one change point, even if multiple change points were found to be above the threshold for statistical significance. Fix major bug where the MKS test identified change points based on the progressive series instead of the U-statistics of the crossing points. Remove unnecessary rounding in the moving window algorithm for AMS variability. Fix bug where the Phillips-Perron and KPSS tests failed to account for drift and trend. Fix bug where the Phillips-Perron and KPSS tests failed to account for serial correlation. Framework Changes If serial correlation is identified, do not run the Phillips-Perron and KPSS tests. Implement the Runs test to detect nonlinearity after fitting Sen's trend estimator. Run change point detection in multiple stages. Flood Frequency Analysis (FFA) Bug Fixes Implement the RFPL uncertainty quantification with the Weibull distribution. Distribution Changes The generalized pareto (GPA) distribution has been removed, since its likelihood function is not amenable to maximum likelihood estimation. This issue occurs because the GPA distribution is used in peaks over threshold modelling, which we have not yet implemented. The R version uses the three parameter Weibull distribution (with location, scale, and shape) parameters instead of the two parameter Weibull distribution (with scale and shape parameters). This ensures consistency with the other distributions, which all have location parameters. The quantile function for the Pearson Type III (PE3) distribution does not have a closed form. The quape3 function, which computes the quantiles of the PE3 distribution in the R version, uses a different approximate formula than the MATLAB version. Therefore, quantiles for the PE3 distribution may vary by up to \\(1\\%\\) between the R version and MATLAB versions. Model Selection Changes The L-distance and L-kurtosis selection methods have been improved by using an optimization algorithm to find the parameters with the closest L-moments to the data instead of using a brute force approach. This is more computational efficient and elegant (but has no significant effect on the results). The procedure for computing the Z-statistic selection metric has been changed. If the L-moments of the dataset do not satisfy \\(\\tau_{4} \\leq (1 + 5\\tau _{3}^2)/6\\) , then the Kappa distribution will not be fitted and the candidate distributions that use the dataset will be omitted. Parameter Estimation Changes Parameterization of the PE3/LP3 distributions fails for some datasets because MATLAB is unable to handle the large numbers created by the gamma function. To manage this issue, the MATLAB version used the conventional moments (i.e. sample mean/variance/skewness) when this occurred. This behaviour is no longer necessary and has been removed. The R implementation uses L-BFGS-B for MLE/GMLE parameter estimation instead of Nelder-Mead, since the gradient is well defined for the likelihood functions we are working with. Additionally, the L-BFGS-B method makes it possible to assign bounds to the variables. This modification produced slight improvements to the MLE/GMLE for some datasets. Model Assessment Changes Use the built-in R function approx() to perform log-linear interpolation of the return periods. The MATLAB implementation uses a hard-coded algorithm which behaves unpredictably when the original and interpolated \\(x\\) -values are equally.","title":"MATLAB Version"},{"location":"matlab/#general","text":"All configuration details are now stored in YAML files. Implement a suite of unit tests using the testthat library. Use knitr and rmarkdown to generate reports with text, equations, and images. Export reports to .md (Pandoc) and .html . Run individual statistical tests using run-stats.R .","title":"General"},{"location":"matlab/#exploratory-data-analysis-eda","text":"","title":"Exploratory Data Analysis (EDA)"},{"location":"matlab/#bug-fixes","text":"Only show statistically significant change points for the Pettitt and MKS plots. Fix bug where the MKS test would only identify one change point, even if multiple change points were found to be above the threshold for statistical significance. Fix major bug where the MKS test identified change points based on the progressive series instead of the U-statistics of the crossing points. Remove unnecessary rounding in the moving window algorithm for AMS variability. Fix bug where the Phillips-Perron and KPSS tests failed to account for drift and trend. Fix bug where the Phillips-Perron and KPSS tests failed to account for serial correlation.","title":"Bug Fixes"},{"location":"matlab/#framework-changes","text":"If serial correlation is identified, do not run the Phillips-Perron and KPSS tests. Implement the Runs test to detect nonlinearity after fitting Sen's trend estimator. Run change point detection in multiple stages.","title":"Framework Changes"},{"location":"matlab/#flood-frequency-analysis-ffa","text":"","title":"Flood Frequency Analysis (FFA)"},{"location":"matlab/#bug-fixes_1","text":"Implement the RFPL uncertainty quantification with the Weibull distribution.","title":"Bug Fixes"},{"location":"matlab/#distribution-changes","text":"The generalized pareto (GPA) distribution has been removed, since its likelihood function is not amenable to maximum likelihood estimation. This issue occurs because the GPA distribution is used in peaks over threshold modelling, which we have not yet implemented. The R version uses the three parameter Weibull distribution (with location, scale, and shape) parameters instead of the two parameter Weibull distribution (with scale and shape parameters). This ensures consistency with the other distributions, which all have location parameters. The quantile function for the Pearson Type III (PE3) distribution does not have a closed form. The quape3 function, which computes the quantiles of the PE3 distribution in the R version, uses a different approximate formula than the MATLAB version. Therefore, quantiles for the PE3 distribution may vary by up to \\(1\\%\\) between the R version and MATLAB versions.","title":"Distribution Changes"},{"location":"matlab/#model-selection-changes","text":"The L-distance and L-kurtosis selection methods have been improved by using an optimization algorithm to find the parameters with the closest L-moments to the data instead of using a brute force approach. This is more computational efficient and elegant (but has no significant effect on the results). The procedure for computing the Z-statistic selection metric has been changed. If the L-moments of the dataset do not satisfy \\(\\tau_{4} \\leq (1 + 5\\tau _{3}^2)/6\\) , then the Kappa distribution will not be fitted and the candidate distributions that use the dataset will be omitted.","title":"Model Selection Changes"},{"location":"matlab/#parameter-estimation-changes","text":"Parameterization of the PE3/LP3 distributions fails for some datasets because MATLAB is unable to handle the large numbers created by the gamma function. To manage this issue, the MATLAB version used the conventional moments (i.e. sample mean/variance/skewness) when this occurred. This behaviour is no longer necessary and has been removed. The R implementation uses L-BFGS-B for MLE/GMLE parameter estimation instead of Nelder-Mead, since the gradient is well defined for the likelihood functions we are working with. Additionally, the L-BFGS-B method makes it possible to assign bounds to the variables. This modification produced slight improvements to the MLE/GMLE for some datasets.","title":"Parameter Estimation Changes"},{"location":"matlab/#model-assessment-changes","text":"Use the built-in R function approx() to perform log-linear interpolation of the return periods. The MATLAB implementation uses a hard-coded algorithm which behaves unpredictably when the original and interpolated \\(x\\) -values are equally.","title":"Model Assessment Changes"},{"location":"model-assessment/","text":"Model Assessment Non-Parametric Models A Plotting Position is a distribution free estimator used to derive empirical exceedance probabilities. By using the plotting position, we can evaluate the quality of our parametric model. To compute the plotting position, arrange the sample observations in descending order of magnitude: \\(x_{n:n} \\geq \\dots \\geq x_{1:n}\\) . Then, the empirical exceedance probabilities are given by the following formula: \\[ p_{i:n} = \\frac{i-a}{n+1 - 2a}, \\quad i \\in \\{1, \\dots , n\\} \\] The coefficient \\(a\\) depends on the plotting position formula: Formula \\(a\\) Simplified Equation Weibull \\(0\\) \\(p_{i:n} = \\frac{i}{n +1}\\) Blom \\(0.375\\) \\(p_{i:n} = \\frac{i-0.375}{n + 0.25}\\) Cunnane \\(0.4\\) \\(p_{i:n} = \\frac{i-0.4}{n+0.2}\\) Gringorten \\(0.44\\) \\(p_{i:n} = \\frac{i-0.44}{n + 0.12}\\) Hazen \\(0.5\\) \\(p_{i:n} = \\frac{i-0.5}{n}\\) By default, the FFA framework uses the Weibull formula, which is unbiased. Accuracy Statistics \\(R^2\\) - Coefficient of Determination To compute the \\(R^2\\) statistic, we perform a linear regression of the streamflow data against the predictions of the parametric model. The \\(R^2\\) statistic describes how well the parametric model captures variance in the streamflow data. Higher is better. RMSE - Root-Mean Squared Error The RMSE statistic describes the average absolute difference between the data and the predictions of the parametric model. Lower is better. Bias The Bias statistic describes the average difference between the data and the predictions of the parametric model. A positive bias indicates that the model tends to overestimate the data while a negative bias indicates that the model tends to underestimate the data. Information Criterion The Akaike Information Criterion ( AIC ) and Bayesian Information Criterion ( BIC ) describe the quality of a model based on the error ( RMSE ) and the number of parameters ( n_theta ). Better models have a lower AIC / BIC , which indicates that they have less parameters and lower error. AIC <- (n * log(RMSE)) + (2 * n_theta) BIC <- (n * log(RMSE)) + (log(n) * n_theta) Uncertainty Statistics The FFA framework uses three statistics to assess the uncertainty in flood quantile estimates: AW captures precision (narrower confidence intervals are better). POC captures reliability (higher coverage of observations is better). CWI is a composite measure balancing both precision and reliability (lower is better). We use these metrics together to evaluate the robustness of the flood frequency analysis. AW \u2013 Average Width AW is the average width of the interpolated confidence intervals across return periods of interest. A smaller AW indicates more precise quantile estimates. To compute AW , we use log-linear interpolation to estimate the confidence intervals of the empirical exceedance probabilities from the confidence intervals computed during uncertainty quantification . POC \u2013 Percent of Coverage POC is the percentage of observed quantiles that fall within their corresponding confidence intervals. A higher POC indicates greater reliability of the confidence intervals. CWI \u2013 Confidence Width Indicator CWI is a composite metric that penalizes wide and/or poorly calibrated confidence intervals. A lower CWI is better. Wide intervals and low coverage increase the penalty. Ideal confidence intervals are both narrow and well-calibrated, resulting in a low CWI . The CWI is computed using the following formula, where alpha is the significance level. CWI <- AW * exp((1 - alpha) - POC / 100)^2;","title":"Model Assessment"},{"location":"model-assessment/#model-assessment","text":"","title":"Model Assessment"},{"location":"model-assessment/#non-parametric-models","text":"A Plotting Position is a distribution free estimator used to derive empirical exceedance probabilities. By using the plotting position, we can evaluate the quality of our parametric model. To compute the plotting position, arrange the sample observations in descending order of magnitude: \\(x_{n:n} \\geq \\dots \\geq x_{1:n}\\) . Then, the empirical exceedance probabilities are given by the following formula: \\[ p_{i:n} = \\frac{i-a}{n+1 - 2a}, \\quad i \\in \\{1, \\dots , n\\} \\] The coefficient \\(a\\) depends on the plotting position formula: Formula \\(a\\) Simplified Equation Weibull \\(0\\) \\(p_{i:n} = \\frac{i}{n +1}\\) Blom \\(0.375\\) \\(p_{i:n} = \\frac{i-0.375}{n + 0.25}\\) Cunnane \\(0.4\\) \\(p_{i:n} = \\frac{i-0.4}{n+0.2}\\) Gringorten \\(0.44\\) \\(p_{i:n} = \\frac{i-0.44}{n + 0.12}\\) Hazen \\(0.5\\) \\(p_{i:n} = \\frac{i-0.5}{n}\\) By default, the FFA framework uses the Weibull formula, which is unbiased.","title":"Non-Parametric Models"},{"location":"model-assessment/#accuracy-statistics","text":"","title":"Accuracy Statistics"},{"location":"model-assessment/#r2-coefficient-of-determination","text":"To compute the \\(R^2\\) statistic, we perform a linear regression of the streamflow data against the predictions of the parametric model. The \\(R^2\\) statistic describes how well the parametric model captures variance in the streamflow data. Higher is better.","title":"\\(R^2\\) - Coefficient of Determination"},{"location":"model-assessment/#rmse-root-mean-squared-error","text":"The RMSE statistic describes the average absolute difference between the data and the predictions of the parametric model. Lower is better.","title":"RMSE - Root-Mean Squared Error"},{"location":"model-assessment/#bias","text":"The Bias statistic describes the average difference between the data and the predictions of the parametric model. A positive bias indicates that the model tends to overestimate the data while a negative bias indicates that the model tends to underestimate the data.","title":"Bias"},{"location":"model-assessment/#information-criterion","text":"The Akaike Information Criterion ( AIC ) and Bayesian Information Criterion ( BIC ) describe the quality of a model based on the error ( RMSE ) and the number of parameters ( n_theta ). Better models have a lower AIC / BIC , which indicates that they have less parameters and lower error. AIC <- (n * log(RMSE)) + (2 * n_theta) BIC <- (n * log(RMSE)) + (log(n) * n_theta)","title":"Information Criterion"},{"location":"model-assessment/#uncertainty-statistics","text":"The FFA framework uses three statistics to assess the uncertainty in flood quantile estimates: AW captures precision (narrower confidence intervals are better). POC captures reliability (higher coverage of observations is better). CWI is a composite measure balancing both precision and reliability (lower is better). We use these metrics together to evaluate the robustness of the flood frequency analysis.","title":"Uncertainty Statistics"},{"location":"model-assessment/#aw-average-width","text":"AW is the average width of the interpolated confidence intervals across return periods of interest. A smaller AW indicates more precise quantile estimates. To compute AW , we use log-linear interpolation to estimate the confidence intervals of the empirical exceedance probabilities from the confidence intervals computed during uncertainty quantification .","title":"AW \u2013 Average Width"},{"location":"model-assessment/#poc-percent-of-coverage","text":"POC is the percentage of observed quantiles that fall within their corresponding confidence intervals. A higher POC indicates greater reliability of the confidence intervals.","title":"POC \u2013 Percent of Coverage"},{"location":"model-assessment/#cwi-confidence-width-indicator","text":"CWI is a composite metric that penalizes wide and/or poorly calibrated confidence intervals. A lower CWI is better. Wide intervals and low coverage increase the penalty. Ideal confidence intervals are both narrow and well-calibrated, resulting in a low CWI . The CWI is computed using the following formula, where alpha is the significance level. CWI <- AW * exp((1 - alpha) - POC / 100)^2;","title":"CWI \u2013 Confidence Width Indicator"},{"location":"model-selection/","text":"Model Selection Our framework uses the method of L-moment ratios to choose a suitable probability model for frequency analysis. This technique involves comparing the L-moments of the data with the known L-moments of various probability distributions. The CRAN package lmom is used extensively in this portion of the framework. An Introduction to L-Moments Definition : The \\(k\\) -th Order Statistic of a statistical sample is its \\(k\\) -th smallest value. Definition : The \\(r\\) -th Population L-moment \\(\\lambda_{r}\\) is a linear combination of the expectation of the order statistics. Let \\(X_{k:n}\\) be the \\(k\\) -th order statistic from a sample of size \\(n\\) . Then, \\[ \\lambda_{r} = \\frac{1}{r} \\sum_{k=0}^{r-1} (-1)^{k} \\binom{r-1}{k} \\mathbb{E}[X_{r-k:r}] \\] Definition : A Probability Weighted Moment (PWM) encodes information about a value's position on the cumulative distribution function. The \\(r\\) -th PWM, denoted \\(\\beta_{r}\\) , is: \\[ \\beta_{r} = \\mathbb{E}[X \\cdot F(X)^{r}] \\] For an ordered sample \\(x_{1:n} \\leq \\dots \\leq x_{n:n}\\) , the sample PWM is often estimated as: \\[ b_{r} = \\frac{1}{n} \\sum_{i=1}^{r} x_{i:n} \\left(\\frac{i-1}{n-1}\\right) ^{r} \\] Remark : The first four sample L-moments can be computed as linear combinations of the PWMs: \\[ \\begin{aligned} l_{1} &= b_{0} \\\\ l_{2} &= 2b_{1} - b_{0} \\\\ l_{3} &= 6b_{2} - 6b_{1} + b_{0} \\\\ l_{4} &= 20b_{3} - 30b_{2} + 12b_{1} - b_{0} \\end{aligned} \\] The L-moments are used to compute the Sample L-variance \\(t_{2}\\) , Sample L-skewness \\(t_{3}\\) and the Sample L-kurtosis \\(t_{4}\\) using the following formulas: \\[ \\begin{aligned} t_{2} &= l_{2} / l_{1} \\\\ t_{3} &= l_{3} / l_{2} \\\\ t_{4} &= l_{4} / l_{2} \\end{aligned} \\] Then, we compare these statistics to their theoretical values to select a distribution. List of Candidate Distributions Distribution Abbreviation Number of Parameters Generalized Extreme Value GEV 3 Gumbel 1 GUM 2 (Log) Normal NOR/LNO 2 Generalized Logistic GLO 3 (Log) Pearson Type III PE3/LP3 3 Generalized Normal GNO 3 Weibull WEI 3 The four-parameter kappa distribution (K4D), generalizes all ten of the distributions above. Note : Probability distributions with less than three parameters have constant L-skewness \\(\\tau_{3}\\) and L-kurtosis \\(\\tau_{4}\\) regardless of their parameters. The L-skewness and L-kurtosis of probability distributions with three parameters is a function of the shape parameter \\(\\kappa\\) . Selection Metrics L-Distance Compare the euclidean distance between the sample L-skewness and sample L-kurtosis \\((t_{3}, t_{4})\\) and the known L-moment ratios \\((\\tau_{3}, \\tau_{4})\\) for each candidate distribution. For probability distributions with three parameters, we use the minimum distance between the L-moment ratio curve \\((\\tau _{3}(\\kappa ), \\tau _{4}(\\kappa ))\\) and the L-moment ratios of the sample \\((t_{3}, t_{4})\\) . L-Kurtosis The L-kurtosis method is only used for three parameter probability distributions. First, identify the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) . Then, compare the difference between the sample L-kurtosis and the theoretical L-kurtosis using the metric \\(|\\tau_{4}(\\kappa ^{*}) - t_{4} |\\) . Z-statistic The Z-statistic selection metric is calculated as follows (for three parameter distributions): Fit the four-parameter Kappa (K4D) distribution to the data using \\(t_{2}\\) , \\(t_{3}\\) , and \\(t_{4}\\) . Generate \\(N_{\\text{sim}}\\) bootstrap samples from the fitted K4D distribution. Calculate the sample L-kurtosis \\(t_{4}^{[i]}\\) of each synthetic dataset. Calculate the bias and standard deviation of the bootstrap distribution: \\[ B_{4} = N_{\\text{sim} }^{-1} \\sum_{i = 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right) \\] \\[ \\sigma _{4} = \\left[(N_{\\text{sim} } - 1)^{-1} \\left\\{\\sum_{i - 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right)^2 - N_{\\text{sim} } B_{4}^2\\right\\} \\right] ^{\\frac{1}{2}} \\] Identify the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) . Use bootstrap distribution to compute the Z-statistic for each distribution: \\[ z = \\frac{\\tau_{4} (\\kappa ^{*}) - t_{4} + B_{4} }{ \\sigma _{4}} \\] Choose the distribution with the smallest Z-statistic. Handling Non-Stationarity There are three non-stationary scenarios that can be identified during EDA: Significant trend in the mean only. Significant trend in the STD only. Significant trend in both the mean and STD. To determine the best probability distribution for non-stationary data, we decompose the data into stationary and non-stationary components and then use one of the methods described above. Decomposition (Scenario 1) Use Sen's Trend Estimator to identify the slope \\(b_{1}\\) and intercept \\(b_{0}\\) of the trend. Detrend the data by subtracting the linear function \\((b_{1} \\cdot \\text{Covariate})\\) from the data, where the covariate is a value between \\([0, 1]\\) derived from the index. If necessary, enforce positivity by adding a constant such that \\(\\min(\\text{data}) = 1\\) . Decomposition (Scenario 2) Use a moving-window algorithm to compute the variance of the data. Use Sen's Trend Estimator to identify the slope \\(c_{1}\\) and intercept \\(c_{0}\\) of the trend in the variance. Normalize the data to have mean \\(0\\) , then divide out the scale factor \\(g_{t}\\) . \\[ g_{t} = \\frac{(c_{1} \\cdot \\text{Covariate} ) + c_{0}}{c_{0}} \\] Add back the long-term mean \\(\\mu\\) , and then ensure positivity as in Scenario 1. Decomposition (Scenario 3) Remove the linear (additive) trend exactly as in Scenario 1. On that detrended series, compute a rolling\u2010window STD series and fit its trend. Divide the detrended data by the time-varying scale factor \\(g_{t}\\) (as in Scenario 2). Shift to preserve the series mean and ensure positivity. The Gumbel distribution is equivalent to the GEV distribution with \\(\\xi = 0\\) . \u21a9","title":"Model Selection"},{"location":"model-selection/#model-selection","text":"Our framework uses the method of L-moment ratios to choose a suitable probability model for frequency analysis. This technique involves comparing the L-moments of the data with the known L-moments of various probability distributions. The CRAN package lmom is used extensively in this portion of the framework.","title":"Model Selection"},{"location":"model-selection/#an-introduction-to-l-moments","text":"Definition : The \\(k\\) -th Order Statistic of a statistical sample is its \\(k\\) -th smallest value. Definition : The \\(r\\) -th Population L-moment \\(\\lambda_{r}\\) is a linear combination of the expectation of the order statistics. Let \\(X_{k:n}\\) be the \\(k\\) -th order statistic from a sample of size \\(n\\) . Then, \\[ \\lambda_{r} = \\frac{1}{r} \\sum_{k=0}^{r-1} (-1)^{k} \\binom{r-1}{k} \\mathbb{E}[X_{r-k:r}] \\] Definition : A Probability Weighted Moment (PWM) encodes information about a value's position on the cumulative distribution function. The \\(r\\) -th PWM, denoted \\(\\beta_{r}\\) , is: \\[ \\beta_{r} = \\mathbb{E}[X \\cdot F(X)^{r}] \\] For an ordered sample \\(x_{1:n} \\leq \\dots \\leq x_{n:n}\\) , the sample PWM is often estimated as: \\[ b_{r} = \\frac{1}{n} \\sum_{i=1}^{r} x_{i:n} \\left(\\frac{i-1}{n-1}\\right) ^{r} \\] Remark : The first four sample L-moments can be computed as linear combinations of the PWMs: \\[ \\begin{aligned} l_{1} &= b_{0} \\\\ l_{2} &= 2b_{1} - b_{0} \\\\ l_{3} &= 6b_{2} - 6b_{1} + b_{0} \\\\ l_{4} &= 20b_{3} - 30b_{2} + 12b_{1} - b_{0} \\end{aligned} \\] The L-moments are used to compute the Sample L-variance \\(t_{2}\\) , Sample L-skewness \\(t_{3}\\) and the Sample L-kurtosis \\(t_{4}\\) using the following formulas: \\[ \\begin{aligned} t_{2} &= l_{2} / l_{1} \\\\ t_{3} &= l_{3} / l_{2} \\\\ t_{4} &= l_{4} / l_{2} \\end{aligned} \\] Then, we compare these statistics to their theoretical values to select a distribution.","title":"An Introduction to L-Moments"},{"location":"model-selection/#list-of-candidate-distributions","text":"Distribution Abbreviation Number of Parameters Generalized Extreme Value GEV 3 Gumbel 1 GUM 2 (Log) Normal NOR/LNO 2 Generalized Logistic GLO 3 (Log) Pearson Type III PE3/LP3 3 Generalized Normal GNO 3 Weibull WEI 3 The four-parameter kappa distribution (K4D), generalizes all ten of the distributions above. Note : Probability distributions with less than three parameters have constant L-skewness \\(\\tau_{3}\\) and L-kurtosis \\(\\tau_{4}\\) regardless of their parameters. The L-skewness and L-kurtosis of probability distributions with three parameters is a function of the shape parameter \\(\\kappa\\) .","title":"List of Candidate Distributions"},{"location":"model-selection/#selection-metrics","text":"","title":"Selection Metrics"},{"location":"model-selection/#l-distance","text":"Compare the euclidean distance between the sample L-skewness and sample L-kurtosis \\((t_{3}, t_{4})\\) and the known L-moment ratios \\((\\tau_{3}, \\tau_{4})\\) for each candidate distribution. For probability distributions with three parameters, we use the minimum distance between the L-moment ratio curve \\((\\tau _{3}(\\kappa ), \\tau _{4}(\\kappa ))\\) and the L-moment ratios of the sample \\((t_{3}, t_{4})\\) .","title":"L-Distance"},{"location":"model-selection/#l-kurtosis","text":"The L-kurtosis method is only used for three parameter probability distributions. First, identify the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) . Then, compare the difference between the sample L-kurtosis and the theoretical L-kurtosis using the metric \\(|\\tau_{4}(\\kappa ^{*}) - t_{4} |\\) .","title":"L-Kurtosis"},{"location":"model-selection/#z-statistic","text":"The Z-statistic selection metric is calculated as follows (for three parameter distributions): Fit the four-parameter Kappa (K4D) distribution to the data using \\(t_{2}\\) , \\(t_{3}\\) , and \\(t_{4}\\) . Generate \\(N_{\\text{sim}}\\) bootstrap samples from the fitted K4D distribution. Calculate the sample L-kurtosis \\(t_{4}^{[i]}\\) of each synthetic dataset. Calculate the bias and standard deviation of the bootstrap distribution: \\[ B_{4} = N_{\\text{sim} }^{-1} \\sum_{i = 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right) \\] \\[ \\sigma _{4} = \\left[(N_{\\text{sim} } - 1)^{-1} \\left\\{\\sum_{i - 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right)^2 - N_{\\text{sim} } B_{4}^2\\right\\} \\right] ^{\\frac{1}{2}} \\] Identify the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) . Use bootstrap distribution to compute the Z-statistic for each distribution: \\[ z = \\frac{\\tau_{4} (\\kappa ^{*}) - t_{4} + B_{4} }{ \\sigma _{4}} \\] Choose the distribution with the smallest Z-statistic.","title":"Z-statistic"},{"location":"model-selection/#handling-non-stationarity","text":"There are three non-stationary scenarios that can be identified during EDA: Significant trend in the mean only. Significant trend in the STD only. Significant trend in both the mean and STD. To determine the best probability distribution for non-stationary data, we decompose the data into stationary and non-stationary components and then use one of the methods described above.","title":"Handling Non-Stationarity"},{"location":"model-selection/#decomposition-scenario-1","text":"Use Sen's Trend Estimator to identify the slope \\(b_{1}\\) and intercept \\(b_{0}\\) of the trend. Detrend the data by subtracting the linear function \\((b_{1} \\cdot \\text{Covariate})\\) from the data, where the covariate is a value between \\([0, 1]\\) derived from the index. If necessary, enforce positivity by adding a constant such that \\(\\min(\\text{data}) = 1\\) .","title":"Decomposition (Scenario 1)"},{"location":"model-selection/#decomposition-scenario-2","text":"Use a moving-window algorithm to compute the variance of the data. Use Sen's Trend Estimator to identify the slope \\(c_{1}\\) and intercept \\(c_{0}\\) of the trend in the variance. Normalize the data to have mean \\(0\\) , then divide out the scale factor \\(g_{t}\\) . \\[ g_{t} = \\frac{(c_{1} \\cdot \\text{Covariate} ) + c_{0}}{c_{0}} \\] Add back the long-term mean \\(\\mu\\) , and then ensure positivity as in Scenario 1.","title":"Decomposition (Scenario 2)"},{"location":"model-selection/#decomposition-scenario-3","text":"Remove the linear (additive) trend exactly as in Scenario 1. On that detrended series, compute a rolling\u2010window STD series and fit its trend. Divide the detrended data by the time-varying scale factor \\(g_{t}\\) (as in Scenario 2). Shift to preserve the series mean and ensure positivity. The Gumbel distribution is equivalent to the GEV distribution with \\(\\xi = 0\\) . \u21a9","title":"Decomposition (Scenario 3)"},{"location":"parameter-estimation/","text":"Parameter Estimation The FFA framework implements three methods for parameter estimation: L-moments Maximum likelihood (MLE) Generalized maximum likelihood (GMLE) Note : The parameterization used for the GEV distributions is different from the parameterization used by the lmom library. In particular, the sign of the shape parameter is inverted for consistency with \"Regional Frequency Analysis\" (Hosking, 1997). L-Moments The method estimates parameter values based on the sample L-moments \\(l_{1}\\) , \\(l_{2}\\) and the sample L-moment ratios \\(t_{3}\\) , \\(t_{4}\\) . For more information about L-moments, see here . The parameter estimation methods are based on these Fortran routines by J.R.M. Hosking. In particular, we use the lmom CRAN package, which implements the aformentioned Fotran routines. Warning : L-moments parameter estimation can yield distributions which do not have support at small values. This is generally not an issue, since we are only interested in the higher quantiles of the distrbution. However, it is important to remember that the probability distributions produced by L-moments should not be used to predict future streamflow data in general. Maximum Likelihood (MLE) For all distributions below, let \\(x\\) be a vector of data with elements \\(x_{1}, \\dots, x_{n}\\) . Two Parameter Distributions For a two-parameter distribution XXX , we use three different models: XXX is the standard model with no non-stationarity. XXX10 has a trend in the location parameter \\(\\mu\\) . XXX11 has a trend in the location \\(\\mu\\) and the scale \\(\\sigma\\) . Shown below is a table summarizing these three models: Feature XXX XXX10 XXX11 Location \\(\\mu\\) constant \\(\\mu_0 + \\mu_1z\\) \\(\\mu_0 + \\mu_1z\\) Scale \\(\\sigma\\) constant constant \\(\\sigma_0 + \\sigma_1z\\) Number of Parameters 2 3 4 Gumbel (GUM) Distribution The probability density function (PDF) of the Gumbel distribution is as follows: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma} \\exp \\left(-z_{i} - e^{-z_{i}}\\right) , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - z_{i} - e^{-z_{i}} \\right] \\] Normal (NOR) Distribution The probability density function (PDF) of the Normal distribution is as follows: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi }}e^{-z_{i}^2/2} , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln (\\sigma \\sqrt{2\\pi }) - \\frac{z_{i}^2}{2} \\right] \\] Log-Normal (LNO) Distribution To perform MLE using the LNO distribution, we use the fact that: \\[ \\text{Data} \\sim \\text{LNO} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{NOR} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LNO}}(x ; \\mu, \\sigma) = \\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma)}{x} \\] Three Parameter Distributions For a three-parameter distribution XXX , we use three different models: XXX is the standard model with no non-stationarity. XXX100 has a trend in the location parameter \\(\\mu\\) . XXX110 has a trend in the location \\(\\mu\\) and the scale \\(\\sigma\\) . Shown below is a table summarizing these three models: Feature XXX XXX100 XXX110 Location \\(\\mu\\) constant \\(\\mu_0 + \\mu_1z\\) \\(\\mu_0 + \\mu_1z\\) Scale \\(\\sigma\\) constant constant \\(\\sigma_0 + \\sigma_1z\\) Shape \\(\\kappa\\) constant constant constant Number of Parameters 3 4 5 Generalized Extreme Value (GEV) Distribution The probability density function (PDF) of the GEV distribution is as follows (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\] Generalized Logistic (GLO) Distribution The probability density function (PDF) of the GLO distribution is as follows (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{1}{\\sigma }t_{i}^{(1/\\kappa) - 1} \\left[1 + t_{i}^{1/\\kappa}\\right]^{-2}, \\quad t_{i} = 1 - \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma + \\left(\\frac{1}{\\kappa }-1\\right) \\ln t_{i} - 2 \\ln \\left(1 + t_{i}^{1/\\kappa }\\right) \\right] \\] Generalized Normal (GNO) Distribution The probability density function (PDF) of the GNO distribution is as follows (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{1}{\\sigma \\sqrt{2\\pi }} t_{i}^{-1} \\exp \\left[-\\frac{(\\ln t_{i})^2}{2\\kappa ^2}\\right], \\quad t_{i} = 1 - \\kappa \\left(\\frac{x - \\mu }{\\sigma }\\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[- \\ln (\\sigma \\sqrt{2\\pi }) - \\ln t_{i} - \\frac{(\\ln t_{i})^2}{2\\kappa ^2}\\right] \\] Pearson Type III (PE3) Distribution The probability density function (PDF) of the PE3 distribution is as follows: \\[ \\begin{aligned} f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{(x_{i} - \\xi)^{\\alpha - 1}e^{-(x_{i} - \\xi )/\\beta }}{\\beta ^{\\alpha } \\Gamma (\\alpha )} \\\\[5pt] \\alpha = \\frac{4}{\\kappa^2}, \\quad \\beta = \\frac{\\sigma |\\kappa|}{2}, \\quad \\xi = \\mu - \\frac{2\\sigma }{\\kappa } \\end{aligned} \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[(\\alpha - 1) \\ln |x_{i} - \\xi | - \\frac{|x_{i} - \\xi |}{\\beta } - \\alpha \\ln\\beta - \\ln \\Gamma (\\alpha )\\right] \\] Log-Pearson Type III (LP3) Distribution To perform MLE using the LP3 distribution, we use the fact that: \\[ \\text{Data} \\sim \\text{LP3} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{PE3} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LP3}}(x ; \\mu, \\sigma, \\kappa) = \\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa ) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa )}{x} \\] Weibull (WEI) Distribution The probability density function (PDF) of the Weibull distribution is as follows for \\(x_{i} > \\mu\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{\\kappa}{\\sigma }\\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa -1} \\exp \\left( - \\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa } \\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[\\ln \\kappa - \\kappa \\ln \\sigma +(\\kappa -1)\\ln (x_{i}-\\mu ) - \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) ^{\\kappa } \\right] \\] Generalized Maximum Likelihood (GMLE) The generalized maximum likelihood (GMLE) parameter estimation method is used to determine the parameters of the generalized extreme value (GEV) distribution given a prior distribution for the shape parameter \\(\\kappa\\) . This method uses maximum a posteriori estimation , which maximizes the product of the likelihood and the prior distribution. Suppose that \\(\\kappa\\) is drawn from a random variable \\(K \\sim \\text{Beta}(p, q)\\) where \\(p\\) and \\(q\\) are determined using prior knowledge. The prior PDF \\(f_{K}(\\kappa)\\) is shown below, where \\(B(p, q)\\) is the Beta function . \\[ f_{K}(\\kappa) = \\frac{\\kappa ^{p - 1}(1 - \\kappa)^{q-1}}{B(p, q)} \\] As in the case of regular maximum likelihood estimation, the likelihood function is: \\[ f_{X}(x : \\mu, \\sigma, \\kappa) =\\prod_{i=1}^{n} \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] As mentioned previously, we want to maximize the product \\(\\mathcal{L} = f_{K}(\\kappa)f_{X}(x:\\mu ,\\sigma ,\\kappa)\\) . To ensure numerical stability, we will maximize \\(\\ln (\\mathcal{L})\\) instead, which has the following form: \\[ \\begin{aligned} \\ln(\\mathcal{L}) &= \\ln(f_{K}(\\kappa)) + \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) \\\\[10pt] \\ln(f_{K}(\\kappa)) &= (p - 1)\\ln \\kappa + (q-1) \\ln (1 - \\kappa) - \\ln (B(p, q)) \\\\[5pt] \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) &= \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\end{aligned} \\]","title":"Parameter Estimation"},{"location":"parameter-estimation/#parameter-estimation","text":"The FFA framework implements three methods for parameter estimation: L-moments Maximum likelihood (MLE) Generalized maximum likelihood (GMLE) Note : The parameterization used for the GEV distributions is different from the parameterization used by the lmom library. In particular, the sign of the shape parameter is inverted for consistency with \"Regional Frequency Analysis\" (Hosking, 1997).","title":"Parameter Estimation"},{"location":"parameter-estimation/#l-moments","text":"The method estimates parameter values based on the sample L-moments \\(l_{1}\\) , \\(l_{2}\\) and the sample L-moment ratios \\(t_{3}\\) , \\(t_{4}\\) . For more information about L-moments, see here . The parameter estimation methods are based on these Fortran routines by J.R.M. Hosking. In particular, we use the lmom CRAN package, which implements the aformentioned Fotran routines. Warning : L-moments parameter estimation can yield distributions which do not have support at small values. This is generally not an issue, since we are only interested in the higher quantiles of the distrbution. However, it is important to remember that the probability distributions produced by L-moments should not be used to predict future streamflow data in general.","title":"L-Moments"},{"location":"parameter-estimation/#maximum-likelihood-mle","text":"For all distributions below, let \\(x\\) be a vector of data with elements \\(x_{1}, \\dots, x_{n}\\) .","title":"Maximum Likelihood (MLE)"},{"location":"parameter-estimation/#two-parameter-distributions","text":"For a two-parameter distribution XXX , we use three different models: XXX is the standard model with no non-stationarity. XXX10 has a trend in the location parameter \\(\\mu\\) . XXX11 has a trend in the location \\(\\mu\\) and the scale \\(\\sigma\\) . Shown below is a table summarizing these three models: Feature XXX XXX10 XXX11 Location \\(\\mu\\) constant \\(\\mu_0 + \\mu_1z\\) \\(\\mu_0 + \\mu_1z\\) Scale \\(\\sigma\\) constant constant \\(\\sigma_0 + \\sigma_1z\\) Number of Parameters 2 3 4","title":"Two Parameter Distributions"},{"location":"parameter-estimation/#gumbel-gum-distribution","text":"The probability density function (PDF) of the Gumbel distribution is as follows: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma} \\exp \\left(-z_{i} - e^{-z_{i}}\\right) , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - z_{i} - e^{-z_{i}} \\right] \\]","title":"Gumbel (GUM) Distribution"},{"location":"parameter-estimation/#normal-nor-distribution","text":"The probability density function (PDF) of the Normal distribution is as follows: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi }}e^{-z_{i}^2/2} , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln (\\sigma \\sqrt{2\\pi }) - \\frac{z_{i}^2}{2} \\right] \\]","title":"Normal (NOR) Distribution"},{"location":"parameter-estimation/#log-normal-lno-distribution","text":"To perform MLE using the LNO distribution, we use the fact that: \\[ \\text{Data} \\sim \\text{LNO} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{NOR} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LNO}}(x ; \\mu, \\sigma) = \\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma)}{x} \\]","title":"Log-Normal (LNO) Distribution"},{"location":"parameter-estimation/#three-parameter-distributions","text":"For a three-parameter distribution XXX , we use three different models: XXX is the standard model with no non-stationarity. XXX100 has a trend in the location parameter \\(\\mu\\) . XXX110 has a trend in the location \\(\\mu\\) and the scale \\(\\sigma\\) . Shown below is a table summarizing these three models: Feature XXX XXX100 XXX110 Location \\(\\mu\\) constant \\(\\mu_0 + \\mu_1z\\) \\(\\mu_0 + \\mu_1z\\) Scale \\(\\sigma\\) constant constant \\(\\sigma_0 + \\sigma_1z\\) Shape \\(\\kappa\\) constant constant constant Number of Parameters 3 4 5","title":"Three Parameter Distributions"},{"location":"parameter-estimation/#generalized-extreme-value-gev-distribution","text":"The probability density function (PDF) of the GEV distribution is as follows (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\]","title":"Generalized Extreme Value (GEV) Distribution"},{"location":"parameter-estimation/#generalized-logistic-glo-distribution","text":"The probability density function (PDF) of the GLO distribution is as follows (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{1}{\\sigma }t_{i}^{(1/\\kappa) - 1} \\left[1 + t_{i}^{1/\\kappa}\\right]^{-2}, \\quad t_{i} = 1 - \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma + \\left(\\frac{1}{\\kappa }-1\\right) \\ln t_{i} - 2 \\ln \\left(1 + t_{i}^{1/\\kappa }\\right) \\right] \\]","title":"Generalized Logistic (GLO) Distribution"},{"location":"parameter-estimation/#generalized-normal-gno-distribution","text":"The probability density function (PDF) of the GNO distribution is as follows (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{1}{\\sigma \\sqrt{2\\pi }} t_{i}^{-1} \\exp \\left[-\\frac{(\\ln t_{i})^2}{2\\kappa ^2}\\right], \\quad t_{i} = 1 - \\kappa \\left(\\frac{x - \\mu }{\\sigma }\\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[- \\ln (\\sigma \\sqrt{2\\pi }) - \\ln t_{i} - \\frac{(\\ln t_{i})^2}{2\\kappa ^2}\\right] \\]","title":"Generalized Normal (GNO) Distribution"},{"location":"parameter-estimation/#pearson-type-iii-pe3-distribution","text":"The probability density function (PDF) of the PE3 distribution is as follows: \\[ \\begin{aligned} f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{(x_{i} - \\xi)^{\\alpha - 1}e^{-(x_{i} - \\xi )/\\beta }}{\\beta ^{\\alpha } \\Gamma (\\alpha )} \\\\[5pt] \\alpha = \\frac{4}{\\kappa^2}, \\quad \\beta = \\frac{\\sigma |\\kappa|}{2}, \\quad \\xi = \\mu - \\frac{2\\sigma }{\\kappa } \\end{aligned} \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[(\\alpha - 1) \\ln |x_{i} - \\xi | - \\frac{|x_{i} - \\xi |}{\\beta } - \\alpha \\ln\\beta - \\ln \\Gamma (\\alpha )\\right] \\]","title":"Pearson Type III (PE3) Distribution"},{"location":"parameter-estimation/#log-pearson-type-iii-lp3-distribution","text":"To perform MLE using the LP3 distribution, we use the fact that: \\[ \\text{Data} \\sim \\text{LP3} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{PE3} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LP3}}(x ; \\mu, \\sigma, \\kappa) = \\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa ) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa )}{x} \\]","title":"Log-Pearson Type III (LP3) Distribution"},{"location":"parameter-estimation/#weibull-wei-distribution","text":"The probability density function (PDF) of the Weibull distribution is as follows for \\(x_{i} > \\mu\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{\\kappa}{\\sigma }\\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa -1} \\exp \\left( - \\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa } \\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[\\ln \\kappa - \\kappa \\ln \\sigma +(\\kappa -1)\\ln (x_{i}-\\mu ) - \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) ^{\\kappa } \\right] \\]","title":"Weibull (WEI) Distribution"},{"location":"parameter-estimation/#generalized-maximum-likelihood-gmle","text":"The generalized maximum likelihood (GMLE) parameter estimation method is used to determine the parameters of the generalized extreme value (GEV) distribution given a prior distribution for the shape parameter \\(\\kappa\\) . This method uses maximum a posteriori estimation , which maximizes the product of the likelihood and the prior distribution. Suppose that \\(\\kappa\\) is drawn from a random variable \\(K \\sim \\text{Beta}(p, q)\\) where \\(p\\) and \\(q\\) are determined using prior knowledge. The prior PDF \\(f_{K}(\\kappa)\\) is shown below, where \\(B(p, q)\\) is the Beta function . \\[ f_{K}(\\kappa) = \\frac{\\kappa ^{p - 1}(1 - \\kappa)^{q-1}}{B(p, q)} \\] As in the case of regular maximum likelihood estimation, the likelihood function is: \\[ f_{X}(x : \\mu, \\sigma, \\kappa) =\\prod_{i=1}^{n} \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] As mentioned previously, we want to maximize the product \\(\\mathcal{L} = f_{K}(\\kappa)f_{X}(x:\\mu ,\\sigma ,\\kappa)\\) . To ensure numerical stability, we will maximize \\(\\ln (\\mathcal{L})\\) instead, which has the following form: \\[ \\begin{aligned} \\ln(\\mathcal{L}) &= \\ln(f_{K}(\\kappa)) + \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) \\\\[10pt] \\ln(f_{K}(\\kappa)) &= (p - 1)\\ln \\kappa + (q-1) \\ln (1 - \\kappa) - \\ln (B(p, q)) \\\\[5pt] \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) &= \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\end{aligned} \\]","title":"Generalized Maximum Likelihood (GMLE)"},{"location":"roadmap/","text":"Roadmap CRAN Package API llv (i.e. llvgum or llvgev110 ): Computes the log-likelihood value. If the distribution is stationary, takes (data, params) . If the distribution is non-stationary, takes (data, params, years) . Makes call to llvxxx helper. gll (i.e. gllgev100 ): Computes the generalized log-likelihood given (data, params, prior) . If the distribution is stationary, takes (data, params, prior) . If the distribution is non-stationary, takes (data, params, prior, years) . Makes call to gllxxx helper. qnt (i.e. qntpe3 or qntlno11 ): Computes the quantile from an exceedance probability. If the distribution is stationary, takes (pe, params) If the distribution is non-stationary, takes (pe, params, year) lmr (i.e. lmrwei or lmrgno ): Computes the first four L-moments given (params) . pel (i.e. pellp3 or pelnor ): Computes the parameters given (lmom) Internal Helper Functions qntxxx : (model, pe, params, years) . Computes the quantile. llvxxx : (model, data, params) . All of the log-likelihood logic is here. gllxxx : (model, data, params, prior) . Call llvxxx , add additional term for the prior. lmrxxx : Implementations of lmom library. pelxxx : Implementations of lmom library. Todo Priority #1 : Make slides. Then, go through each function: Carefully write/edit/read documentation. Update the website with the contents of the file. Add additional tests to cover edge cases. Ensure all tests are passing. Preparing for CRAN: Ensure all tests are passing. Carefully edit documentation. Ensure CRAN check is passing. Update the list of functions below. Remove Rcpp dependency. Make the xxx functions fast, do argument validation in the distribution functions. Add support for custom plot labels using an optional argument. There is an issue in rfpl-uncertainty for the Weibull distribution: When finding the upper confidence interval we iteratively adjust yp up until f < 0 . This iterative process drives u upwards through the reparameterization. Sometimes the yp value required to get f < 0 causes data > u for a point. Then, the Weibull distribution has no support and it blows up. List of Functions Helper functions: mw.variance get.distributions get.covariates EDA: pettitt.test mks.test mk.test spearman.test bbmk.test pp.test kpss.test runs.test white.test sens.trend Likelihood Functions: likelihood generalized.likelihood fixed.likelihood reparameterized.likelihood FFA: ld.selection lk.selection z.selection lmom.estimation mle.estimation gmle.estimation sb.uncertainty rfpl.uncertainty rfgpl.uncertainty model.assessment Plotting: mks.plot bbmk.plot pettitt.plot runs.plot spearman.plot lmom.plot uncertainty.plot assessment.plot Command Line Interface Exploratory Data Analysis : Complete. Flood Frequency Analysis : In progress. API Development Link: Building APIs with R Need to learn: OpenAPI Swagger Docker For the web app, use Flask/HTMX/Alpine. https://api.weather.gc.ca/collections/hydrometric-annual-statistics Sites used for the paper: Application_1 : 07BE001 (Athabasca River at Athabasca) Application_2 : 08NH021 (Kootenai River at Porthill) Application_3_1 : 05BB001 (Bow River at Banff) Application_3_2 : 08MH016 (Chilliwack River at Chilliwack Lake) Application_3_3 : 08NM050 (Okanagan River at Penticton) Query for getting a station ( hydrometric-annual-statistics/items ) limit = 200 to get all the annual data skipGeometry = TRUE to ignore geographical data DATA_TYPE_EN = Discharge to get flows STATION_NUMBER set to the station number Get a list of stations with ( hydrometric-stations/items ) Web App Use Flaks/HTML/CSS with leaflet.js Further Research Read through papers on different NS metrics and possibly implement them: EWT: Olsen et al. 1998; Wigley, 2009 ENE: Parey et al. 2007; 2010 ERP: Katz et al. 2002 R: Read & Vogel, 2015; Salas & Obeysekera, 2014; Serinaldi & Kilsby, 2015 DLL: Rootzen & Katz, 2013 ADLL: Yan et al. 2017","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"roadmap/#cran-package","text":"","title":"CRAN Package"},{"location":"roadmap/#api","text":"llv (i.e. llvgum or llvgev110 ): Computes the log-likelihood value. If the distribution is stationary, takes (data, params) . If the distribution is non-stationary, takes (data, params, years) . Makes call to llvxxx helper. gll (i.e. gllgev100 ): Computes the generalized log-likelihood given (data, params, prior) . If the distribution is stationary, takes (data, params, prior) . If the distribution is non-stationary, takes (data, params, prior, years) . Makes call to gllxxx helper. qnt (i.e. qntpe3 or qntlno11 ): Computes the quantile from an exceedance probability. If the distribution is stationary, takes (pe, params) If the distribution is non-stationary, takes (pe, params, year) lmr (i.e. lmrwei or lmrgno ): Computes the first four L-moments given (params) . pel (i.e. pellp3 or pelnor ): Computes the parameters given (lmom)","title":"API"},{"location":"roadmap/#internal-helper-functions","text":"qntxxx : (model, pe, params, years) . Computes the quantile. llvxxx : (model, data, params) . All of the log-likelihood logic is here. gllxxx : (model, data, params, prior) . Call llvxxx , add additional term for the prior. lmrxxx : Implementations of lmom library. pelxxx : Implementations of lmom library.","title":"Internal Helper Functions"},{"location":"roadmap/#todo","text":"Priority #1 : Make slides. Then, go through each function: Carefully write/edit/read documentation. Update the website with the contents of the file. Add additional tests to cover edge cases. Ensure all tests are passing. Preparing for CRAN: Ensure all tests are passing. Carefully edit documentation. Ensure CRAN check is passing. Update the list of functions below. Remove Rcpp dependency. Make the xxx functions fast, do argument validation in the distribution functions. Add support for custom plot labels using an optional argument. There is an issue in rfpl-uncertainty for the Weibull distribution: When finding the upper confidence interval we iteratively adjust yp up until f < 0 . This iterative process drives u upwards through the reparameterization. Sometimes the yp value required to get f < 0 causes data > u for a point. Then, the Weibull distribution has no support and it blows up.","title":"Todo"},{"location":"roadmap/#list-of-functions","text":"Helper functions: mw.variance get.distributions get.covariates EDA: pettitt.test mks.test mk.test spearman.test bbmk.test pp.test kpss.test runs.test white.test sens.trend Likelihood Functions: likelihood generalized.likelihood fixed.likelihood reparameterized.likelihood FFA: ld.selection lk.selection z.selection lmom.estimation mle.estimation gmle.estimation sb.uncertainty rfpl.uncertainty rfgpl.uncertainty model.assessment Plotting: mks.plot bbmk.plot pettitt.plot runs.plot spearman.plot lmom.plot uncertainty.plot assessment.plot","title":"List of Functions"},{"location":"roadmap/#command-line-interface","text":"Exploratory Data Analysis : Complete. Flood Frequency Analysis : In progress.","title":"Command Line Interface"},{"location":"roadmap/#api-development","text":"Link: Building APIs with R Need to learn: OpenAPI Swagger Docker For the web app, use Flask/HTMX/Alpine. https://api.weather.gc.ca/collections/hydrometric-annual-statistics Sites used for the paper: Application_1 : 07BE001 (Athabasca River at Athabasca) Application_2 : 08NH021 (Kootenai River at Porthill) Application_3_1 : 05BB001 (Bow River at Banff) Application_3_2 : 08MH016 (Chilliwack River at Chilliwack Lake) Application_3_3 : 08NM050 (Okanagan River at Penticton) Query for getting a station ( hydrometric-annual-statistics/items ) limit = 200 to get all the annual data skipGeometry = TRUE to ignore geographical data DATA_TYPE_EN = Discharge to get flows STATION_NUMBER set to the station number Get a list of stations with ( hydrometric-stations/items )","title":"API Development"},{"location":"roadmap/#web-app","text":"Use Flaks/HTML/CSS with leaflet.js","title":"Web App"},{"location":"roadmap/#further-research","text":"Read through papers on different NS metrics and possibly implement them: EWT: Olsen et al. 1998; Wigley, 2009 ENE: Parey et al. 2007; 2010 ERP: Katz et al. 2002 R: Read & Vogel, 2015; Salas & Obeysekera, 2014; Serinaldi & Kilsby, 2015 DLL: Rootzen & Katz, 2013 ADLL: Yan et al. 2017","title":"Further Research"},{"location":"uncertainty-quantification/","text":"Uncertainty Quantification The FFA framework implements three methods for uncertainty quantification: Sample bootstrap Regula-Falsi profile likelihood (RFPL) Regula-Falsi generalized profile likelihood (RFGPL) Sample Bootstrap The sample bootstrap is a flexible method for uncertainty quantification that works with all probability models and parameter estimation methods. Let \\(n\\) be the size of the original dataset. Draw \\(N_{\\text{sim}}\\) bootstrap samples of size \\(n\\) from the selected probability distribution. Fit a probability distribution to each bootstrap sample using the same model selection method and parameter estimation method that was used to generate the original distribution. Compute the quantiles for each of the bootstrapped distributions. Generate confidence intervals using the mean and variance of the bootstrapped quantiles . Handling Non-Stationarity If the selected probability distribution is non-stationary, the quantiles for the bootstrapped distributions change in time. Therefore, the confidence intervals vary with time as well. Luckily, it is computationally inexpensive to determine the set of confidence intervals from the bootstrap distributions. Therefore, the FFA framework will report the confidence intervals for all years in the dataset by default when using the sample bootstrap quantification method. Regula-Falsi Profile Likelihood (RFPL) Consider a statistical model with parameters \\((\\theta, \\psi_{1}, \\dots, \\psi_{n})\\) . The Profile Likelihood for the scalar parameter \\(\\theta\\) and vector of nuisance parameters \\(\\psi\\) is defined as: \\[ \\ell_{p}(\\theta) = \\max_{\\psi } \\ell(\\theta , \\psi) \\] Let \\(\\hat{\\theta}\\) be MLE of \\(\\theta\\) . To find a confidence interval with significance \\(1-\\alpha\\) , we find the two solutions to the following equation (where \\(\\chi_{1;1-\\alpha}^2\\) is the \\(1-\\alpha\\) quantile of the Chi-squared distribution ): \\[ 2[\\ell_{p}(\\hat{\\theta }) - \\ell_{p}(\\theta )] = \\chi_{1;1-\\alpha }^2 \\] This is equivalent to finding the two points \\(\\theta_{L} < \\hat{\\theta} < \\theta_{U}\\) such that the profile log-likelihood has dropped by \\(\\chi _{1;1-\\alpha }^2 / 2\\) . To find \\(\\theta_{L}\\) and \\(\\theta_{U}\\) we find the roots of \\(f(\\theta)\\) using a secant-based algorithm. \\[ f(\\theta) = \\ell_{p}(\\theta) - \\left[\\ell_{p}(\\hat{\\theta}) - \\frac{\\chi_{1;1-\\alpha }^2}{2}\\right] \\] In the FFA framework, we compute the profile likelihood of each quantile \\(y\\) by reparameterizing the location parameter \\(\\mu\\) . Let \\(q(p, \\mu, \\psi)\\) be a function that takes an exceedance probability \\(p\\) , location parameter \\(\\mu\\) and nuisance parameters \\(\\psi\\) and returns a quantile \\(y\\) . All quantile functions satisfy: \\[ y = q(p, \\mu, \\psi) = \\mu + q(p, 0, \\psi) \\] Therefore, we can define \\(\\mu\\) as a function of \\((p, y, \\psi)\\) as shown below: \\[ \\mu = y - q(p, 0, \\psi) \\] We use this relationship to find the profile likelihood \\(\\ell_{p}(y)\\) by evaluating \\(\\mu(p, y, \\psi)\\) and substituting it into the log-likelihood functions listed here . Initialization Algorithm Before we can find the roots of \\(f\\) , we need to identify initial values for the regula-falsi algorithm: Let \\(a_{0}\\) be a number such that \\(a_{0} < y\\) and \\(f(a_{0}) < 0\\) . Let \\(b_{0}\\) be a number such that \\(b_{0} > y\\) and \\(f(b_{0}) < 0\\) . To find \\(a_{0}\\) , start by computing \\(f(a^{*})\\) for \\(a^{*} = 0.95y\\) . If \\(f(a^{*}) < 0\\) , then assign \\(a_{0} = a^{*}\\) . Otherwise, update \\(a^{*}\\) to \\(0.95a^{*}\\) until \\(f(a^{*}) < 0\\) . To find \\(b_{0}\\) , we use a similar process. However, instead of iteratively revising \\(b^{*}\\) down, we revise it up to \\(1.05b^{*}\\) . Iteration Algorithm At iteration \\(i\\) , compute the following: \\[ c_{i} = \\frac{a_{i-1}f(b_{i-1}) - b_{i-1}f(a_{i-1})}{f(b_{i-1}) - f(a_{i-1})} \\] Evaluate \\(\\ell_{p}(c_{i})\\) by maximizing over the nuisance parameters \\(\\psi\\) , then find \\(f(c_{i})\\) . If \\(|f(c_{i})| < \\epsilon\\) (where \\(\\epsilon\\) is small), then stop. \\(c_{i}\\) is the confidence interval bound. Otherwise, assign \\(a_{i} = c_{i}\\) if \\(f(c_{i}) < 0\\) and \\(b_{i} = c_{i}\\) if \\(f(c_{i}) > 0\\) and continue to iteration \\(i + 1\\) . Handling Non-Stationarity Under non-stationarity, the quantiles \\(y(t)\\) vary with time. Therefore, we must execute the RFPL algorithm individually for each timestamp of interest. This can be quite computationally expensive, so the FFA framework defaults to running the RFPL algorithm on the last year in the dataset. Regula-Falsi Generalized Profile Likelihood (RFGPL) The regula-falsi generalized profile likelihood (RFGPL) method performs the regula-falsi algorithm shown above on the GEV distributions with a \\(\\text{Beta}(p, q)\\) prior for the shape parameter \\(\\kappa\\) . For more information about generalized parameter estimation, see here . Handling Non-Stationarity See the section for the RFPL method.","title":"Uncertainty Quantification"},{"location":"uncertainty-quantification/#uncertainty-quantification","text":"The FFA framework implements three methods for uncertainty quantification: Sample bootstrap Regula-Falsi profile likelihood (RFPL) Regula-Falsi generalized profile likelihood (RFGPL)","title":"Uncertainty Quantification"},{"location":"uncertainty-quantification/#sample-bootstrap","text":"The sample bootstrap is a flexible method for uncertainty quantification that works with all probability models and parameter estimation methods. Let \\(n\\) be the size of the original dataset. Draw \\(N_{\\text{sim}}\\) bootstrap samples of size \\(n\\) from the selected probability distribution. Fit a probability distribution to each bootstrap sample using the same model selection method and parameter estimation method that was used to generate the original distribution. Compute the quantiles for each of the bootstrapped distributions. Generate confidence intervals using the mean and variance of the bootstrapped quantiles .","title":"Sample Bootstrap"},{"location":"uncertainty-quantification/#handling-non-stationarity","text":"If the selected probability distribution is non-stationary, the quantiles for the bootstrapped distributions change in time. Therefore, the confidence intervals vary with time as well. Luckily, it is computationally inexpensive to determine the set of confidence intervals from the bootstrap distributions. Therefore, the FFA framework will report the confidence intervals for all years in the dataset by default when using the sample bootstrap quantification method.","title":"Handling Non-Stationarity"},{"location":"uncertainty-quantification/#regula-falsi-profile-likelihood-rfpl","text":"Consider a statistical model with parameters \\((\\theta, \\psi_{1}, \\dots, \\psi_{n})\\) . The Profile Likelihood for the scalar parameter \\(\\theta\\) and vector of nuisance parameters \\(\\psi\\) is defined as: \\[ \\ell_{p}(\\theta) = \\max_{\\psi } \\ell(\\theta , \\psi) \\] Let \\(\\hat{\\theta}\\) be MLE of \\(\\theta\\) . To find a confidence interval with significance \\(1-\\alpha\\) , we find the two solutions to the following equation (where \\(\\chi_{1;1-\\alpha}^2\\) is the \\(1-\\alpha\\) quantile of the Chi-squared distribution ): \\[ 2[\\ell_{p}(\\hat{\\theta }) - \\ell_{p}(\\theta )] = \\chi_{1;1-\\alpha }^2 \\] This is equivalent to finding the two points \\(\\theta_{L} < \\hat{\\theta} < \\theta_{U}\\) such that the profile log-likelihood has dropped by \\(\\chi _{1;1-\\alpha }^2 / 2\\) . To find \\(\\theta_{L}\\) and \\(\\theta_{U}\\) we find the roots of \\(f(\\theta)\\) using a secant-based algorithm. \\[ f(\\theta) = \\ell_{p}(\\theta) - \\left[\\ell_{p}(\\hat{\\theta}) - \\frac{\\chi_{1;1-\\alpha }^2}{2}\\right] \\] In the FFA framework, we compute the profile likelihood of each quantile \\(y\\) by reparameterizing the location parameter \\(\\mu\\) . Let \\(q(p, \\mu, \\psi)\\) be a function that takes an exceedance probability \\(p\\) , location parameter \\(\\mu\\) and nuisance parameters \\(\\psi\\) and returns a quantile \\(y\\) . All quantile functions satisfy: \\[ y = q(p, \\mu, \\psi) = \\mu + q(p, 0, \\psi) \\] Therefore, we can define \\(\\mu\\) as a function of \\((p, y, \\psi)\\) as shown below: \\[ \\mu = y - q(p, 0, \\psi) \\] We use this relationship to find the profile likelihood \\(\\ell_{p}(y)\\) by evaluating \\(\\mu(p, y, \\psi)\\) and substituting it into the log-likelihood functions listed here .","title":"Regula-Falsi Profile Likelihood (RFPL)"},{"location":"uncertainty-quantification/#initialization-algorithm","text":"Before we can find the roots of \\(f\\) , we need to identify initial values for the regula-falsi algorithm: Let \\(a_{0}\\) be a number such that \\(a_{0} < y\\) and \\(f(a_{0}) < 0\\) . Let \\(b_{0}\\) be a number such that \\(b_{0} > y\\) and \\(f(b_{0}) < 0\\) . To find \\(a_{0}\\) , start by computing \\(f(a^{*})\\) for \\(a^{*} = 0.95y\\) . If \\(f(a^{*}) < 0\\) , then assign \\(a_{0} = a^{*}\\) . Otherwise, update \\(a^{*}\\) to \\(0.95a^{*}\\) until \\(f(a^{*}) < 0\\) . To find \\(b_{0}\\) , we use a similar process. However, instead of iteratively revising \\(b^{*}\\) down, we revise it up to \\(1.05b^{*}\\) .","title":"Initialization Algorithm"},{"location":"uncertainty-quantification/#iteration-algorithm","text":"At iteration \\(i\\) , compute the following: \\[ c_{i} = \\frac{a_{i-1}f(b_{i-1}) - b_{i-1}f(a_{i-1})}{f(b_{i-1}) - f(a_{i-1})} \\] Evaluate \\(\\ell_{p}(c_{i})\\) by maximizing over the nuisance parameters \\(\\psi\\) , then find \\(f(c_{i})\\) . If \\(|f(c_{i})| < \\epsilon\\) (where \\(\\epsilon\\) is small), then stop. \\(c_{i}\\) is the confidence interval bound. Otherwise, assign \\(a_{i} = c_{i}\\) if \\(f(c_{i}) < 0\\) and \\(b_{i} = c_{i}\\) if \\(f(c_{i}) > 0\\) and continue to iteration \\(i + 1\\) .","title":"Iteration Algorithm"},{"location":"uncertainty-quantification/#handling-non-stationarity_1","text":"Under non-stationarity, the quantiles \\(y(t)\\) vary with time. Therefore, we must execute the RFPL algorithm individually for each timestamp of interest. This can be quite computationally expensive, so the FFA framework defaults to running the RFPL algorithm on the last year in the dataset.","title":"Handling Non-Stationarity"},{"location":"uncertainty-quantification/#regula-falsi-generalized-profile-likelihood-rfgpl","text":"The regula-falsi generalized profile likelihood (RFGPL) method performs the regula-falsi algorithm shown above on the GEV distributions with a \\(\\text{Beta}(p, q)\\) prior for the shape parameter \\(\\kappa\\) . For more information about generalized parameter estimation, see here .","title":"Regula-Falsi Generalized Profile Likelihood (RFGPL)"},{"location":"uncertainty-quantification/#handling-non-stationarity_2","text":"See the section for the RFPL method.","title":"Handling Non-Stationarity"}]}