{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FFA Framework Wiki The FFA Framework is an open-source tool for flood frequency analysis (FFA), designed to support systematic and repeatable workflows for stationary (S-FFA) and nonstationary (NS-FFA) analysis. It is developed at the University of Calgary and the University of Saskatchewan, Canada, and is freely available to the public. The framework is available as an R package . The original version was released as a stand-alone GUI and MATLAB source code, published in Vidrio-Sahag\u00fan et al. (2024) . For details on changes from the MATLAB version, see this page . Getting Started R Package : Installation Instructions \u00b7 Documentation CLI : Installation Instructions \u00b7 User manual Additional Resources Project Goals Development Roadmap Release Notes Warning : The FFA Framework is currently under development. Code and documentation may change frequently and drastically. Use at your own risk.","title":"Home"},{"location":"#ffa-framework-wiki","text":"The FFA Framework is an open-source tool for flood frequency analysis (FFA), designed to support systematic and repeatable workflows for stationary (S-FFA) and nonstationary (NS-FFA) analysis. It is developed at the University of Calgary and the University of Saskatchewan, Canada, and is freely available to the public. The framework is available as an R package . The original version was released as a stand-alone GUI and MATLAB source code, published in Vidrio-Sahag\u00fan et al. (2024) . For details on changes from the MATLAB version, see this page .","title":"FFA Framework Wiki"},{"location":"#getting-started","text":"R Package : Installation Instructions \u00b7 Documentation CLI : Installation Instructions \u00b7 User manual","title":"Getting Started"},{"location":"#additional-resources","text":"Project Goals Development Roadmap Release Notes Warning : The FFA Framework is currently under development. Code and documentation may change frequently and drastically. Use at your own risk.","title":"Additional Resources"},{"location":"cli-getting-started/","text":"Getting Started To begin, install the CLI by following the installation instructions . The FFA Framework CLI is a single script: ffaframework.R . It performs exploratory data analysis and/or flood frequency analysis (FFA) on one or more datasets. It can also generate reports in Markdown, PDF, HTML, or JSON formats. Configuration is handled through the config.yml file. For more information, see the user manual . Folder Structure The /data folder contains input CSV files for the CLI. The following files are included by default: Application_1.csv : Station 07BE001 (Athabasca River at Athabasca) Application_2.csv : Station 08NH021 (Kootenai River at Porthill) Application_3_1.csv : Station 05BB001 (Bow River at Banff) Application_3_2.csv : Station 08MH016 (Chilliwack River at Chilliwack Lake) Application_3_3.csv : Station 08NM050 (Okanagan River at Penticton) Application_4.csv : Station 05BA001 (Bow River at Lake Louise) Application_5.csv : Station 08NM116 (Mission Creek near East Kelowna) The /reports folder stores output reports and figures generated by the CLI. Trying it out! To test the framework, navigate to the ffa-framework folder in your terminal and run the command Rscript ffaframework.R . By default, the CLI uses the dataset Application_1.csv . What Next? We recommend exploring the \"Exploratory Data Analysis\" and \"Flood Frequency Analysis\" sections in the sidebar to gain a better understanding of the statistical methods used in the framework. Once you're familiar with the framework, try running an analysis on another example dataset!","title":"Getting Started"},{"location":"cli-getting-started/#getting-started","text":"To begin, install the CLI by following the installation instructions . The FFA Framework CLI is a single script: ffaframework.R . It performs exploratory data analysis and/or flood frequency analysis (FFA) on one or more datasets. It can also generate reports in Markdown, PDF, HTML, or JSON formats. Configuration is handled through the config.yml file. For more information, see the user manual .","title":"Getting Started"},{"location":"cli-getting-started/#folder-structure","text":"The /data folder contains input CSV files for the CLI. The following files are included by default: Application_1.csv : Station 07BE001 (Athabasca River at Athabasca) Application_2.csv : Station 08NH021 (Kootenai River at Porthill) Application_3_1.csv : Station 05BB001 (Bow River at Banff) Application_3_2.csv : Station 08MH016 (Chilliwack River at Chilliwack Lake) Application_3_3.csv : Station 08NM050 (Okanagan River at Penticton) Application_4.csv : Station 05BA001 (Bow River at Lake Louise) Application_5.csv : Station 08NM116 (Mission Creek near East Kelowna) The /reports folder stores output reports and figures generated by the CLI.","title":"Folder Structure"},{"location":"cli-getting-started/#trying-it-out","text":"To test the framework, navigate to the ffa-framework folder in your terminal and run the command Rscript ffaframework.R . By default, the CLI uses the dataset Application_1.csv .","title":"Trying it out!"},{"location":"cli-getting-started/#what-next","text":"We recommend exploring the \"Exploratory Data Analysis\" and \"Flood Frequency Analysis\" sections in the sidebar to gain a better understanding of the statistical methods used in the framework. Once you're familiar with the framework, try running an analysis on another example dataset!","title":"What Next?"},{"location":"cli-installation-instructions/","text":"Installing the CLI Prerequisites Install Git : https://git-scm.com/downloads . Install R (v4.5.0 recommended): https://www.r-project.org/ . Install Pandoc : https://pandoc.org/ . Verify that the installations were successful by executing the following commands in a shell: git --version R --version pandoc --version Note for Windows Users You may need to add Git, R and Pandoc to your PATH to run them from the command line. The default paths for Git, R and Pandoc are: C:\\Program Files\\Git\\bin C:\\Program Files\\R\\R-4.5.0\\bin C:\\Program Files\\Pandoc To update your PATH , edit your system environment variables from the settings menu. Instructions First, install the FFA Framework R package . For details, see the R package installation guide . Step-by-Step Guide Open a shell (terminal) and navigate to the directory where you want to install the framework. Clone the GitHub FFA Framework repository by running the following command in a shell: git clone https://github.com/rileywheadon/ffa-framework.git Navigate to the ffa-framework directory (e.g., cd ffa-framework ). This directory contains a renv.lock file, which is used to manage R package dependencies. Launch a command prompt with R. The renv package may install itself automatically. If it does not, run install.packages(\"renv\") to install it manually. Load libraries from the renv.lock file by executing renv::restore() . Install the FFA Framework R package using devtools.install(\"path/to/ffa-package\") . You will need to replace \"path/to/ffa-package\" with the actual path to the package directory. Exit the command prompt with q() . This guide has been tested on Linux, Windows, and MacOS. Please submit a Github Issue if you have any problems.","title":"Installing the CLI"},{"location":"cli-installation-instructions/#installing-the-cli","text":"","title":"Installing the CLI"},{"location":"cli-installation-instructions/#prerequisites","text":"Install Git : https://git-scm.com/downloads . Install R (v4.5.0 recommended): https://www.r-project.org/ . Install Pandoc : https://pandoc.org/ . Verify that the installations were successful by executing the following commands in a shell: git --version R --version pandoc --version","title":"Prerequisites"},{"location":"cli-installation-instructions/#note-for-windows-users","text":"You may need to add Git, R and Pandoc to your PATH to run them from the command line. The default paths for Git, R and Pandoc are: C:\\Program Files\\Git\\bin C:\\Program Files\\R\\R-4.5.0\\bin C:\\Program Files\\Pandoc To update your PATH , edit your system environment variables from the settings menu.","title":"Note for Windows Users"},{"location":"cli-installation-instructions/#instructions","text":"First, install the FFA Framework R package . For details, see the R package installation guide .","title":"Instructions"},{"location":"cli-installation-instructions/#step-by-step-guide","text":"Open a shell (terminal) and navigate to the directory where you want to install the framework. Clone the GitHub FFA Framework repository by running the following command in a shell: git clone https://github.com/rileywheadon/ffa-framework.git Navigate to the ffa-framework directory (e.g., cd ffa-framework ). This directory contains a renv.lock file, which is used to manage R package dependencies. Launch a command prompt with R. The renv package may install itself automatically. If it does not, run install.packages(\"renv\") to install it manually. Load libraries from the renv.lock file by executing renv::restore() . Install the FFA Framework R package using devtools.install(\"path/to/ffa-package\") . You will need to replace \"path/to/ffa-package\" with the actual path to the package directory. Exit the command prompt with q() . This guide has been tested on Linux, Windows, and MacOS. Please submit a Github Issue if you have any problems.","title":"Step-by-Step Guide"},{"location":"cli-user-manual/","text":"User Manual The ffaframework.R script runs the complete EDA and FFA frameworks. Configuration Reference The configuration is stored in the config.yml file using the YAML markup language. Data Preparation data_source : Character (1); method for sourcing data for the framework. Must be one of: \"Local\" : Source data locally using the csv_files option. \"GeoMet\" : Pull data from the MSC GeoMet API using the station_ids option. csv_files : Character; CSV files located in /data used to run the framework. station_ids : Character; station IDs for hydrological monitoring stations. You can search for station IDs by name, province, drainage basin, and location here . Some stations may have little or no data. EDA split_selection : Character (1); method for determining split points. Must be one of: \"Automatic\" : Identify split points and split the data automatically. \"Manual\" : Confirm split points after identification. \"Preset\" : Set the split points ahead of time using split_points . split_points : Integer; preset values for split points or null for no split points. significance_level : Numeric (1); significance level. Must be between 0.01 and 0.1 . bbmk_samples : Integer (1); number of bootstrap samples to use for the BB-MK Test . Distribution Selection distribution_selection : Character (1); distribution selection method: \"L-distance\" : Euclidian distance from (L-skewness, L-kurtosis) point. \"L-kurtosis\" : Difference between theoretical and sample L-kurtosis. \"L-statistic\" : Bootstrapped Z-statistic computed using the Kappa distribution. \"Preset\" : Specify a distribution ahead of time with the distribution_name option. distribution_name : Character (1); name of the probability distributions to use. z_samples : Integer (1); number of bootstrap samples for Z-statistic selection . Parameter Estimation s_estimation : Character (1); parameter estimation method for stationary models: \"L-moments\" : Method of L-moments using formulas from Hosking (1997). \"MLE\" : Maximum likelihood estimation. \"GMLE\" : Generalized maximum likelihood estimation (GEV distribution only). ns_estimation : Character (1); parameter estimation method for nonstationary models: \"MLE\" : Maximum likelihood estimation. \"GMLE\" : Generalized maximum likelihood estimation (GEV distribution only). gev_prior : Numeric (2); prior parameters for the shape parameter of the GEV distribution. If you are using maximum likelihood estimation without a prior, set this option to null . Uncertainty Quantification s_uncertainty : Character (1); uncertainty quantification method for stationary models: \"Bootstrap\" : Parametric bootstrap method. \"RFPL\" : Regula-falsi profile likelihood (MLE estimation only). \"RFGPL\" : Generalized regula-falsi profile likelihood (GMLE estimation only). ns_uncertainty : Character (1); uncertainty quantification method for nonstationary models: \"Bootstrap\" : Parametric bootstrap method. \"RFPL\" : Regula-falsi profile likelihood (MLE estimation only). \"RFGPL\" : Generalized regula-falsi profile likelihood (GMLE estimation only). return_periods : Numeric; list of return periods (in years) for estimating return levels. sb_samples : Integer (1); number of samples for bootstrap uncertainty quantification . rfpl_tolerance : Numeric (1); log-likelihood tolerance for RFPL uncertainty quantification . Model Assessment pp_formula : Character (1); plotting position formula for model assessment . Must be one of: \"Weibull\" : \\(i / (n + 1)\\) \"Blom\" : \\((i - 0.375) / (n + 0.25)\\) \"Cunnane\" : \\((i - 0.4) / (n + 0.2)\\) \"Gringorten\" : \\((i - 0.44) / (n + 0.12)\\) \"Hazen\" : \\((i - 0.5) / n\\) Plot Generation show_trend : Boolean (1); if true , add a trend line through the AMS data where applicable. slices : Integer; years used to estimate return levels for nonstationary models. Slices outside of the dataset (or trend-stationary partition) will be ignored. Report Generation generate_report : Boolean (1); if true , generate a report in .html format.","title":"User Manual"},{"location":"cli-user-manual/#user-manual","text":"The ffaframework.R script runs the complete EDA and FFA frameworks.","title":"User Manual"},{"location":"cli-user-manual/#configuration-reference","text":"The configuration is stored in the config.yml file using the YAML markup language.","title":"Configuration Reference"},{"location":"cli-user-manual/#data-preparation","text":"data_source : Character (1); method for sourcing data for the framework. Must be one of: \"Local\" : Source data locally using the csv_files option. \"GeoMet\" : Pull data from the MSC GeoMet API using the station_ids option. csv_files : Character; CSV files located in /data used to run the framework. station_ids : Character; station IDs for hydrological monitoring stations. You can search for station IDs by name, province, drainage basin, and location here . Some stations may have little or no data.","title":"Data Preparation"},{"location":"cli-user-manual/#eda","text":"split_selection : Character (1); method for determining split points. Must be one of: \"Automatic\" : Identify split points and split the data automatically. \"Manual\" : Confirm split points after identification. \"Preset\" : Set the split points ahead of time using split_points . split_points : Integer; preset values for split points or null for no split points. significance_level : Numeric (1); significance level. Must be between 0.01 and 0.1 . bbmk_samples : Integer (1); number of bootstrap samples to use for the BB-MK Test .","title":"EDA"},{"location":"cli-user-manual/#distribution-selection","text":"distribution_selection : Character (1); distribution selection method: \"L-distance\" : Euclidian distance from (L-skewness, L-kurtosis) point. \"L-kurtosis\" : Difference between theoretical and sample L-kurtosis. \"L-statistic\" : Bootstrapped Z-statistic computed using the Kappa distribution. \"Preset\" : Specify a distribution ahead of time with the distribution_name option. distribution_name : Character (1); name of the probability distributions to use. z_samples : Integer (1); number of bootstrap samples for Z-statistic selection .","title":"Distribution Selection"},{"location":"cli-user-manual/#parameter-estimation","text":"s_estimation : Character (1); parameter estimation method for stationary models: \"L-moments\" : Method of L-moments using formulas from Hosking (1997). \"MLE\" : Maximum likelihood estimation. \"GMLE\" : Generalized maximum likelihood estimation (GEV distribution only). ns_estimation : Character (1); parameter estimation method for nonstationary models: \"MLE\" : Maximum likelihood estimation. \"GMLE\" : Generalized maximum likelihood estimation (GEV distribution only). gev_prior : Numeric (2); prior parameters for the shape parameter of the GEV distribution. If you are using maximum likelihood estimation without a prior, set this option to null .","title":"Parameter Estimation"},{"location":"cli-user-manual/#uncertainty-quantification","text":"s_uncertainty : Character (1); uncertainty quantification method for stationary models: \"Bootstrap\" : Parametric bootstrap method. \"RFPL\" : Regula-falsi profile likelihood (MLE estimation only). \"RFGPL\" : Generalized regula-falsi profile likelihood (GMLE estimation only). ns_uncertainty : Character (1); uncertainty quantification method for nonstationary models: \"Bootstrap\" : Parametric bootstrap method. \"RFPL\" : Regula-falsi profile likelihood (MLE estimation only). \"RFGPL\" : Generalized regula-falsi profile likelihood (GMLE estimation only). return_periods : Numeric; list of return periods (in years) for estimating return levels. sb_samples : Integer (1); number of samples for bootstrap uncertainty quantification . rfpl_tolerance : Numeric (1); log-likelihood tolerance for RFPL uncertainty quantification .","title":"Uncertainty Quantification"},{"location":"cli-user-manual/#model-assessment","text":"pp_formula : Character (1); plotting position formula for model assessment . Must be one of: \"Weibull\" : \\(i / (n + 1)\\) \"Blom\" : \\((i - 0.375) / (n + 0.25)\\) \"Cunnane\" : \\((i - 0.4) / (n + 0.2)\\) \"Gringorten\" : \\((i - 0.44) / (n + 0.12)\\) \"Hazen\" : \\((i - 0.5) / n\\)","title":"Model Assessment"},{"location":"cli-user-manual/#plot-generation","text":"show_trend : Boolean (1); if true , add a trend line through the AMS data where applicable. slices : Integer; years used to estimate return levels for nonstationary models. Slices outside of the dataset (or trend-stationary partition) will be ignored.","title":"Plot Generation"},{"location":"cli-user-manual/#report-generation","text":"generate_report : Boolean (1); if true , generate a report in .html format.","title":"Report Generation"},{"location":"eda-change-points/","text":"Change Point Detection The EDA module of the FFA Framework includes two statistical tests for detecting change points in annual maximum series (AMS) data: the Mann-Kendall-Sneyers (MKS) test and the Pettitt test . Mann-Kendall-Sneyers Test The Mann-Kendall-Sneyers (MKS) test detects the beginning of a trend in a time series: Null hypothesis: There are no change points. Alternative hypothesis: There are one or more change points. Define \\(\\mathbb{I}(y_{i} > y_{j})\\) to be \\(1\\) if \\(y_{i} > y_{j}\\) and \\(0\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , we compute the progressive series \\(S^{F}_{t}\\) : \\[ S^{F}_{t} = \\sum_{i=i}^{t} \\sum_{j=1}^{i-1} \\mathbb{I}(y_{i} > y_{j}) \\] Next, we reverse the time series \\(y\\) . This gives us a new time series \\(y'\\) such that \\(y_{i}' = y_{n+1-i}\\) . Then we compute the regressive series \\(S^{B}_{t}\\) , where \\(\\text{rev}()\\) indicates that the vector is reversed: \\[ S^{B}_{t} = \\text{rev}\\left( \\sum_{i=i}^{t} \\sum_{j=1}^{i-1} \\mathbb{I}(y'_{i} > y'_{j})\\right) \\] Then, we compute the normalized progressive series \\(UF_{t}\\) and normalized regressive series \\(UB_{t}\\) : \\[ UF_{t} = \\frac{S^{F}_{t} - \\mathbb{E}[S^{F}_{t}]}{\\sqrt{\\text{Var}\\,(S^{F}_{t})}}, \\quad UB_{t} = \\frac{S^{B}_{t} - \\mathbb{E}[S^{B}_{t}]}{\\sqrt{\\text{Var}\\,(S^{B}_{t})}} \\] For both the progressive and regressive series, the expectation and variance are as follows: \\[ \\mathbb{E}[S^{F}_{t}] = \\mathbb{E}[S^{B}_{t}] = \\frac{t(t-1)}{4}, \\quad \\text{Var}(S^{F}_{t}) = \\text{Var}(S^{B}_{t}) = \\frac{t(t-1)(2t+5)}{72} \\] Finally, we plot \\(UF_{t}\\) and \\(UB_{t}\\) with confidence bounds at the \\(\\alpha/2\\) and \\(1 - (\\alpha /2)\\) quantiles of the standard normal distribution, where \\(\\alpha\\) is the chosen significance level. A crossing point between \\(UF_{t}\\) and \\(UB_{t}\\) that lies outside the confidence bounds is a potential change point. Example Plot Pettitt Test The Pettitt test detects abrupt changes in the mean of a time series. Null hypothesis: There are no abrupt changes. Alternative hypothesis: There is one abrupt change. Define \\(\\text{sign}(x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , compute the following test statistic: \\[ U_{t} = \\sum_{i=1}^{t} \\sum_{j=t+1}^{n} \\text{sign} (y_{j} - y_{i}), \\quad K = \\max_{t}|U_{t}| \\] The value of \\(t\\) such that \\(U_{t} = K\\) is a potential change point . The p-value of the potential change point can be approximated using the following formula for a one-sided test: \\[ p \\approx \\exp \\left(-\\frac{6K^2}{n^3 + n^2}\\right) \\] If the p-value is less than the significance level \\(\\alpha\\) , we reject the null hypothesis and conclude that there is evidence for an abrupt change in the mean at the potential change point. Example Plot","title":"Change Points"},{"location":"eda-change-points/#change-point-detection","text":"The EDA module of the FFA Framework includes two statistical tests for detecting change points in annual maximum series (AMS) data: the Mann-Kendall-Sneyers (MKS) test and the Pettitt test .","title":"Change Point Detection"},{"location":"eda-change-points/#mann-kendall-sneyers-test","text":"The Mann-Kendall-Sneyers (MKS) test detects the beginning of a trend in a time series: Null hypothesis: There are no change points. Alternative hypothesis: There are one or more change points. Define \\(\\mathbb{I}(y_{i} > y_{j})\\) to be \\(1\\) if \\(y_{i} > y_{j}\\) and \\(0\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , we compute the progressive series \\(S^{F}_{t}\\) : \\[ S^{F}_{t} = \\sum_{i=i}^{t} \\sum_{j=1}^{i-1} \\mathbb{I}(y_{i} > y_{j}) \\] Next, we reverse the time series \\(y\\) . This gives us a new time series \\(y'\\) such that \\(y_{i}' = y_{n+1-i}\\) . Then we compute the regressive series \\(S^{B}_{t}\\) , where \\(\\text{rev}()\\) indicates that the vector is reversed: \\[ S^{B}_{t} = \\text{rev}\\left( \\sum_{i=i}^{t} \\sum_{j=1}^{i-1} \\mathbb{I}(y'_{i} > y'_{j})\\right) \\] Then, we compute the normalized progressive series \\(UF_{t}\\) and normalized regressive series \\(UB_{t}\\) : \\[ UF_{t} = \\frac{S^{F}_{t} - \\mathbb{E}[S^{F}_{t}]}{\\sqrt{\\text{Var}\\,(S^{F}_{t})}}, \\quad UB_{t} = \\frac{S^{B}_{t} - \\mathbb{E}[S^{B}_{t}]}{\\sqrt{\\text{Var}\\,(S^{B}_{t})}} \\] For both the progressive and regressive series, the expectation and variance are as follows: \\[ \\mathbb{E}[S^{F}_{t}] = \\mathbb{E}[S^{B}_{t}] = \\frac{t(t-1)}{4}, \\quad \\text{Var}(S^{F}_{t}) = \\text{Var}(S^{B}_{t}) = \\frac{t(t-1)(2t+5)}{72} \\] Finally, we plot \\(UF_{t}\\) and \\(UB_{t}\\) with confidence bounds at the \\(\\alpha/2\\) and \\(1 - (\\alpha /2)\\) quantiles of the standard normal distribution, where \\(\\alpha\\) is the chosen significance level. A crossing point between \\(UF_{t}\\) and \\(UB_{t}\\) that lies outside the confidence bounds is a potential change point.","title":"Mann-Kendall-Sneyers Test"},{"location":"eda-change-points/#example-plot","text":"","title":"Example Plot"},{"location":"eda-change-points/#pettitt-test","text":"The Pettitt test detects abrupt changes in the mean of a time series. Null hypothesis: There are no abrupt changes. Alternative hypothesis: There is one abrupt change. Define \\(\\text{sign}(x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , compute the following test statistic: \\[ U_{t} = \\sum_{i=1}^{t} \\sum_{j=t+1}^{n} \\text{sign} (y_{j} - y_{i}), \\quad K = \\max_{t}|U_{t}| \\] The value of \\(t\\) such that \\(U_{t} = K\\) is a potential change point . The p-value of the potential change point can be approximated using the following formula for a one-sided test: \\[ p \\approx \\exp \\left(-\\frac{6K^2}{n^3 + n^2}\\right) \\] If the p-value is less than the significance level \\(\\alpha\\) , we reject the null hypothesis and conclude that there is evidence for an abrupt change in the mean at the potential change point.","title":"Pettitt Test"},{"location":"eda-change-points/#example-plot_1","text":"","title":"Example Plot"},{"location":"eda-introduction/","text":"Introduction to Exploratory Data Analysis (EDA) The EDA module of the FFA Framework is used to evaluate whether the available evidence supports the assumption of stationarity. To do this, the EDA module applies a structured sequence of statistical tests to the AMS data to detect statistically significant nonstationary signatures. These statistical tests serve three purposes: Detect change points (i.e., abrupt shifts or trend changes). Detect trends in the mean and identify them as deterministic/stochastic, linear/non-linear. Detect trends in the variability (i.e., heteroskedasticity and trends in standard deviation). The primary goal of EDA is to inform the choice between stationary and nonstationary FFA.","title":"Introduction"},{"location":"eda-introduction/#introduction-to-exploratory-data-analysis-eda","text":"The EDA module of the FFA Framework is used to evaluate whether the available evidence supports the assumption of stationarity. To do this, the EDA module applies a structured sequence of statistical tests to the AMS data to detect statistically significant nonstationary signatures. These statistical tests serve three purposes: Detect change points (i.e., abrupt shifts or trend changes). Detect trends in the mean and identify them as deterministic/stochastic, linear/non-linear. Detect trends in the variability (i.e., heteroskedasticity and trends in standard deviation). The primary goal of EDA is to inform the choice between stationary and nonstationary FFA.","title":"Introduction to Exploratory Data Analysis (EDA)"},{"location":"eda-trend-ams-mean/","text":"Detecting and Characterizing Trends in the AMS Mean This section describes the statistical tests (listed in alphabetical order) used to detect and characterizesignificant trends in the mean of the annual maximum series (AMS). These tests help identify a trend, identify autocorrelation, and determine whether a trend is deterministic/stochastic and linear/non-linear. BB-MK Test The Block Bootstrap Mann-Kendall (BB-MK) Test assesses the presence of a statistically significant monotonic trend in a time series. The BB-MK test is insensitive to autocorrelation , which is known to produce false positives in the MK test . Null hypothesis: No monotonic trend. Alternative hypothesis: A monotonic upward or downward trend exists. To conduct the BB-MK test, we rely on the results of the MK test and the Spearman autocorrelation test. Steps Compute the MK test statistic (see below). Use the Spearman test (see below) to identify the least insignificant lag \\(k\\) . Resample the time series in blocks of size \\(k+1\\) without replacement. Compute the MK test statistic for each bootstrapped sample. Derive the empirical distribution of the MK test statistic from the bootstrapped statistics. Estimate the significance of the observed test statistic using the empirical distribution. Example Plot KPSS Test The KPSS Test determines whether an autoregressive time series has a unit root . This test helps assess if the time series has a deterministic linear trend. Null hypothesis: The time series has a deterministic linear trend. Alternative hypothesis: The time series has a unit root (stochastic trend). The autoregressive time series shown below has a unit root if \\(\\sigma_{v}^2 > 0\\) : \\[ \\begin{align} y_{t} &= \\mu_{t} + \\beta t + \\epsilon_{t} \\\\[5pt] \\mu_{t} &= \\mu_{t-1} + v_{t} \\\\[5pt] v_{t} &\\sim \\mathcal{N}(0, \\sigma_{v}^2) \\end{align} \\] where: \\(\\mu_{t}\\) is the drift , or the deviation of \\(y_{t}\\) from \\(0\\) . Under the null hypothesis, \\(\\mu_{t}\\) is constant (since \\(v_{t}\\) is constant). Under the alternative hypothesis, \\(\\mu_t\\) is a stochastic process with a unit root. \\(\\beta t\\) is a linear trend , which represents deterministic nonstationarity (e.g., climate change). \\(\\epsilon_{t}\\) is stationary noise , corresponding to reversible fluctuations in \\(y_{t}\\) . In hydrology, \\(\\epsilon_{t}\\) represents fluctuations in streamflow due to natural variability. \\(v_{t}\\) is random walk innovation , or irreversible fluctuations in \\(\\mu_{t}\\) . Steps Fit a linear model to \\(y_{t}\\) and get the residuals \\(\\hat{r}_{t}\\) . Compute the cumulative partial-sum statistics \\(S_{k}\\) using the following formula: \\[ S_{k} = \\sum_{t=1}^{k} \\hat{r}_{t} \\] Under the null hypothesis, \\(S_{k}\\) will behave like a random walk with finite variance. If \\(y_{t}\\) has a unit root, then the sums will \"drift\" too much. Estimate the long-run variance of the time series using a Newey-West estimator : \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_0 + 2 \\sum_{j=1}^{q} \\left(1 - \\frac{j}{q+1} \\right) \\hat{\\gamma}_j \\] Where \\(q = \\left\\lfloor \\frac{3\\sqrt{n}}{13} \\right\\rfloor\\) and each autocovariance \\(\\hat{\\gamma}_j\\) is: \\[ \\hat{\\gamma}_j = \\frac{1}{n} \\sum_{t = j+1}^{n} \\hat{r}_t \\hat{r}_{t-j} \\] Compute the test statistic \\(z_{K}\\) : \\[ z_{K} = \\frac{1}{n^2\\hat{\\lambda }^2}\\sum_{k=1}^{n} S_{k}^2 \\] Since the test statistic \\(z_{K}\\) is non-normally distributed, we compute the p-value by interpolating the table of quantiles from Hobjin et al. (2004) shown below. \\(q\\) 0.90 0.95 0.975 0.99 Statistic 0.119 0.146 0.176 0.216 Warning : The interpolation only works for \\(0.01 < p < 0.10\\) (p-values below \\(0.01\\) and above \\(0.10\\) will be truncated) and significance levels \\(\\alpha\\) between \\(0.01\\) and \\(0.10\\) . Mann-Kendall (MK) Test The Mann-Kendall (MK) Test detects statistically significant monotonic trends in a time series under the assumption of independence (i.e. no autocorrelation). Null hypothesis: There is no monotonic trend. Alternative hypothesis: A (upward or downward) monotonic trend exists. Define \\(\\text{sign} (x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. The test statistic \\(S\\) is defined as follows: \\[ S = \\sum_{k-1}^{n-1} \\sum_{j - k + 1}^{n} \\text{sign} (y_{j} - y_{k}) \\] Next, we need to compute \\(\\text{Var}(S)\\) , which depends on the number of tied groups in the data. Let \\(g\\) be the number of tied groups and \\(t_{p}\\) be the number of observations in the \\(p\\) -th group. \\[\\text{Var}(S) = \\frac{1}{18} \\left[n(n-1)(2n + 1) - \\sum_{p-1}^{g} t_{p}(t_{p} - 1)(2t_{p} + 5) \\right]\\] Then, compute the normally distributed test statistic \\(Z_{MK}\\) as follows: \\[ Z_{MK} = \\begin{cases} \\frac{S-1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S > 0 \\\\ 0 &\\text{if } S = 0 \\\\ \\frac{S+1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S < 0 \\end{cases} \\] For a two-sided test, we reject the null hypothesis if \\(|Z_{MK}| \\geq Z_{1 - (\\alpha/2) }\\) and conclude that there is a statistically significant monotonic trend in the data. For more information, see here . Phillips-Perron (PP) Test The PP Test identifies if an autoregressive time series has a unit root . Null hypothesis: The time series has a unit root (stochastic trend). Alternative hypothesis: The time series has a deterministic linear trend. Precisely, let \\(x_{t}\\) be an AR(1) model. Let \\(y_{t}\\) be a function of \\(x_{t}\\) with drift \\(\\beta_{0}\\) and trend \\(\\beta_{1} t\\) . \\[ \\begin{align} y_{t} &= \\beta_{0} + \\beta_{1} t + x_{t} \\\\[5pt] x_{t} &= \\rho x_{t-1} + \\epsilon_{t} \\end{align} \\] If \\(\\rho = 1\\) , then \\(x_t\\) and hence \\(y_t\\) has a unit root (null hypothesis). If \\(\\rho < 1\\) , then \\(y_t\\) is trend stationary (alternative hypothesis). Steps Fit a linear autoregressive model to the time series \\(y_{t}\\) . Let \\(\\hat{r}_{t}\\) be the residuals of this model. From this model, we can determine \\(\\hat{\\rho}\\) (the estimated coefficient on \\(y_{t-1}\\) ) and \\(\\text{SE}(\\hat{\\rho})\\) . Estimate the variance of the residuals \\(\\hat{\\sigma}^2\\) : \\[ \\hat{\\sigma^2} = \\frac{1}{n - 3} \\sum_{t=1}^{n} \\hat{r}_{t}^2 \\] where \\(n\\) is the number of data points in the sample. We have \\(n-3\\) degrees of freedom since there are three parameters in the autoregressive model ( \\(\\beta_{0}\\) , \\(\\beta_{1}\\) , and \\(\\rho\\) ). Estimate the long-run variance \\(\\hat{\\lambda}^2\\) using a Newey-West style estimator. This estimator corrects for the additional variability in \\(\\epsilon_{t}\\) caused by autocorrelation and heteroskedasticity. \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_{0} + 2\\sum_{j=1}^{q} \\left(1 - \\frac{j}{q + 1} \\right) \\gamma_{j} \\] Each sample autocovariances \\(\\gamma_{j}\\) above is computed for up to \\(q = \\left\\lfloor \\sqrt[4]{\\frac{n}{25}}\\right\\rfloor\\) lags: \\[ \\hat{\\gamma}_{j} = \\frac{1}{n} \\sum_{t = j + 1}^{n} \\hat{r}_{t}\\hat{r}_{t-j} \\] Compute the test statistic \\(z_{\\rho}\\) using the following formula: \\[ z_{\\rho } = n(\\hat{\\rho} - 1) - \\frac{n^2 \\text{SE}(\\hat{\\rho})^2}{2 \\hat{\\sigma}^2}(\\hat{\\lambda }^2 - \\hat{\\gamma}_{0}) \\] The test statistic \\(z_{\\rho}\\) is not normally distributed. Instead, we compute the p-value by interpolating a table from Fuller, W. A. (1996). This table is shown below for sample sizes \\(n\\) and probabilities \\(p\\) : \\(n\\) \\ \\(p\\) 0.01 0.025 0.05 0.10 0.50 0.90 0.95 0.975 0.99 25 -22.5 -20.0 -17.9 -15.6 -8.49 -3.65 -2.51 -1.53 -0.46 50 -25.8 -22.4 -19.7 -16.8 -8.80 -3.71 -2.60 -1.67 -0.67 100 -27.4 -23.7 -20.6 -17.5 -8.96 -3.74 -2.63 -1.74 -0.76 250 -28.5 -24.4 -21.3 -17.9 -9.05 -3.76 -2.65 -1.79 -0.83 500 -28.9 -24.7 -21.5 -18.1 -9.08 -3.76 -2.66 -1.80 -0.86 1000 -29.4 -25.0 -21.7 -18.3 -9.11 -3.77 -2.67 -1.81 -0.88 Warning : The interpolation only works for p-values \\(p > 0.01\\) (p-values below \\(0.01\\) are truncated) and confidence levels \\(\\alpha > 0.01\\) . Runs Test The Runs Test checks whether the residuals of a regression (e.g., trend approximation from Sen's estimator ) are randomly distributed. If the Runs test identifies non-randomness in the residuals, it is a strong indication that the nonstationarity in the data is non-linear. Null hypothesis: Residuals are distributed randomly. Alternative hypothesis: Residuals are not distributed randomly (e.g., due to nonlinearity). Steps Classify the data based on whether it is above ( \\(+\\) ) or below \\((-)\\) the median. All data points that are equal to the median are removed. Compute the number of contiguous blocks of \\(+\\) or \\(-\\) (known as runs ) in the data. For example, the sequence \\(+++--+++-+-\\) has six runs with length \\((3, 2, 3, 1, 1, 1)\\) . Let \\(R\\) be the number of runs in \\(N\\) data points (with category counts \\(N_{+}\\) and \\(N_{-}\\) ). Then, under the null hypothesis, \\(R\\) is asymptotically normal with: \\[ \\mathbb{E}[R] = \\frac{2N_{+}N_{-}}{N} + 1, \\quad \\text{Var}(R) = \\frac{2N_{+}N_{-}(2N_{+}N_{-} - N)}{N^2(N - 1)} \\] Compute the p-value by normalizing \\(R\\) using the expectation and variance given above. Example Plot Sen's Trend Estimator Sen's Trend Estimator approximates the slope of a regression line. Unlike Least Squares , Sen's trend estimator uses a non-parametric approach which makes it robust to outliers. Steps For all pairs \\((x_i, y_i)\\) and \\((x_j, y_j)\\) where \\(x_i \\neq x_j\\) , compute slopes: \\[ m_{ij} = \\frac{y_j - y_i}{x_j - x_i} \\] Take the median of all slopes: \\(\\hat{m}\\) . Estimate the \\(y\\) -intercept \\(\\hat{b}\\) as the median of \\(y_{i} - \\hat{m}x_{i}\\) for all \\(i\\) . Example Plot Spearman Test The Spearman test identifies autocorrelation in a time series \\(y_{t}\\) . A significant lag is a number \\(i\\) such that the correlation between \\(y_{t}\\) and \\(y_{t-i}\\) is statistically significant. The least insignificant lag is the smallest \\(i\\) that is not a significant lag. Null hypothesis: The least insignificant lag is \\(1\\) . Alternative hypothesis: The least insignificant lag is greater than \\(1\\) . To carry out the Spearman test, we use the following procedure: Compute Spearman's correlation coefficient \\(\\rho_{i}\\) for \\(y_{t}\\) and \\(y_{t-i}\\) for all \\(0 \\leq i < n\\) . Compute the \\(p\\) -value \\(p_{i}\\) for each correlation coefficient \\(\\rho _{i}\\) using the formula: $$ t_{i}= \\rho_{i} \\sqrt{\\frac{n-2}{1 - \\rho _{i}^2}} $$ The test statistic \\(t_{i}\\) has the \\(t\\) -distribution with \\(n-2\\) degrees of freedom. Find the smallest \\(i\\) such that \\(p_{i} > \\alpha\\) . Then \\(i\\) is the least insignificant lag at confidence level \\(\\alpha\\) . For more information, see the Wikipedia pages on Autocorrelation and Spearman's Rho . Example Plot","title":"Trends in AMS Mean"},{"location":"eda-trend-ams-mean/#detecting-and-characterizing-trends-in-the-ams-mean","text":"This section describes the statistical tests (listed in alphabetical order) used to detect and characterizesignificant trends in the mean of the annual maximum series (AMS). These tests help identify a trend, identify autocorrelation, and determine whether a trend is deterministic/stochastic and linear/non-linear.","title":"Detecting and Characterizing Trends in the AMS Mean"},{"location":"eda-trend-ams-mean/#bb-mk-test","text":"The Block Bootstrap Mann-Kendall (BB-MK) Test assesses the presence of a statistically significant monotonic trend in a time series. The BB-MK test is insensitive to autocorrelation , which is known to produce false positives in the MK test . Null hypothesis: No monotonic trend. Alternative hypothesis: A monotonic upward or downward trend exists. To conduct the BB-MK test, we rely on the results of the MK test and the Spearman autocorrelation test.","title":"BB-MK Test"},{"location":"eda-trend-ams-mean/#steps","text":"Compute the MK test statistic (see below). Use the Spearman test (see below) to identify the least insignificant lag \\(k\\) . Resample the time series in blocks of size \\(k+1\\) without replacement. Compute the MK test statistic for each bootstrapped sample. Derive the empirical distribution of the MK test statistic from the bootstrapped statistics. Estimate the significance of the observed test statistic using the empirical distribution.","title":"Steps"},{"location":"eda-trend-ams-mean/#example-plot","text":"","title":"Example Plot"},{"location":"eda-trend-ams-mean/#kpss-test","text":"The KPSS Test determines whether an autoregressive time series has a unit root . This test helps assess if the time series has a deterministic linear trend. Null hypothesis: The time series has a deterministic linear trend. Alternative hypothesis: The time series has a unit root (stochastic trend). The autoregressive time series shown below has a unit root if \\(\\sigma_{v}^2 > 0\\) : \\[ \\begin{align} y_{t} &= \\mu_{t} + \\beta t + \\epsilon_{t} \\\\[5pt] \\mu_{t} &= \\mu_{t-1} + v_{t} \\\\[5pt] v_{t} &\\sim \\mathcal{N}(0, \\sigma_{v}^2) \\end{align} \\] where: \\(\\mu_{t}\\) is the drift , or the deviation of \\(y_{t}\\) from \\(0\\) . Under the null hypothesis, \\(\\mu_{t}\\) is constant (since \\(v_{t}\\) is constant). Under the alternative hypothesis, \\(\\mu_t\\) is a stochastic process with a unit root. \\(\\beta t\\) is a linear trend , which represents deterministic nonstationarity (e.g., climate change). \\(\\epsilon_{t}\\) is stationary noise , corresponding to reversible fluctuations in \\(y_{t}\\) . In hydrology, \\(\\epsilon_{t}\\) represents fluctuations in streamflow due to natural variability. \\(v_{t}\\) is random walk innovation , or irreversible fluctuations in \\(\\mu_{t}\\) .","title":"KPSS Test"},{"location":"eda-trend-ams-mean/#steps_1","text":"Fit a linear model to \\(y_{t}\\) and get the residuals \\(\\hat{r}_{t}\\) . Compute the cumulative partial-sum statistics \\(S_{k}\\) using the following formula: \\[ S_{k} = \\sum_{t=1}^{k} \\hat{r}_{t} \\] Under the null hypothesis, \\(S_{k}\\) will behave like a random walk with finite variance. If \\(y_{t}\\) has a unit root, then the sums will \"drift\" too much. Estimate the long-run variance of the time series using a Newey-West estimator : \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_0 + 2 \\sum_{j=1}^{q} \\left(1 - \\frac{j}{q+1} \\right) \\hat{\\gamma}_j \\] Where \\(q = \\left\\lfloor \\frac{3\\sqrt{n}}{13} \\right\\rfloor\\) and each autocovariance \\(\\hat{\\gamma}_j\\) is: \\[ \\hat{\\gamma}_j = \\frac{1}{n} \\sum_{t = j+1}^{n} \\hat{r}_t \\hat{r}_{t-j} \\] Compute the test statistic \\(z_{K}\\) : \\[ z_{K} = \\frac{1}{n^2\\hat{\\lambda }^2}\\sum_{k=1}^{n} S_{k}^2 \\] Since the test statistic \\(z_{K}\\) is non-normally distributed, we compute the p-value by interpolating the table of quantiles from Hobjin et al. (2004) shown below. \\(q\\) 0.90 0.95 0.975 0.99 Statistic 0.119 0.146 0.176 0.216 Warning : The interpolation only works for \\(0.01 < p < 0.10\\) (p-values below \\(0.01\\) and above \\(0.10\\) will be truncated) and significance levels \\(\\alpha\\) between \\(0.01\\) and \\(0.10\\) .","title":"Steps"},{"location":"eda-trend-ams-mean/#mann-kendall-mk-test","text":"The Mann-Kendall (MK) Test detects statistically significant monotonic trends in a time series under the assumption of independence (i.e. no autocorrelation). Null hypothesis: There is no monotonic trend. Alternative hypothesis: A (upward or downward) monotonic trend exists. Define \\(\\text{sign} (x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. The test statistic \\(S\\) is defined as follows: \\[ S = \\sum_{k-1}^{n-1} \\sum_{j - k + 1}^{n} \\text{sign} (y_{j} - y_{k}) \\] Next, we need to compute \\(\\text{Var}(S)\\) , which depends on the number of tied groups in the data. Let \\(g\\) be the number of tied groups and \\(t_{p}\\) be the number of observations in the \\(p\\) -th group. \\[\\text{Var}(S) = \\frac{1}{18} \\left[n(n-1)(2n + 1) - \\sum_{p-1}^{g} t_{p}(t_{p} - 1)(2t_{p} + 5) \\right]\\] Then, compute the normally distributed test statistic \\(Z_{MK}\\) as follows: \\[ Z_{MK} = \\begin{cases} \\frac{S-1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S > 0 \\\\ 0 &\\text{if } S = 0 \\\\ \\frac{S+1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S < 0 \\end{cases} \\] For a two-sided test, we reject the null hypothesis if \\(|Z_{MK}| \\geq Z_{1 - (\\alpha/2) }\\) and conclude that there is a statistically significant monotonic trend in the data. For more information, see here .","title":"Mann-Kendall (MK) Test"},{"location":"eda-trend-ams-mean/#phillips-perron-pp-test","text":"The PP Test identifies if an autoregressive time series has a unit root . Null hypothesis: The time series has a unit root (stochastic trend). Alternative hypothesis: The time series has a deterministic linear trend. Precisely, let \\(x_{t}\\) be an AR(1) model. Let \\(y_{t}\\) be a function of \\(x_{t}\\) with drift \\(\\beta_{0}\\) and trend \\(\\beta_{1} t\\) . \\[ \\begin{align} y_{t} &= \\beta_{0} + \\beta_{1} t + x_{t} \\\\[5pt] x_{t} &= \\rho x_{t-1} + \\epsilon_{t} \\end{align} \\] If \\(\\rho = 1\\) , then \\(x_t\\) and hence \\(y_t\\) has a unit root (null hypothesis). If \\(\\rho < 1\\) , then \\(y_t\\) is trend stationary (alternative hypothesis).","title":"Phillips-Perron (PP) Test"},{"location":"eda-trend-ams-mean/#steps_2","text":"Fit a linear autoregressive model to the time series \\(y_{t}\\) . Let \\(\\hat{r}_{t}\\) be the residuals of this model. From this model, we can determine \\(\\hat{\\rho}\\) (the estimated coefficient on \\(y_{t-1}\\) ) and \\(\\text{SE}(\\hat{\\rho})\\) . Estimate the variance of the residuals \\(\\hat{\\sigma}^2\\) : \\[ \\hat{\\sigma^2} = \\frac{1}{n - 3} \\sum_{t=1}^{n} \\hat{r}_{t}^2 \\] where \\(n\\) is the number of data points in the sample. We have \\(n-3\\) degrees of freedom since there are three parameters in the autoregressive model ( \\(\\beta_{0}\\) , \\(\\beta_{1}\\) , and \\(\\rho\\) ). Estimate the long-run variance \\(\\hat{\\lambda}^2\\) using a Newey-West style estimator. This estimator corrects for the additional variability in \\(\\epsilon_{t}\\) caused by autocorrelation and heteroskedasticity. \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_{0} + 2\\sum_{j=1}^{q} \\left(1 - \\frac{j}{q + 1} \\right) \\gamma_{j} \\] Each sample autocovariances \\(\\gamma_{j}\\) above is computed for up to \\(q = \\left\\lfloor \\sqrt[4]{\\frac{n}{25}}\\right\\rfloor\\) lags: \\[ \\hat{\\gamma}_{j} = \\frac{1}{n} \\sum_{t = j + 1}^{n} \\hat{r}_{t}\\hat{r}_{t-j} \\] Compute the test statistic \\(z_{\\rho}\\) using the following formula: \\[ z_{\\rho } = n(\\hat{\\rho} - 1) - \\frac{n^2 \\text{SE}(\\hat{\\rho})^2}{2 \\hat{\\sigma}^2}(\\hat{\\lambda }^2 - \\hat{\\gamma}_{0}) \\] The test statistic \\(z_{\\rho}\\) is not normally distributed. Instead, we compute the p-value by interpolating a table from Fuller, W. A. (1996). This table is shown below for sample sizes \\(n\\) and probabilities \\(p\\) : \\(n\\) \\ \\(p\\) 0.01 0.025 0.05 0.10 0.50 0.90 0.95 0.975 0.99 25 -22.5 -20.0 -17.9 -15.6 -8.49 -3.65 -2.51 -1.53 -0.46 50 -25.8 -22.4 -19.7 -16.8 -8.80 -3.71 -2.60 -1.67 -0.67 100 -27.4 -23.7 -20.6 -17.5 -8.96 -3.74 -2.63 -1.74 -0.76 250 -28.5 -24.4 -21.3 -17.9 -9.05 -3.76 -2.65 -1.79 -0.83 500 -28.9 -24.7 -21.5 -18.1 -9.08 -3.76 -2.66 -1.80 -0.86 1000 -29.4 -25.0 -21.7 -18.3 -9.11 -3.77 -2.67 -1.81 -0.88 Warning : The interpolation only works for p-values \\(p > 0.01\\) (p-values below \\(0.01\\) are truncated) and confidence levels \\(\\alpha > 0.01\\) .","title":"Steps"},{"location":"eda-trend-ams-mean/#runs-test","text":"The Runs Test checks whether the residuals of a regression (e.g., trend approximation from Sen's estimator ) are randomly distributed. If the Runs test identifies non-randomness in the residuals, it is a strong indication that the nonstationarity in the data is non-linear. Null hypothesis: Residuals are distributed randomly. Alternative hypothesis: Residuals are not distributed randomly (e.g., due to nonlinearity).","title":"Runs Test"},{"location":"eda-trend-ams-mean/#steps_3","text":"Classify the data based on whether it is above ( \\(+\\) ) or below \\((-)\\) the median. All data points that are equal to the median are removed. Compute the number of contiguous blocks of \\(+\\) or \\(-\\) (known as runs ) in the data. For example, the sequence \\(+++--+++-+-\\) has six runs with length \\((3, 2, 3, 1, 1, 1)\\) . Let \\(R\\) be the number of runs in \\(N\\) data points (with category counts \\(N_{+}\\) and \\(N_{-}\\) ). Then, under the null hypothesis, \\(R\\) is asymptotically normal with: \\[ \\mathbb{E}[R] = \\frac{2N_{+}N_{-}}{N} + 1, \\quad \\text{Var}(R) = \\frac{2N_{+}N_{-}(2N_{+}N_{-} - N)}{N^2(N - 1)} \\] Compute the p-value by normalizing \\(R\\) using the expectation and variance given above.","title":"Steps"},{"location":"eda-trend-ams-mean/#example-plot_1","text":"","title":"Example Plot"},{"location":"eda-trend-ams-mean/#sens-trend-estimator","text":"Sen's Trend Estimator approximates the slope of a regression line. Unlike Least Squares , Sen's trend estimator uses a non-parametric approach which makes it robust to outliers.","title":"Sen's Trend Estimator"},{"location":"eda-trend-ams-mean/#steps_4","text":"For all pairs \\((x_i, y_i)\\) and \\((x_j, y_j)\\) where \\(x_i \\neq x_j\\) , compute slopes: \\[ m_{ij} = \\frac{y_j - y_i}{x_j - x_i} \\] Take the median of all slopes: \\(\\hat{m}\\) . Estimate the \\(y\\) -intercept \\(\\hat{b}\\) as the median of \\(y_{i} - \\hat{m}x_{i}\\) for all \\(i\\) .","title":"Steps"},{"location":"eda-trend-ams-mean/#example-plot_2","text":"","title":"Example Plot"},{"location":"eda-trend-ams-mean/#spearman-test","text":"The Spearman test identifies autocorrelation in a time series \\(y_{t}\\) . A significant lag is a number \\(i\\) such that the correlation between \\(y_{t}\\) and \\(y_{t-i}\\) is statistically significant. The least insignificant lag is the smallest \\(i\\) that is not a significant lag. Null hypothesis: The least insignificant lag is \\(1\\) . Alternative hypothesis: The least insignificant lag is greater than \\(1\\) . To carry out the Spearman test, we use the following procedure: Compute Spearman's correlation coefficient \\(\\rho_{i}\\) for \\(y_{t}\\) and \\(y_{t-i}\\) for all \\(0 \\leq i < n\\) . Compute the \\(p\\) -value \\(p_{i}\\) for each correlation coefficient \\(\\rho _{i}\\) using the formula: $$ t_{i}= \\rho_{i} \\sqrt{\\frac{n-2}{1 - \\rho _{i}^2}} $$ The test statistic \\(t_{i}\\) has the \\(t\\) -distribution with \\(n-2\\) degrees of freedom. Find the smallest \\(i\\) such that \\(p_{i} > \\alpha\\) . Then \\(i\\) is the least insignificant lag at confidence level \\(\\alpha\\) . For more information, see the Wikipedia pages on Autocorrelation and Spearman's Rho .","title":"Spearman Test"},{"location":"eda-trend-ams-mean/#example-plot_3","text":"","title":"Example Plot"},{"location":"eda-trend-ams-variability/","text":"Detecting Trends in the AMS Variability This section describes the methods used to detect trends or changes in the variability (e.g., variance or standard deviation) of annual maximum series (AMS) data. Moving Window Mann-Kendall (MW-MK) Test The MW-MK test detects statistically significant monotonic trends in the standard deviation of AMS data. Null hypothesis: No significant trend in the standard deviation. Alternative hypothesis: Significant monotonic trend in the standard deviation. Steps To compute the standard deviations of the AMS data, we use a moving window algorithm. Let \\(w\\) be the length of the moving window and \\(s\\) be the step size. Then, Initialize the moving window at indices \\([1, w]\\) . Compute the sample standard deviation within the window. Move the window forward by \\(s\\) steps. Repeat steps 2 and 3 until the window reaches the end of the data. This produces a time series of moving-window standard deviations. Then, the Mann-Kendall Test is used to test for a monotonic trend in the standard deviation. Sen's Trend Estimator Used to estimate the slope of a trend in the standard deviations (see here ). Runs Test Used to the check residuals of trend fitted to the standard deviations for randomness (see here ). White Test The White Test detects changes in variability (heteroskedasticity) in a time series. Null hypothesis: Constant variability (homoskedasticity). Alternative hypothesis: Time-dependent variability (heteroskedasticity). Steps Fit a simple linear regression model using ordinary least squares: \\[y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i}\\] Compute the squared residuals: \\[ \\hat{r}_i^2 = \\left(y_i - \\hat{y}_i\\right)^2 \\] Fit an auxiliary regression model to the squared residuals. This model includes each regressor, the square of each regressor, and the cross products between all regressors. Since \\(x\\) is the only regressor, the regression model is simply: \\[ \\hat{r}_i^2 = \\alpha_0 + \\alpha_1 x_i + \\alpha_2 x_i^2 + u_i \\] Compute the coefficient of determination \\(R^2\\) for the auxillary model. Compute the test statistic \\(nR^2 \\sim \\chi_{d}^2\\) where \\(n\\) is the number of observations and \\(d = 2\\) is the number of regressors, excluding the intercept. If \\(nR^2 > \\chi^2_{1-\\alpha, d}\\) , we reject the null hypothesis and conclude that the time series exhibits heteroskedasticity.","title":"Trends in AMS Variability"},{"location":"eda-trend-ams-variability/#detecting-trends-in-the-ams-variability","text":"This section describes the methods used to detect trends or changes in the variability (e.g., variance or standard deviation) of annual maximum series (AMS) data.","title":"Detecting Trends in the AMS Variability"},{"location":"eda-trend-ams-variability/#moving-window-mann-kendall-mw-mk-test","text":"The MW-MK test detects statistically significant monotonic trends in the standard deviation of AMS data. Null hypothesis: No significant trend in the standard deviation. Alternative hypothesis: Significant monotonic trend in the standard deviation.","title":"Moving Window Mann-Kendall (MW-MK) Test"},{"location":"eda-trend-ams-variability/#steps","text":"To compute the standard deviations of the AMS data, we use a moving window algorithm. Let \\(w\\) be the length of the moving window and \\(s\\) be the step size. Then, Initialize the moving window at indices \\([1, w]\\) . Compute the sample standard deviation within the window. Move the window forward by \\(s\\) steps. Repeat steps 2 and 3 until the window reaches the end of the data. This produces a time series of moving-window standard deviations. Then, the Mann-Kendall Test is used to test for a monotonic trend in the standard deviation.","title":"Steps"},{"location":"eda-trend-ams-variability/#sens-trend-estimator","text":"Used to estimate the slope of a trend in the standard deviations (see here ).","title":"Sen's Trend Estimator"},{"location":"eda-trend-ams-variability/#runs-test","text":"Used to the check residuals of trend fitted to the standard deviations for randomness (see here ).","title":"Runs Test"},{"location":"eda-trend-ams-variability/#white-test","text":"The White Test detects changes in variability (heteroskedasticity) in a time series. Null hypothesis: Constant variability (homoskedasticity). Alternative hypothesis: Time-dependent variability (heteroskedasticity).","title":"White Test"},{"location":"eda-trend-ams-variability/#steps_1","text":"Fit a simple linear regression model using ordinary least squares: \\[y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i}\\] Compute the squared residuals: \\[ \\hat{r}_i^2 = \\left(y_i - \\hat{y}_i\\right)^2 \\] Fit an auxiliary regression model to the squared residuals. This model includes each regressor, the square of each regressor, and the cross products between all regressors. Since \\(x\\) is the only regressor, the regression model is simply: \\[ \\hat{r}_i^2 = \\alpha_0 + \\alpha_1 x_i + \\alpha_2 x_i^2 + u_i \\] Compute the coefficient of determination \\(R^2\\) for the auxillary model. Compute the test statistic \\(nR^2 \\sim \\chi_{d}^2\\) where \\(n\\) is the number of observations and \\(d = 2\\) is the number of regressors, excluding the intercept. If \\(nR^2 > \\chi^2_{1-\\alpha, d}\\) , we reject the null hypothesis and conclude that the time series exhibits heteroskedasticity.","title":"Steps"},{"location":"ffa-introduction/","text":"Flood Frequency Analysis (FFA) Overview Flood Frequency Analysis (FFA) uses a probability distribution fitted to extreme streamflow observations (e.g., annual maxima) to estimate the recurrence likelihood of floods. To perform FFA, we require a probability model and corresponding parameter estimates based on the data. FFA relates flood peak magnitudes \\(Q\\) to their expected frequency of occurrence, expressed as a return period . For example, a flood with a 10-year return period\u2014commonly referred to as a 10-year flood \u2014has a 1-in-10 chance of being equalled or exceeded in any given year. This corresponds to an annual exceedance probability of \\(p_e = 0.1\\) . Since the FFA Framework uses annual maxima data, this equates to the 90th percentile (i.e., the \\(0.90\\) quantile) of the fitted probability distribution. Here is a summary of the return periods, exceedance probabilities, and associated quantiles used by default in the FFA framework: Return Period ( \\(T\\) ) Exceedance Probability ( \\(p_e\\) ) Quantile ( \\(F(q)\\) ) 2 Years 0.50 0.50 5 Years 0.20 0.80 10 Years 0.10 0.90 20 Years 0.05 0.95 50 Years 0.02 0.98 100 Years 0.01 0.99 Let \\(F(q)\\) be the cumulative distribution function (CDF) of the fitted model. This function maps flood magnitudes to exceedance probabilities: \\(p_e = 1 - F(q)\\) . To estimate flood magnitudes for a given exceedance probability, we use the inverse of the CDF, better known as the quantile function : \\(\\hat{q} = F^{-1}(p_e)\\) . Example Plot FFA results are typically visualized with return period on the \\(x\\) -axis and flood magnitude on the \\(y\\) -axis. These plots can be interpreted in two directions: Estimate flood magnitude for a given return period. For example, A 50-year flood is estimated to be about \\(85\\ \\text{m}^3/\\text{s}\\) . Estimate return period for a given flood magnitude. For example, A streamflow of \\(50\\ \\text{m}^3/\\text{s}\\) is expected to occur roughly every 4 years. Note : For an explanation of the confidence bounds in this plot, see Uncertainty Quantification . Handling Nonstationarity A probability model is considered nonstationary if its statistical properties (e.g., location or scale) change over time. In such cases, the quantile function becomes time-dependent: \\(F^{-1}(p_e, t)\\) . As a result, return levels and exceedance probabilities vary with time, and a static return period curve is no longer valid. To address this, the FFA framework computes effective return periods , which yield flood estimates for a specific year based on the time-varying distribution. Example Plot The plot below illustrates effective return levels for the year 2017. Remember, a 100-year effective return level does not imply that such a flood is expected to occur once in the next 100 years. It means that in the year 2017 , the probability of exceeding that flood magnitude is 1 in 100.","title":"Introduction"},{"location":"ffa-introduction/#flood-frequency-analysis-ffa","text":"","title":"Flood Frequency Analysis (FFA)"},{"location":"ffa-introduction/#overview","text":"Flood Frequency Analysis (FFA) uses a probability distribution fitted to extreme streamflow observations (e.g., annual maxima) to estimate the recurrence likelihood of floods. To perform FFA, we require a probability model and corresponding parameter estimates based on the data. FFA relates flood peak magnitudes \\(Q\\) to their expected frequency of occurrence, expressed as a return period . For example, a flood with a 10-year return period\u2014commonly referred to as a 10-year flood \u2014has a 1-in-10 chance of being equalled or exceeded in any given year. This corresponds to an annual exceedance probability of \\(p_e = 0.1\\) . Since the FFA Framework uses annual maxima data, this equates to the 90th percentile (i.e., the \\(0.90\\) quantile) of the fitted probability distribution. Here is a summary of the return periods, exceedance probabilities, and associated quantiles used by default in the FFA framework: Return Period ( \\(T\\) ) Exceedance Probability ( \\(p_e\\) ) Quantile ( \\(F(q)\\) ) 2 Years 0.50 0.50 5 Years 0.20 0.80 10 Years 0.10 0.90 20 Years 0.05 0.95 50 Years 0.02 0.98 100 Years 0.01 0.99 Let \\(F(q)\\) be the cumulative distribution function (CDF) of the fitted model. This function maps flood magnitudes to exceedance probabilities: \\(p_e = 1 - F(q)\\) . To estimate flood magnitudes for a given exceedance probability, we use the inverse of the CDF, better known as the quantile function : \\(\\hat{q} = F^{-1}(p_e)\\) .","title":"Overview"},{"location":"ffa-introduction/#example-plot","text":"FFA results are typically visualized with return period on the \\(x\\) -axis and flood magnitude on the \\(y\\) -axis. These plots can be interpreted in two directions: Estimate flood magnitude for a given return period. For example, A 50-year flood is estimated to be about \\(85\\ \\text{m}^3/\\text{s}\\) . Estimate return period for a given flood magnitude. For example, A streamflow of \\(50\\ \\text{m}^3/\\text{s}\\) is expected to occur roughly every 4 years. Note : For an explanation of the confidence bounds in this plot, see Uncertainty Quantification .","title":"Example Plot"},{"location":"ffa-introduction/#handling-nonstationarity","text":"A probability model is considered nonstationary if its statistical properties (e.g., location or scale) change over time. In such cases, the quantile function becomes time-dependent: \\(F^{-1}(p_e, t)\\) . As a result, return levels and exceedance probabilities vary with time, and a static return period curve is no longer valid. To address this, the FFA framework computes effective return periods , which yield flood estimates for a specific year based on the time-varying distribution.","title":"Handling Nonstationarity"},{"location":"ffa-introduction/#example-plot_1","text":"The plot below illustrates effective return levels for the year 2017. Remember, a 100-year effective return level does not imply that such a flood is expected to occur once in the next 100 years. It means that in the year 2017 , the probability of exceeding that flood magnitude is 1 in 100.","title":"Example Plot"},{"location":"goals/","text":"Goals Vidrio-Sahag\u00fan et al. (2024) identified a number of issues with FFA research in Canada. Our framework aims to address these issues. In particular, we hope to achieve: Standardization : The United States uses a standardized distribution (LP3) and parameter estimation method (expected moments) for FFA. However, Canada does not provide such guidance. By developing this framework, we hope to limit subjectivity in flood estimation. Reproducibility : Many FFA studies in Canada do not provide essential information for reproduction, such information about exploratory data analysis (EDA) performed, the distribution selection mechanism, and even the probability distribution itself. Statistical Rigour : Historically, many FFA studies have not performed uncertainty quantification, which makes it difficult to draw accurate conclusion from the results. Our framework automatically performs uncertainty analysis to quantify potential errors. Research-to-Practice Translation : The disorganization of FFA research in Canada makes it difficult for regulatory agencies to access cutting-edge advancements in FFA methodologies. By providing a common set of techniques for modellers to use, we hope to bridge the gap between research and practice. Development is guided by the following principles: Software Freedom : Our framework is built on free and open source software. Modularity : Users are allowed to use as much or as little of the framework as they like. Interoperability : Our framework can be seamlessly integrated with other flood models. Flexibility : Users can tailor their analysis to the nuances of individual watersheds. Clarity : The source code is easy to read and understand. Robustness : The framework can handle datasets with small sample sizes or missing values.","title":"Goals"},{"location":"goals/#goals","text":"Vidrio-Sahag\u00fan et al. (2024) identified a number of issues with FFA research in Canada. Our framework aims to address these issues. In particular, we hope to achieve: Standardization : The United States uses a standardized distribution (LP3) and parameter estimation method (expected moments) for FFA. However, Canada does not provide such guidance. By developing this framework, we hope to limit subjectivity in flood estimation. Reproducibility : Many FFA studies in Canada do not provide essential information for reproduction, such information about exploratory data analysis (EDA) performed, the distribution selection mechanism, and even the probability distribution itself. Statistical Rigour : Historically, many FFA studies have not performed uncertainty quantification, which makes it difficult to draw accurate conclusion from the results. Our framework automatically performs uncertainty analysis to quantify potential errors. Research-to-Practice Translation : The disorganization of FFA research in Canada makes it difficult for regulatory agencies to access cutting-edge advancements in FFA methodologies. By providing a common set of techniques for modellers to use, we hope to bridge the gap between research and practice. Development is guided by the following principles: Software Freedom : Our framework is built on free and open source software. Modularity : Users are allowed to use as much or as little of the framework as they like. Interoperability : Our framework can be seamlessly integrated with other flood models. Flexibility : Users can tailor their analysis to the nuances of individual watersheds. Clarity : The source code is easy to read and understand. Robustness : The framework can handle datasets with small sample sizes or missing values.","title":"Goals"},{"location":"license/","text":"License FFA Framework \u00a9 2025 by Riley Wheadon, Cuauht\u00e9moc Tonatiuh Vidrio-Sahag\u00fan, and Alain Pietroniro is licensed under the GNU General Public License 3.0. To view a copy of this license, visit https://www.gnu.org/licenses/gpl-3.0.en.html .","title":"License"},{"location":"license/#license","text":"FFA Framework \u00a9 2025 by Riley Wheadon, Cuauht\u00e9moc Tonatiuh Vidrio-Sahag\u00fan, and Alain Pietroniro is licensed under the GNU General Public License 3.0. To view a copy of this license, visit https://www.gnu.org/licenses/gpl-3.0.en.html .","title":"License"},{"location":"matlab-version/","text":"Changes from MATLAB Version This page documents changes from original MATLAB code . Exploratory Data Analysis (EDA) Bug Fixes Only show statistically significant change points for the Pettitt and MKS plots. Fix bug causing the MKS test to only identify one change point, even if multiple change points were found to be above the threshold for statistical significance. Fix major bug causing the MKS test to identify change points based on the progressive series instead of the z-statistics of the crossing points. Remove unnecessary rounding in the moving window algorithm for AMS variability. Re-implement the Phillips-Perron and KPSS tests to account for drift and trend. Framework Changes If serial correlation is identified, do not run the Phillips-Perron and KPSS tests. Add the the Runs test to detect nonlinearity after fitting Sen's trend estimator. Run change point detection in multiple stages to prevent overpartitioning. Flood Frequency Analysis (FFA) Distribution Changes The generalized Pareto (GPA) distribution has been removed, since its likelihood function is not amenable to maximum likelihood estimation. Issues occur because the GPA distribution is intended for peaks over threshold modelling, which we do not use. The R version uses the three parameter Weibull distribution (with location, scale, and shape) parameters instead of the two parameter Weibull distribution (with scale and shape parameters). This ensures consistency with the other distributions, which all have location parameters. Model Selection Changes The L-distance and L-kurtosis selection methods have been improved by using an optimization algorithm to find the parameters with the closest L-moments to the data instead of using a brute force approach. This is more computationally efficient and gives more precise results. The procedure for computing the Z-statistic selection metric has been changed. If the L-moments of the dataset do not satisfy \\(\\tau_{4} \\leq (1 + 5\\tau _{3}^2)/6\\) , then the Kappa distribution will not be fitted and the candidate distributions that use the dataset will be omitted. Parameter Estimation Changes Parameterization of the PE3/LP3 distributions fails for some datasets because MATLAB is unable to handle the large numbers created by the gamma function. To manage this issue, the MATLAB version used the conventional moments (i.e. sample mean/variance/skewness) when this occurred. We avoid this problem in the R version by using the lgamma (log-gamma) function. The R implementation uses L-BFGS-B for MLE/GMLE parameter estimation instead of Nelder-Mead, since the gradient is well defined for the likelihood functions we are working with. Additionally, the L-BFGS-B method makes it possible to assign bounds to the variables. This modification produced slight improvements to the MLL/GMLL for some datasets. Uncertainty Quantification Implement RFPL uncertainty quantification for the Weibull distribution. Model Assessment Changes Use the built-in R function approx() to perform log-linear interpolation of the return periods. The MATLAB implementation uses a hard-coded algorithm which behaves unpredictably when the original and interpolated \\(x\\) -values are equal.","title":"MATLAB Version"},{"location":"matlab-version/#changes-from-matlab-version","text":"This page documents changes from original MATLAB code .","title":"Changes from MATLAB Version"},{"location":"matlab-version/#exploratory-data-analysis-eda","text":"","title":"Exploratory Data Analysis (EDA)"},{"location":"matlab-version/#bug-fixes","text":"Only show statistically significant change points for the Pettitt and MKS plots. Fix bug causing the MKS test to only identify one change point, even if multiple change points were found to be above the threshold for statistical significance. Fix major bug causing the MKS test to identify change points based on the progressive series instead of the z-statistics of the crossing points. Remove unnecessary rounding in the moving window algorithm for AMS variability. Re-implement the Phillips-Perron and KPSS tests to account for drift and trend.","title":"Bug Fixes"},{"location":"matlab-version/#framework-changes","text":"If serial correlation is identified, do not run the Phillips-Perron and KPSS tests. Add the the Runs test to detect nonlinearity after fitting Sen's trend estimator. Run change point detection in multiple stages to prevent overpartitioning.","title":"Framework Changes"},{"location":"matlab-version/#flood-frequency-analysis-ffa","text":"","title":"Flood Frequency Analysis (FFA)"},{"location":"matlab-version/#distribution-changes","text":"The generalized Pareto (GPA) distribution has been removed, since its likelihood function is not amenable to maximum likelihood estimation. Issues occur because the GPA distribution is intended for peaks over threshold modelling, which we do not use. The R version uses the three parameter Weibull distribution (with location, scale, and shape) parameters instead of the two parameter Weibull distribution (with scale and shape parameters). This ensures consistency with the other distributions, which all have location parameters.","title":"Distribution Changes"},{"location":"matlab-version/#model-selection-changes","text":"The L-distance and L-kurtosis selection methods have been improved by using an optimization algorithm to find the parameters with the closest L-moments to the data instead of using a brute force approach. This is more computationally efficient and gives more precise results. The procedure for computing the Z-statistic selection metric has been changed. If the L-moments of the dataset do not satisfy \\(\\tau_{4} \\leq (1 + 5\\tau _{3}^2)/6\\) , then the Kappa distribution will not be fitted and the candidate distributions that use the dataset will be omitted.","title":"Model Selection Changes"},{"location":"matlab-version/#parameter-estimation-changes","text":"Parameterization of the PE3/LP3 distributions fails for some datasets because MATLAB is unable to handle the large numbers created by the gamma function. To manage this issue, the MATLAB version used the conventional moments (i.e. sample mean/variance/skewness) when this occurred. We avoid this problem in the R version by using the lgamma (log-gamma) function. The R implementation uses L-BFGS-B for MLE/GMLE parameter estimation instead of Nelder-Mead, since the gradient is well defined for the likelihood functions we are working with. Additionally, the L-BFGS-B method makes it possible to assign bounds to the variables. This modification produced slight improvements to the MLL/GMLL for some datasets.","title":"Parameter Estimation Changes"},{"location":"matlab-version/#uncertainty-quantification","text":"Implement RFPL uncertainty quantification for the Weibull distribution.","title":"Uncertainty Quantification"},{"location":"matlab-version/#model-assessment-changes","text":"Use the built-in R function approx() to perform log-linear interpolation of the return periods. The MATLAB implementation uses a hard-coded algorithm which behaves unpredictably when the original and interpolated \\(x\\) -values are equal.","title":"Model Assessment Changes"},{"location":"model-assessment/","text":"Model Assessment Nonparametric Models A Plotting Position is a non-parametric estimator used to derive empirical exceedance probabilities. By using the plotting position, we can evaluate the quality of our parametric model (assuming the model is stationary). To compute the plotting position, arrange the sample observations in descending order of magnitude: \\(x_{n:n} \\geq \\dots \\geq x_{1:n}\\) . Then, the empirical exceedance probabilities are given by the following formula: \\[ p_{i:n} = \\frac{i-a}{n+1 - 2a}, \\quad i \\in \\{1, \\dots , n\\} \\] The coefficient \\(a\\) depends on the plotting position formula: Formula \\(a\\) Simplified Equation Weibull \\(0\\) \\(p_{i:n} = \\frac{i}{n +1}\\) Blom \\(0.375\\) \\(p_{i:n} = \\frac{i-0.375}{n + 0.25}\\) Cunnane \\(0.4\\) \\(p_{i:n} = \\frac{i-0.4}{n+0.2}\\) Gringorten \\(0.44\\) \\(p_{i:n} = \\frac{i-0.44}{n + 0.12}\\) Hazen \\(0.5\\) \\(p_{i:n} = \\frac{i-0.5}{n}\\) By default, the FFA framework uses the Weibull formula, which is unbiased. Accuracy Statistics \\(R^2\\) - Coefficient of Determination To compute the \\(R^2\\) statistic, we perform a linear regression of the annual maximum series data against the predictions of the parametric model at the plotting positions. The \\(R^2\\) statistic describes how well the parametric model captures variance in the data. Higher is better. The plot below shows the deviation of the estimated quantiles (red dots), from the data (black line). RMSE - Root-Mean Squared Error The RMSE statistic describes the average squared difference between the data and the predictions of the parametric model. Lower is better. Bias The Bias statistic describes the average difference between the data and the predictions of the parametric model. A positive bias indicates that the model tends to overestimate the data while a negative bias indicates that the model tends to underestimate the data. Information Criterion The Akaike Information Criterion ( AIC ) and Bayesian Information Criterion ( BIC ) describe the quality of a model based on the error ( RMSE ) and the number of parameters ( n_theta ). Better models have a lower AIC / BIC , which indicates that they have less parameters and lower error. AIC <- (n * log(RMSE)) + (2 * n_theta) BIC <- (n * log(RMSE)) + (log(n) * n_theta) The Akaike/Bayesian information criterion can also be computed using the maximum log-likelihood from maximum likelihood estimation . These statistics are reported as AIC_MLL and BIC_MLL . AIC_MLL <- (n * log(MLL)) + (2 * n_theta) BIC_MLL <- (n * log(MLL)) + (log(n) * n_theta) Uncertainty Statistics The FFA framework uses three statistics to assess the uncertainty in flood quantile estimates: AW captures precision (narrower confidence intervals are better). POC captures reliability (higher coverage of observations is better). CWI is a composite measure balancing both precision and reliability (lower is better). We use these metrics together to evaluate the robustness of the flood frequency analysis. AW \u2013 Average Width AW is the average width of the interpolated confidence intervals across return periods of interest. A smaller AW indicates more precise quantile estimates. To compute AW , we use log-linear interpolation to estimate the confidence intervals of the empirical exceedance probabilities from the confidence intervals computed during uncertainty quantification . POC \u2013 Percent of Coverage POC is the percentage of observed quantiles that fall within their corresponding confidence intervals. A higher POC indicates greater reliability of the confidence intervals. CWI \u2013 Confidence Width Indicator CWI is a composite metric that penalizes wide and/or poorly calibrated confidence intervals. A lower CWI is better. Wide intervals and low coverage increase the penalty. Ideal confidence intervals are both narrow and well-calibrated, resulting in a low CWI . The CWI is computed using the following formula, where alpha is the significance level. CWI <- AW * exp((1 - alpha) - POC / 100)^2;","title":"Model Assessment"},{"location":"model-assessment/#model-assessment","text":"","title":"Model Assessment"},{"location":"model-assessment/#nonparametric-models","text":"A Plotting Position is a non-parametric estimator used to derive empirical exceedance probabilities. By using the plotting position, we can evaluate the quality of our parametric model (assuming the model is stationary). To compute the plotting position, arrange the sample observations in descending order of magnitude: \\(x_{n:n} \\geq \\dots \\geq x_{1:n}\\) . Then, the empirical exceedance probabilities are given by the following formula: \\[ p_{i:n} = \\frac{i-a}{n+1 - 2a}, \\quad i \\in \\{1, \\dots , n\\} \\] The coefficient \\(a\\) depends on the plotting position formula: Formula \\(a\\) Simplified Equation Weibull \\(0\\) \\(p_{i:n} = \\frac{i}{n +1}\\) Blom \\(0.375\\) \\(p_{i:n} = \\frac{i-0.375}{n + 0.25}\\) Cunnane \\(0.4\\) \\(p_{i:n} = \\frac{i-0.4}{n+0.2}\\) Gringorten \\(0.44\\) \\(p_{i:n} = \\frac{i-0.44}{n + 0.12}\\) Hazen \\(0.5\\) \\(p_{i:n} = \\frac{i-0.5}{n}\\) By default, the FFA framework uses the Weibull formula, which is unbiased.","title":"Nonparametric Models"},{"location":"model-assessment/#accuracy-statistics","text":"","title":"Accuracy Statistics"},{"location":"model-assessment/#r2-coefficient-of-determination","text":"To compute the \\(R^2\\) statistic, we perform a linear regression of the annual maximum series data against the predictions of the parametric model at the plotting positions. The \\(R^2\\) statistic describes how well the parametric model captures variance in the data. Higher is better. The plot below shows the deviation of the estimated quantiles (red dots), from the data (black line).","title":"\\(R^2\\) - Coefficient of Determination"},{"location":"model-assessment/#rmse-root-mean-squared-error","text":"The RMSE statistic describes the average squared difference between the data and the predictions of the parametric model. Lower is better.","title":"RMSE - Root-Mean Squared Error"},{"location":"model-assessment/#bias","text":"The Bias statistic describes the average difference between the data and the predictions of the parametric model. A positive bias indicates that the model tends to overestimate the data while a negative bias indicates that the model tends to underestimate the data.","title":"Bias"},{"location":"model-assessment/#information-criterion","text":"The Akaike Information Criterion ( AIC ) and Bayesian Information Criterion ( BIC ) describe the quality of a model based on the error ( RMSE ) and the number of parameters ( n_theta ). Better models have a lower AIC / BIC , which indicates that they have less parameters and lower error. AIC <- (n * log(RMSE)) + (2 * n_theta) BIC <- (n * log(RMSE)) + (log(n) * n_theta) The Akaike/Bayesian information criterion can also be computed using the maximum log-likelihood from maximum likelihood estimation . These statistics are reported as AIC_MLL and BIC_MLL . AIC_MLL <- (n * log(MLL)) + (2 * n_theta) BIC_MLL <- (n * log(MLL)) + (log(n) * n_theta)","title":"Information Criterion"},{"location":"model-assessment/#uncertainty-statistics","text":"The FFA framework uses three statistics to assess the uncertainty in flood quantile estimates: AW captures precision (narrower confidence intervals are better). POC captures reliability (higher coverage of observations is better). CWI is a composite measure balancing both precision and reliability (lower is better). We use these metrics together to evaluate the robustness of the flood frequency analysis.","title":"Uncertainty Statistics"},{"location":"model-assessment/#aw-average-width","text":"AW is the average width of the interpolated confidence intervals across return periods of interest. A smaller AW indicates more precise quantile estimates. To compute AW , we use log-linear interpolation to estimate the confidence intervals of the empirical exceedance probabilities from the confidence intervals computed during uncertainty quantification .","title":"AW \u2013 Average Width"},{"location":"model-assessment/#poc-percent-of-coverage","text":"POC is the percentage of observed quantiles that fall within their corresponding confidence intervals. A higher POC indicates greater reliability of the confidence intervals.","title":"POC \u2013 Percent of Coverage"},{"location":"model-assessment/#cwi-confidence-width-indicator","text":"CWI is a composite metric that penalizes wide and/or poorly calibrated confidence intervals. A lower CWI is better. Wide intervals and low coverage increase the penalty. Ideal confidence intervals are both narrow and well-calibrated, resulting in a low CWI . The CWI is computed using the following formula, where alpha is the significance level. CWI <- AW * exp((1 - alpha) - POC / 100)^2;","title":"CWI \u2013 Confidence Width Indicator"},{"location":"model-selection/","text":"Model Selection This module selects a statistical model for S-FFA or NS-FFA based on the annual maximum series. S-FFA : A time-invariant probability distribution is selected from the candidate distributions. NS-FFA : A distribution is chosen along with a nonstationary structure to capture its evolution over time. In piecewise NS-FFA, the series is segmented into subperiods, each modelled with either time-invariant or time-varying distributions. The framework uses the L-moment ratio method to identify the best-fit distribution family by comparing sample L-moments with the L-moments of various distribution families. For NS-FFA, the series is decomposed to isolate its stationary component following Vidrio-Sahag\u00fan and He (2022) . This decomposed sample is then used for distribution selection, as in S-FFA. An Introduction to L-Moments Definition 1 : The \\(k\\) -th Order Statistic of a statistical sample is its \\(k\\) -th smallest value. Definition 2 : The \\(r\\) -th Population L-moment \\(\\lambda_{r}\\) is a linear combination of the expectation of the order statistics. Let \\(X_{k:n}\\) be the \\(k\\) -th order statistic from a sample of size \\(n\\) . Then, \\[ \\lambda_{r} = \\frac{1}{r} \\sum_{k=0}^{r-1} (-1)^{k} \\binom{r-1}{k} \\mathbb{E}[X_{r-k:r}] \\] Definition 3 : A Probability Weighted Moment (PWM) encodes information about a value's position on the cumulative distribution function. The \\(r\\) -th PWM, denoted \\(\\beta_{r}\\) , is: \\[ \\beta_{r} = \\mathbb{E}[X \\cdot F(X)^{r}] \\] For an ordered sample \\(x_{1:n} \\leq \\dots \\leq x_{n:n}\\) , the sample PWM is often estimated as: \\[ b_{r} = \\frac{1}{n} \\sum_{i=1}^{r} x_{i:n} \\left(\\frac{i-1}{n-1}\\right) ^{r} \\] Sample L-Moments (from PWMs) and L-Moment Ratios The first four sample L-moments can be computed as linear combinations of the PWMs: \\[ \\begin{aligned} l_{1} &= b_{0} \\\\ l_{2} &= 2b_{1} - b_{0} \\\\ l_{3} &= 6b_{2} - 6b_{1} + b_{0} \\\\ l_{4} &= 20b_{3} - 30b_{2} + 12b_{1} - b_{0} \\end{aligned} \\] The L-moments are used to compute the Sample L-variance \\(t_{2}\\) , Sample L-skewness \\(t_{3}\\) and the Sample L-kurtosis \\(t_{4}\\) using the following formulas: \\[ \\begin{aligned} t_{2} &= l_{2} / l_{1} \\\\ t_{3} &= l_{3} / l_{2} \\\\ t_{4} &= l_{4} / l_{2} \\end{aligned} \\] Then, we compare these statistics, specifically the L-skewness and L-kurtosis to their theoretical values (given here ) using one of three different metrics to select a distribution. Note : Probability distributions with two parameters have constant L-skewness \\(\\tau_{3}\\) and L-kurtosis \\(\\tau_{4}\\) regardless of their parameters. The L-skewness and L-kurtosis of probability distributions with three parameters is a function of the shape parameter \\(\\kappa\\) . The notation \\(\\tau_{3}(\\kappa)\\) and \\(\\tau_{4}(\\kappa)\\) refers to the L-skewness and L-kurtosis curves for three parameter distributions. Example Plot Shown below are the L-moment curves of the GEV, GLO, GNO, PE3/LP3, and WEI distributions as well as the L-moment ratios of the two parameter distributions GUM and NOR/LNO. This L-moment diagram depicts the \"L-distance\" selection metric, which compares the euclidian distance between the sample and theoretical L-moment ratios. The inset shows that the GEV distribution (yellow line) has the closest L-moments to the data. Selection Metrics 1. L-Distance The Euclidean distance between the sample \\((t_3, t_4)\\) and theoretical \\((\\tau_3, \\tau_4)\\) for each candidate distribution. For 3-parameter distributions, this is the minimum distance along their L-moment ratio curve. 2. L-Kurtosis The L-kurtosis method is only used for three-parameter probability distributions. First, the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) is identified. Then, the difference between the sample L-kurtosis and the theoretical L-kurtosis is computed using the metric \\(|\\tau_{4}(\\kappa ^{*}) - t_{4} |\\) . 3. Z-statistic The Z-statistic selection metric is calculated as follows (for three-parameter distributions): Fit the four-parameter Kappa (K4D) distribution to the sample. Generate \\(N_{\\text{sim}}\\) bootstrap samples from the fitted K4D distribution. Calculate the sample L-kurtosis \\(t_{4}^{[i]}\\) of each synthetic dataset. Calculate the bias and standard deviation of the bootstrap distribution: \\[ B_{4} = N_{\\text{sim} }^{-1} \\sum_{i = 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right) \\] \\[ \\sigma _{4} = \\left[(N_{\\text{sim} } - 1)^{-1} \\left\\{\\sum_{i - 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right)^2 - N_{\\text{sim} } B_{4}^2\\right\\} \\right] ^{\\frac{1}{2}} \\] Identify the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) . Use the bootstrap distribution to compute the Z-statistic for each distribution: \\[ z = \\frac{\\tau_{4} (\\kappa ^{*}) - t_{4} + B_{4} }{ \\sigma _{4}} \\] Choose the distribution with the smallest Z-statistic. Handling Nonstationarity When nonstationarity is detected, the annual maximum series is decomposed before model selection. We consider three nonstationary scenarios that can be identified in EDA: Trend in mean only. Trend in standard deviation only. Trend in both mean and standard deviation. Decomposition Steps Scenario 1: Trend in mean Use Sen's Trend Estimator to approximate the slope \\(b_1\\) and intercept \\(b_0\\) . Detrend: subtract the linear function \\((b_{1} \\cdot \\text{Covariate})\\) from the time series, where the covariate is a time index calculated using the formula \\((\\text{Years} - 1900) / 100\\) . Ensure positivity: if necessary, shift series by adding a constant such that \\(\\min(\\text{data}) = 1\\) . Scenario 2: Trend in standard deviation Generate a time series of standard deviations using the moving windows method. Use Sen's Trend Estimator to identify the slope \\(c_{1}\\) and intercept \\(c_{0}\\) of the trend in the standard deviations. Normalize the data to have mean \\(0\\) , then divide out the scale factor \\(g_{t}\\) . \\[ g_{t} = \\frac{(c_{1} \\cdot \\text{Covariate} ) + c_{0}}{c_{0}} \\] Add back the long-term mean \\(\\mu\\) , and then ensure positivity as in Scenario 1. Scenario 3: Trend in both mean and standard deviation Remove the linear trend in mean exactly as in Scenario 1. On that detrended series, generate a rolling\u2010window STD series and fit its trend. Divide the detrended data by the time-varying scale factor \\(g_{t}\\) (as in Scenario 2). Shift to preserve the series mean and ensure positivity.","title":"Model Selection"},{"location":"model-selection/#model-selection","text":"This module selects a statistical model for S-FFA or NS-FFA based on the annual maximum series. S-FFA : A time-invariant probability distribution is selected from the candidate distributions. NS-FFA : A distribution is chosen along with a nonstationary structure to capture its evolution over time. In piecewise NS-FFA, the series is segmented into subperiods, each modelled with either time-invariant or time-varying distributions. The framework uses the L-moment ratio method to identify the best-fit distribution family by comparing sample L-moments with the L-moments of various distribution families. For NS-FFA, the series is decomposed to isolate its stationary component following Vidrio-Sahag\u00fan and He (2022) . This decomposed sample is then used for distribution selection, as in S-FFA.","title":"Model Selection"},{"location":"model-selection/#an-introduction-to-l-moments","text":"Definition 1 : The \\(k\\) -th Order Statistic of a statistical sample is its \\(k\\) -th smallest value. Definition 2 : The \\(r\\) -th Population L-moment \\(\\lambda_{r}\\) is a linear combination of the expectation of the order statistics. Let \\(X_{k:n}\\) be the \\(k\\) -th order statistic from a sample of size \\(n\\) . Then, \\[ \\lambda_{r} = \\frac{1}{r} \\sum_{k=0}^{r-1} (-1)^{k} \\binom{r-1}{k} \\mathbb{E}[X_{r-k:r}] \\] Definition 3 : A Probability Weighted Moment (PWM) encodes information about a value's position on the cumulative distribution function. The \\(r\\) -th PWM, denoted \\(\\beta_{r}\\) , is: \\[ \\beta_{r} = \\mathbb{E}[X \\cdot F(X)^{r}] \\] For an ordered sample \\(x_{1:n} \\leq \\dots \\leq x_{n:n}\\) , the sample PWM is often estimated as: \\[ b_{r} = \\frac{1}{n} \\sum_{i=1}^{r} x_{i:n} \\left(\\frac{i-1}{n-1}\\right) ^{r} \\]","title":"An Introduction to L-Moments"},{"location":"model-selection/#sample-l-moments-from-pwms-and-l-moment-ratios","text":"The first four sample L-moments can be computed as linear combinations of the PWMs: \\[ \\begin{aligned} l_{1} &= b_{0} \\\\ l_{2} &= 2b_{1} - b_{0} \\\\ l_{3} &= 6b_{2} - 6b_{1} + b_{0} \\\\ l_{4} &= 20b_{3} - 30b_{2} + 12b_{1} - b_{0} \\end{aligned} \\] The L-moments are used to compute the Sample L-variance \\(t_{2}\\) , Sample L-skewness \\(t_{3}\\) and the Sample L-kurtosis \\(t_{4}\\) using the following formulas: \\[ \\begin{aligned} t_{2} &= l_{2} / l_{1} \\\\ t_{3} &= l_{3} / l_{2} \\\\ t_{4} &= l_{4} / l_{2} \\end{aligned} \\] Then, we compare these statistics, specifically the L-skewness and L-kurtosis to their theoretical values (given here ) using one of three different metrics to select a distribution. Note : Probability distributions with two parameters have constant L-skewness \\(\\tau_{3}\\) and L-kurtosis \\(\\tau_{4}\\) regardless of their parameters. The L-skewness and L-kurtosis of probability distributions with three parameters is a function of the shape parameter \\(\\kappa\\) . The notation \\(\\tau_{3}(\\kappa)\\) and \\(\\tau_{4}(\\kappa)\\) refers to the L-skewness and L-kurtosis curves for three parameter distributions.","title":"Sample L-Moments (from PWMs) and L-Moment Ratios"},{"location":"model-selection/#example-plot","text":"Shown below are the L-moment curves of the GEV, GLO, GNO, PE3/LP3, and WEI distributions as well as the L-moment ratios of the two parameter distributions GUM and NOR/LNO. This L-moment diagram depicts the \"L-distance\" selection metric, which compares the euclidian distance between the sample and theoretical L-moment ratios. The inset shows that the GEV distribution (yellow line) has the closest L-moments to the data.","title":"Example Plot"},{"location":"model-selection/#selection-metrics","text":"","title":"Selection Metrics"},{"location":"model-selection/#1-l-distance","text":"The Euclidean distance between the sample \\((t_3, t_4)\\) and theoretical \\((\\tau_3, \\tau_4)\\) for each candidate distribution. For 3-parameter distributions, this is the minimum distance along their L-moment ratio curve.","title":"1. L-Distance"},{"location":"model-selection/#2-l-kurtosis","text":"The L-kurtosis method is only used for three-parameter probability distributions. First, the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) is identified. Then, the difference between the sample L-kurtosis and the theoretical L-kurtosis is computed using the metric \\(|\\tau_{4}(\\kappa ^{*}) - t_{4} |\\) .","title":"2. L-Kurtosis"},{"location":"model-selection/#3-z-statistic","text":"The Z-statistic selection metric is calculated as follows (for three-parameter distributions): Fit the four-parameter Kappa (K4D) distribution to the sample. Generate \\(N_{\\text{sim}}\\) bootstrap samples from the fitted K4D distribution. Calculate the sample L-kurtosis \\(t_{4}^{[i]}\\) of each synthetic dataset. Calculate the bias and standard deviation of the bootstrap distribution: \\[ B_{4} = N_{\\text{sim} }^{-1} \\sum_{i = 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right) \\] \\[ \\sigma _{4} = \\left[(N_{\\text{sim} } - 1)^{-1} \\left\\{\\sum_{i - 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right)^2 - N_{\\text{sim} } B_{4}^2\\right\\} \\right] ^{\\frac{1}{2}} \\] Identify the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) . Use the bootstrap distribution to compute the Z-statistic for each distribution: \\[ z = \\frac{\\tau_{4} (\\kappa ^{*}) - t_{4} + B_{4} }{ \\sigma _{4}} \\] Choose the distribution with the smallest Z-statistic.","title":"3. Z-statistic"},{"location":"model-selection/#handling-nonstationarity","text":"When nonstationarity is detected, the annual maximum series is decomposed before model selection. We consider three nonstationary scenarios that can be identified in EDA: Trend in mean only. Trend in standard deviation only. Trend in both mean and standard deviation.","title":"Handling Nonstationarity"},{"location":"model-selection/#decomposition-steps","text":"","title":"Decomposition Steps"},{"location":"model-selection/#scenario-1-trend-in-mean","text":"Use Sen's Trend Estimator to approximate the slope \\(b_1\\) and intercept \\(b_0\\) . Detrend: subtract the linear function \\((b_{1} \\cdot \\text{Covariate})\\) from the time series, where the covariate is a time index calculated using the formula \\((\\text{Years} - 1900) / 100\\) . Ensure positivity: if necessary, shift series by adding a constant such that \\(\\min(\\text{data}) = 1\\) .","title":"Scenario 1: Trend in mean"},{"location":"model-selection/#scenario-2-trend-in-standard-deviation","text":"Generate a time series of standard deviations using the moving windows method. Use Sen's Trend Estimator to identify the slope \\(c_{1}\\) and intercept \\(c_{0}\\) of the trend in the standard deviations. Normalize the data to have mean \\(0\\) , then divide out the scale factor \\(g_{t}\\) . \\[ g_{t} = \\frac{(c_{1} \\cdot \\text{Covariate} ) + c_{0}}{c_{0}} \\] Add back the long-term mean \\(\\mu\\) , and then ensure positivity as in Scenario 1.","title":"Scenario 2: Trend in standard deviation"},{"location":"model-selection/#scenario-3-trend-in-both-mean-and-standard-deviation","text":"Remove the linear trend in mean exactly as in Scenario 1. On that detrended series, generate a rolling\u2010window STD series and fit its trend. Divide the detrended data by the time-varying scale factor \\(g_{t}\\) (as in Scenario 2). Shift to preserve the series mean and ensure positivity.","title":"Scenario 3: Trend in both mean and standard deviation"},{"location":"parameter-estimation/","text":"Parameter Estimation This module estimates parameters for both S-FFA and NS-FFA. In NS-FFA, parameter estimation also involves estimating regression coefficients for time-varying parameters. The framework supports three estimation methods: L-moments Maximum Likelihood (MLE) Generalized Maximum Likelihood (GMLE) Note : We adopt the GEV distribution convention from Coles (2001) 1 , where a positive shape parameter \\(\\kappa\\) indicates a heavy tail. This differs from the convention used by some other sources. L-Moments The L-moments parameter estimation method is implemented for all distributions in S-FFA. This method uses the sample L-moments ( \\(l_1\\) , \\(l_2\\) ) and L-moment ratios ( \\(t_3\\) , \\(t_4\\) ) to estimate parameters. For more information about L-moments, see here . Warning : L-moment-based estimates can yield distributions which do not have support at small values. However, this is typically not an issue for quantile estimation of mid- to high-return periods. Maximum Likelihood (MLE) MLE is implemented for all distributions across both S-FFA and NS-FFA. Maximum likelihood estimation aims to maximize the log-likelihood function \\(\\ell(x : \\theta)\\) of the data \\(x = x_{1}, \\dots , x_{n}\\) given the parameters \\(\\theta\\) . The log-likelihood functions for each distribution are defined here . To find the optimal parameters, we use the nlminb function from the stats library. This function implements the \"L-BFGS-B\" algorithm for box-constrained optimization. Generalized Maximum Likelihood (GMLE) GMLE is used for GEV models when incorporating prior knowledge 2 of the shape parameter \\(\\kappa\\) using Bayesian reasoning via maximum a posteriori estimation , which maximizes the product of the likelihood and the prior distribution. Suppose that \\(\\kappa\\) is drawn from \\(K \\sim \\text{Beta}(p, q)\\) where \\(p\\) and \\(q\\) are determined using prior knowledge. The prior PDF \\(f_{K}(\\kappa)\\) is shown below, where \\(B(p, q)\\) is the Beta function . \\[ f_{K}(\\kappa) = \\frac{\\kappa ^{p - 1}(1 - \\kappa)^{q-1}}{B(p, q)} \\] As in the case of regular maximum likelihood estimation, the likelihood function is: \\[ f_{X}(x : \\mu, \\sigma, \\kappa) =\\prod_{i=1}^{n} \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] As mentioned previously, we want to maximize the product \\(\\mathcal{L} = f_{K}(\\kappa)f_{X}(x:\\mu ,\\sigma ,\\kappa)\\) . To ensure numerical stability, we will maximize \\(\\ln (\\mathcal{L})\\) instead, which has the following form: \\[ \\begin{aligned} \\ln(\\mathcal{L}) &= \\ln(f_{K}(\\kappa)) + \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) \\\\[10pt] \\ln(f_{K}(\\kappa)) &= (p - 1)\\ln \\kappa + (q-1) \\ln (1 - \\kappa) - \\ln (B(p, q)) \\\\[5pt] \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) &= \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\end{aligned} \\] Coles, S. (2001). An introduction to statistical modeling of extreme values . Springer. \u21a9 Martins, E. S., and Stedinger, J. R. (2000). Generalized maximum-likelihood generalized extreme-value quantile estimators for hydrologic data. Water Resources Research, 36(3), 737\u2013744. \\doi{10.1029/1999WR900330} \u21a9","title":"Parameter Estimation"},{"location":"parameter-estimation/#parameter-estimation","text":"This module estimates parameters for both S-FFA and NS-FFA. In NS-FFA, parameter estimation also involves estimating regression coefficients for time-varying parameters. The framework supports three estimation methods: L-moments Maximum Likelihood (MLE) Generalized Maximum Likelihood (GMLE) Note : We adopt the GEV distribution convention from Coles (2001) 1 , where a positive shape parameter \\(\\kappa\\) indicates a heavy tail. This differs from the convention used by some other sources.","title":"Parameter Estimation"},{"location":"parameter-estimation/#l-moments","text":"The L-moments parameter estimation method is implemented for all distributions in S-FFA. This method uses the sample L-moments ( \\(l_1\\) , \\(l_2\\) ) and L-moment ratios ( \\(t_3\\) , \\(t_4\\) ) to estimate parameters. For more information about L-moments, see here . Warning : L-moment-based estimates can yield distributions which do not have support at small values. However, this is typically not an issue for quantile estimation of mid- to high-return periods.","title":"L-Moments"},{"location":"parameter-estimation/#maximum-likelihood-mle","text":"MLE is implemented for all distributions across both S-FFA and NS-FFA. Maximum likelihood estimation aims to maximize the log-likelihood function \\(\\ell(x : \\theta)\\) of the data \\(x = x_{1}, \\dots , x_{n}\\) given the parameters \\(\\theta\\) . The log-likelihood functions for each distribution are defined here . To find the optimal parameters, we use the nlminb function from the stats library. This function implements the \"L-BFGS-B\" algorithm for box-constrained optimization.","title":"Maximum Likelihood (MLE)"},{"location":"parameter-estimation/#generalized-maximum-likelihood-gmle","text":"GMLE is used for GEV models when incorporating prior knowledge 2 of the shape parameter \\(\\kappa\\) using Bayesian reasoning via maximum a posteriori estimation , which maximizes the product of the likelihood and the prior distribution. Suppose that \\(\\kappa\\) is drawn from \\(K \\sim \\text{Beta}(p, q)\\) where \\(p\\) and \\(q\\) are determined using prior knowledge. The prior PDF \\(f_{K}(\\kappa)\\) is shown below, where \\(B(p, q)\\) is the Beta function . \\[ f_{K}(\\kappa) = \\frac{\\kappa ^{p - 1}(1 - \\kappa)^{q-1}}{B(p, q)} \\] As in the case of regular maximum likelihood estimation, the likelihood function is: \\[ f_{X}(x : \\mu, \\sigma, \\kappa) =\\prod_{i=1}^{n} \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] As mentioned previously, we want to maximize the product \\(\\mathcal{L} = f_{K}(\\kappa)f_{X}(x:\\mu ,\\sigma ,\\kappa)\\) . To ensure numerical stability, we will maximize \\(\\ln (\\mathcal{L})\\) instead, which has the following form: \\[ \\begin{aligned} \\ln(\\mathcal{L}) &= \\ln(f_{K}(\\kappa)) + \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) \\\\[10pt] \\ln(f_{K}(\\kappa)) &= (p - 1)\\ln \\kappa + (q-1) \\ln (1 - \\kappa) - \\ln (B(p, q)) \\\\[5pt] \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) &= \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\end{aligned} \\] Coles, S. (2001). An introduction to statistical modeling of extreme values . Springer. \u21a9 Martins, E. S., and Stedinger, J. R. (2000). Generalized maximum-likelihood generalized extreme-value quantile estimators for hydrologic data. Water Resources Research, 36(3), 737\u2013744. \\doi{10.1029/1999WR900330} \u21a9","title":"Generalized Maximum Likelihood (GMLE)"},{"location":"probability-distributions/","text":"Candidate Probability Distributions The FFA framework considers nine candidate probability distributions: Distribution Abbreviation Parameters Gumbel GUM \\(\\mu\\) (location), \\(\\sigma\\) (scale) Normal NOR \\(\\mu\\) (location), \\(\\sigma\\) (scale) Log-Normal LNO \\(\\mu\\) (location), \\(\\sigma\\) (scale) Generalized Extreme Value GEV \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Generalized Logistic Value GLO \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Generalized Normal GNO \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Pearson Type III PE3 \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Log-Pearson Type III LP3 \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Weibull WEI \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Each distribution also has three nonstationary variants: A trend in the location parameter \\(\\mu\\) (+1 parameter). A trend in the scale parameter \\(\\sigma\\) (+1 parameter). A trend in the location \\(\\mu\\) and the scale \\(\\sigma\\) (+2 parameters). The FFA framework also uses the four-parameter Kappa distribution (KAP) for the Z-statistic selection metric. The Kappa distribution generalizes the nine distributions listed above. List of Distributions 1 Gumbel (GUM) Distribution Support \\(-\\infty < x < \\infty\\) Quantiles \\(x(F) = \\mu - \\sigma \\log (-\\log F)\\) Likelihood Function Its probability density function (PDF) is: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma} \\exp \\left(-z_{i} - e^{-z_{i}}\\right) , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, is Log-likelihood function is: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - z_{i} - e^{-z_{i}} \\right] \\] L-Moments In the equations below, \\(\\gamma \\approx 0.5772\\) is Euler's constant . \\(\\lambda_{1} = \\mu + \\sigma \\gamma\\) \\(\\lambda_{2} = \\sigma \\log 2\\) \\(\\tau_{3} = \\log(9/8)/\\log 2 \\approx 0.1699\\) \\(\\tau_{4} = (16 \\log 2 - 10\\log 3) / \\log 2 \\approx 0.1504\\) We can also express the parameters in terms of the L-moments: \\(\\sigma = \\lambda_{2} / \\log 2\\) \\(\\mu = \\lambda_{1} - \\sigma \\gamma\\) Normal (NOR) Distribution Support \\(-\\infty < x < \\infty\\) Quantiles \\(x(F) = \\mu + \\sigma \\Phi^{-1}(F)\\) Likelihood Function Its probability density function (PDF) is: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi }}e^{-z_{i}^2/2} , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, its Log-likelihood function is: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln (\\sigma \\sqrt{2\\pi }) - \\frac{z_{i}^2}{2} \\right] \\] L-Moments \\(\\lambda_{1} = \\mu\\) \\(\\lambda_{2} = \\pi^{-1/2}\\sigma \\approx 0.5642\\sigma\\) \\(\\tau_{3} = 0\\) \\(\\tau_{4} = 30\\pi^{-1}\\arctan \\sqrt{2} - 9 \\approx 0.1226\\) We can also express the parameters in terms of the L-moments: \\(\\mu = \\lambda_{1}\\) \\(\\sigma = \\pi^{1/2}\\lambda_{2}\\) Log-Normal (LNO) Distribution Support \\(0 < x < \\infty\\) Quantiles \\(x(F) = \\exp(\\mu + \\sigma \\Phi^{-1}(F))\\) Likelihood Function To derive its likelihood, we use the fact that: \\[ \\text{Data} \\sim \\text{LNO} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{NOR} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LNO}}(x ; \\mu, \\sigma) = \\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma)}{x} \\] L-Moments See Normal Distribution . Generalized Extreme Value (GEV) Distribution Support \\[ \\begin{cases} \\mu + (\\sigma /\\kappa) \\leq x < \\infty & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] -\\infty < x \\leq \\mu + (\\sigma/\\kappa ) &\\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{dcases} \\mu + \\sigma (1 - (-\\log F)^{\\kappa })/\\kappa &\\kappa \\neq 0\\\\[5pt] \\mu - \\sigma \\log (-\\log F) &\\kappa = 0 \\end{dcases} \\] Likelihood Function Its probability density function (PDF) is (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] Therefore, its Log-likelihood is: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\] L-Moments The L-moments are defined for \\(\\kappa > -1\\) : \\(\\lambda_{1} = \\mu + \\sigma (1 - \\Gamma (1 + \\kappa)) / \\kappa\\) \\(\\lambda_{2} = \\sigma (1 - 2^{-\\kappa })\\Gamma (1 + \\kappa) / \\kappa\\) \\(\\tau_{3} = 2(1 - 3^{-\\kappa})/(1 - 2^{-\\kappa}) - 3\\) \\(\\tau_{4} = [5(1 - 4^{-\\kappa })-10(1-3^{-\\kappa}) + 6(1-2^{-\\kappa })]/(1 - 2^{-\\kappa })\\) To compute the parameters from the L-moments, we first compute \\(c\\) : \\[ c = \\frac{2}{3 + \\tau_{3}} - \\frac{\\log 2}{\\log 3} \\] Then, we use the following approximation 2 : \\[ \\begin{cases} \\kappa \\approx 7.8590c + 2.9554c^2 \\\\[5pt] \\sigma \\approx \\lambda_{2}\\kappa / (1 - 2^{-\\kappa })\\Gamma (1 + \\kappa) \\\\[5pt] \\mu \\approx \\lambda_{1} - \\sigma (1 - \\Gamma (1 + \\kappa )) / \\kappa \\end{cases} \\] Note : Other sources often use a different notation for the GEV distribution in which the sign of the shape parameter \\(\\kappa\\) is flipped. Generalized Logistic (GLO) Distribution Support \\[ \\begin{cases} -\\infty < x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] \\mu + (\\sigma /\\kappa ) \\leq x < \\infty & \\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu +\\sigma [1 - ((1 - F) / F)^{\\kappa}] / \\kappa &\\kappa \\neq 0 \\\\[5pt] \\mu - \\sigma \\log ((1 - F) / F) & k = 0 \\end{cases} \\] Likelihood Function Its probability density function (PDF) is (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{1}{\\sigma }t_{i}^{(1/\\kappa) - 1} \\left[1 + t_{i}^{1/\\kappa}\\right]^{-2}, \\quad t_{i} = 1 - \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) \\] Therefore, its Log-likelihood function is: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma + \\left(\\frac{1}{\\kappa }-1\\right) \\ln t_{i} - 2 \\ln \\left(1 + t_{i}^{1/\\kappa }\\right) \\right] \\] L-Moments The L-moments are defined for \\(-1 < \\kappa < 1\\) : \\(\\lambda_{1} = \\mu +\\sigma [(1 / \\kappa) - (\\pi / \\sin (\\kappa\\pi))]\\) \\(\\lambda_{2} = \\sigma \\kappa \\pi / \\sin (\\kappa \\pi)\\) \\(\\tau_{3} = -\\kappa\\) \\(\\tau_{4} = (1 + 5\\kappa ^2) / 6\\) We can also express the parameters in terms of the L-moments: \\(\\kappa = -\\tau_{3}\\) \\(\\sigma = \\lambda_{2}\\sin (\\kappa \\pi ) / \\kappa \\pi\\) \\(\\mu = \\lambda_{1} - \\sigma [(1 / \\kappa) - (\\pi / \\sin (\\kappa\\pi))]\\) Generalized Normal (GNO) Distribution Support \\[ \\begin{cases} -\\infty < x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] \\mu + (\\sigma /\\kappa ) \\leq x < \\infty & \\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu + \\sigma [1 - \\exp(-\\kappa \\Phi^{-1}(F))] / \\kappa &\\kappa \\neq 0 \\\\[5pt] \\mu + \\sigma \\Phi^{-1}(F) &\\kappa = 0 \\end{cases} \\] Likelihood Function L-Moments The L-moments are defined for all values of \\(\\kappa\\) . \\(\\lambda_{1} = \\mu + \\sigma (1 - e^{\\kappa ^2/2}) / \\kappa\\) \\(\\lambda_{2} = \\sigma e^{-\\kappa ^2/ 2}[1 - 2\\Phi (-\\kappa / \\sqrt{2})] / \\kappa\\) To compute \\(\\tau_{3}\\) and \\(\\tau_{4}\\) we use the following approximation: \\[ \\begin{aligned} \\tau_{3} &\\approx -\\kappa \\left(\\frac{A_{0} + A_{1}\\kappa ^2 + A_{2}\\kappa ^{4} + A_{3}\\kappa ^{6}}{1 + B_{1}\\kappa ^2 + B_{2}\\kappa ^{4} + B_{3}\\kappa ^{6}}\\right) \\\\[5pt] \\tau_{4} &\\approx \\tau_{4}^{0} + \\kappa ^2 \\left(\\frac{C_{0} + C_{1}\\kappa ^2 + C_{2}\\kappa ^{4} + C_{3}\\kappa ^{6}}{1 + D_{1}\\kappa ^2 + D_{2}\\kappa ^{4} + D_{3}\\kappa ^{6}}\\right) \\end{aligned} \\] To determine the parameters from the L-moments we also use a rational approximation: \\[ \\kappa \\approx -\\tau_{3} \\left(\\frac{E_{0} + E_{1}\\tau_{3}^2 + E_{2}\\tau_{3}^{4} + E_{3}\\tau _{3}^{6}}{1 + F_{1}\\tau _{3}^2 + F_{2}\\tau _{3}^{4} + F_{3}\\tau _{3}^{6}}\\right) \\] Then, we can find \\(\\mu\\) and \\(\\sigma\\) as a function of \\(\\kappa\\) : \\[ \\sigma \\approx \\frac{\\lambda_{2}\\kappa e^{-\\kappa ^2 / 2}}{1 - 2\\Phi (-\\kappa / \\sqrt{2})}, \\quad \\mu \\approx \\lambda_{1} - \\frac{\\sigma }{\\kappa }\\left(1 - e^{-\\kappa ^2 / 2 }\\right) \\] The coefficients ( \\(A_{i}\\) , \\(B_{i}\\) , \\(C_{i}\\) , \\(D_{i}\\) , \\(E_{i}\\) , \\(F_{i}\\) , and \\(\\tau_{4}^{0}\\) ) are defined in Appendix A.8 of Hosking, 1997 1 . Although this appendix covers the 3-parameter log-normal distribution, the L-moments of the generalized normal distribution are the same. Pearson Type III (PE3) Distribution The Pearson Type III distribution is typically reparameterized as follows for \\(\\kappa \\neq 0\\) : \\[ \\begin{aligned} \\alpha &= 4 / \\kappa^2 \\\\[5pt] \\beta &= \\sigma |\\kappa | / 2 \\\\[5pt] \\xi &= \\mu - 2\\sigma /\\kappa \\end{aligned} \\] Support \\[ \\begin{cases} \\xi \\leq x < \\infty &\\kappa > 0 \\\\[5pt] -\\infty < x < \\infty &\\kappa =0 \\\\[5pt] -\\infty < x \\leq \\xi &\\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu - \\alpha \\beta + q(F, \\alpha, \\beta) &\\kappa > 0\\\\[5pt] \\mu + \\sigma \\Phi^{-1}(F) &\\kappa = 0\\\\[5pt] \\mu + \\alpha \\beta - q(1 - F, \\alpha, \\beta) &\\kappa < 0 \\end{cases} \\] In the equations above, \\(q\\) is the quantile function of the Gamma distribution with shape \\(\\alpha\\) and scale \\(\\beta\\) . \\(q\\) is defined below, where \\(\\gamma\\) is the lower incomplete Gamma function . \\[q(F, \\alpha, \\beta) = \\beta \\gamma ^{-1}(\\alpha, p \\Gamma (\\alpha))\\] Likelihood Function The probability density function (PDF) of the PE3 distribution is given below: \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{(x_{i} - \\xi)^{\\alpha - 1}e^{-(x_{i} - \\xi )/\\beta }}{\\beta ^{\\alpha } \\Gamma (\\alpha )} \\] Therefore, its Log-likelihood function is: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[(\\alpha - 1) \\ln |x_{i} - \\xi | - \\frac{|x_{i} - \\xi |}{\\beta } - \\alpha \\ln\\beta - \\ln \\Gamma (\\alpha )\\right] \\] L-Moments All subsequent definitions assume that \\(\\kappa > 0\\) . If \\(\\kappa < 0\\) , the L-moments can be obtained by changing the signs of \\(\\lambda_{1}\\) , \\(\\tau_{3}\\) , and \\(\\xi\\) whenever they appear. If \\(\\kappa = 0\\) , the L-moments are the same as the Normal Distribution . The first two L-moments are defined as follows: \\(\\lambda_{1} = \\xi + \\alpha \\beta\\) \\(\\lambda_{2} = \\pi ^{-1/2} \\beta \\Gamma (\\alpha + 0.5) / \\Gamma (\\alpha )\\) Rational approximation is necessary to determine \\(\\tau_{3}\\) and \\(\\tau_{4}\\) . If \\(\\alpha \\geq 1\\) : \\[ \\begin{aligned} \\tau_{3} &\\approx \\alpha^{-1/2} \\left(\\frac{A_{0} + A_{1}\\alpha^{-1} + A_{2}\\alpha^{-2} + A_{3}\\alpha^{-3}}{1 + B_{1}\\alpha^{-1} + B_{2}\\alpha ^{-2}}\\right) \\\\[5pt] \\tau_{4} &\\approx \\frac{C_{0} + C_{1}\\alpha^{-1} + C_{2}\\alpha ^{-2} +C_{3}\\alpha ^{-3}}{1 + D_{1}\\alpha ^{-1} + D_{2}\\alpha ^{-2}} \\end{aligned} \\] If \\(\\alpha < 1\\) , we use a different set of coefficients: \\[ \\begin{aligned} \\tau_{3} &\\approx \\frac{1 + E_{1}\\alpha + E_{2}\\alpha ^2 + E_{3}\\alpha ^3}{1 + F_{1}\\alpha + F_{2}\\alpha ^2 + F_{3}\\alpha ^3} \\\\[5pt] \\tau_{4} &\\approx \\frac{1 + G_{1}\\alpha + G_{2}\\alpha ^2 + G_{3}\\alpha ^3}{1 + H_{1}\\alpha + H_{2}\\alpha ^2 + H_{3}\\alpha ^3} \\end{aligned} \\] Coefficients are given in Appendix A.9 of Hosking, 1997 1 . To estimate parameters from the L-moments, we use one of two approximations for \\(\\alpha\\) depending on the value of \\(\\tau_{3}\\) : \\[ \\alpha \\approx \\begin{dcases} \\frac{1 + 0.2906z}{z + 0.1882z^2 + 0.0442z^3}, &z = 3\\pi \\tau_{3}^2, &0 < |\\tau_{3}| < \\frac{1}{3} \\\\[5pt] \\frac{0.36067z - 0.59567z^2 + 0.25361z^3}{1 - 2.78861z + 2.56096z^2 - 0.77045z^3}, &z = 1 - |\\tau_{3}|, &\\frac{1}{3} \\leq |\\tau_{3}| < 1 \\end{dcases} \\] Then, we can determine the parameters from the approximated \\(\\alpha\\) : \\[ \\begin{aligned} \\kappa &= 2\\alpha ^{-1/2} \\text{sign} (\\tau_{3}) \\\\[5pt] \\sigma &= \\lambda_{2} \\pi^{1/2}\\alpha ^{1/2} \\Gamma (\\alpha )/\\Gamma (\\alpha + 0.5)\\\\[5pt] \\mu &= \\lambda_{1 } \\end{aligned} \\] Log-Pearson Type III (LP3) Distribution The LP3 distribution uses the same reparameterization as the PE3 distribution . Support \\[ \\begin{cases} \\max(0, \\xi) \\leq x < \\infty &\\kappa > 0 \\\\[5pt] 0 < x < \\infty &\\kappa =0 \\\\[5pt] 0 < x \\leq \\max(0, \\xi) &\\kappa < 0 \\end{cases} \\] Quantiles \\(x(F) = \\exp(x_{\\text{PE3}}(F ))\\) , where \\(x_{\\text{PE3}}(F)\\) is the quantile function of the PE3 distribution . Likelihood Function To derive the likelihood of the LP3 distribution, we use the fact that: \\[ \\text{Data} \\sim \\text{LP3} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{PE3} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LP3}}(x ; \\mu, \\sigma, \\kappa) = \\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa ) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa )}{x} \\] L-Moments Same as the PE3 distribution . Weibull (WEI) Distribution The Weibull distribution is implemented as a reparameterized version of the generalized extreme value distribution: \\[ \\begin{aligned} \\kappa &= 1 / \\kappa_{\\text{GEV}} \\\\[5pt] \\sigma &= \\kappa \\sigma_{\\text{GEV} } \\\\[5pt] \\mu &= \\sigma + \\mu_{\\text{GEV} } \\end{aligned} \\] Under this reparameterization, it is required that \\(\\sigma > 0\\) and \\(\\kappa > 0\\) . Support \\(\\mu \\leq x < \\infty\\) Quantiles \\(x(F) = \\mu + \\sigma (-\\log (1 - F))^{1/\\kappa}\\) Likelihood Function Its probability density function (PDF) is given below for \\(x_{i} > \\mu\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{\\kappa}{\\sigma }\\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa -1} \\exp \\left( - \\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa } \\right) \\] Therefore, its Log-likelihood function is: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[\\ln \\kappa - \\kappa \\ln \\sigma +(\\kappa -1)\\ln (x_{i}-\\mu ) - \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) ^{\\kappa } \\right] \\] L-Moments First, reparameterize the Weibull distribution to recover the GEV parameters: \\[ \\begin{aligned} \\kappa_{\\text{GEV}} &= 1 / \\kappa \\\\[5pt] \\sigma_{\\text{GEV}} &= \\sigma / \\kappa \\\\[5pt] \\end{aligned} \\] Next, compute the L-moments for the GEV distribution with \\(\\mu_{\\text{GEV}} = 0\\) . Then, \\(\\lambda_{1} = \\mu + \\sigma - \\lambda_{1, \\text{GEV}}\\) \\(\\lambda_{2} = \\lambda_{2, \\text{GEV}}\\) \\(\\tau_{3} = -\\tau_{3, \\text{GEV}}\\) \\(\\tau_{4} = \\tau_{4, \\text{GEV} }\\) To compute the parameters from the L-moments, first flip the sign of \\(\\lambda_{1}\\) and \\(\\tau_{3}\\) . Then, estimate the parameters of the GEV distribution to get \\(\\hat{\\mu}_{\\text{GEV}}\\) , \\(\\hat{\\sigma}_{\\text{GEV}}\\) , and \\(\\hat{\\kappa}_{\\text{GEV}}\\) . Finally, reparameterize the GEV parameters as shown here and then flip the sign of \\(\\mu\\) . Kappa (KAP) Distribution The Kappa distribution has location \\(\\mu\\) , scale \\(\\sigma\\) , and two shape parameters \\(\\kappa\\) and \\(h\\) . Support \\[ \\begin{cases} \\mu + \\sigma (1 - h^{-\\kappa}) \\leq x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0, h > 0 \\\\[5pt] -\\infty < x \\leq \\mu + (\\sigma /\\kappa) & \\kappa > 0, h \\leq 0 \\\\[5pt] \\mu + \\sigma (1 - h^{-\\kappa}) \\leq x < \\infty &\\kappa \\leq 0, h > 0 \\\\[5pt] \\mu + (\\sigma / \\kappa ) \\leq x <\\infty &\\kappa \\leq 0, h \\leq 0 \\end{cases} \\] Quantiles \\[ x(F) = \\mu + \\frac{\\sigma }{\\kappa }\\left[1 - \\left(\\frac{1 - F^{h}}{h}\\right)^{\\kappa }\\right] \\] L-Moments The L-moments are defined if \\(h \\geq 0\\) and \\(k > -1\\) or if \\(h < 0\\) and \\(-1 < k < -1/h\\) . \\(\\lambda_{1} = \\mu + \\sigma (1 - g_{1})/\\kappa\\) \\(\\lambda_{2} = \\sigma(g_{1} - g_{2})/\\kappa\\) \\(\\tau_{3} = (-g_{1} + 3g_{2} - 2g_{3}) / (g_{1} - g_{2})\\) \\(\\tau_{4} = (-g_{1} + 6g_{2} - 10g_{3} + 5g_{4}) / (g_{1} - g_{2})\\) In the expression above, \\(g_{r}\\) is defined as follows: \\[ g_{r} = \\begin{dcases} \\frac{r\\Gamma (1 + \\kappa )\\Gamma (r / h)}{h^{1 + \\kappa }\\Gamma (1 + \\kappa + r/h)} &h > 0 \\\\[5pt] \\frac{r\\Gamma (1 + \\kappa ) \\Gamma (-\\kappa - r/h)}{(-h)^{1 + \\kappa }\\Gamma (1 - r/h)} &h < 0 \\end{dcases} \\] There is no closed-form solution for the parameters in terms of the L-moments. However, \\(\\tau_{3}\\) and \\(\\tau_{4}\\) can be computed in terms of \\(\\kappa\\) and \\(h\\) using Newton-Raphson iteration. Sources Hosking, J.R.M. & Wallis, J.R., 1997. Regional frequency analysis: an aproach based on L-Moments. Cambridge University Press, New York, USA. \u21a9 \u21a9 \u21a9 Hosking, J.R.M., Wallis, J.R., & Wood, E.F., 1985. Estimation of the generalized extreme-value distribution by the method of probability-weighted moments. Technometrics, 27, 251-61. \u21a9","title":"Probability Distributions"},{"location":"probability-distributions/#candidate-probability-distributions","text":"The FFA framework considers nine candidate probability distributions: Distribution Abbreviation Parameters Gumbel GUM \\(\\mu\\) (location), \\(\\sigma\\) (scale) Normal NOR \\(\\mu\\) (location), \\(\\sigma\\) (scale) Log-Normal LNO \\(\\mu\\) (location), \\(\\sigma\\) (scale) Generalized Extreme Value GEV \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Generalized Logistic Value GLO \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Generalized Normal GNO \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Pearson Type III PE3 \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Log-Pearson Type III LP3 \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Weibull WEI \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Each distribution also has three nonstationary variants: A trend in the location parameter \\(\\mu\\) (+1 parameter). A trend in the scale parameter \\(\\sigma\\) (+1 parameter). A trend in the location \\(\\mu\\) and the scale \\(\\sigma\\) (+2 parameters). The FFA framework also uses the four-parameter Kappa distribution (KAP) for the Z-statistic selection metric. The Kappa distribution generalizes the nine distributions listed above.","title":"Candidate Probability Distributions"},{"location":"probability-distributions/#list-of-distributions","text":"","title":"List of Distributions"},{"location":"probability-distributions/#gumbel-gum-distribution","text":"Support \\(-\\infty < x < \\infty\\) Quantiles \\(x(F) = \\mu - \\sigma \\log (-\\log F)\\) Likelihood Function Its probability density function (PDF) is: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma} \\exp \\left(-z_{i} - e^{-z_{i}}\\right) , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, is Log-likelihood function is: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - z_{i} - e^{-z_{i}} \\right] \\] L-Moments In the equations below, \\(\\gamma \\approx 0.5772\\) is Euler's constant . \\(\\lambda_{1} = \\mu + \\sigma \\gamma\\) \\(\\lambda_{2} = \\sigma \\log 2\\) \\(\\tau_{3} = \\log(9/8)/\\log 2 \\approx 0.1699\\) \\(\\tau_{4} = (16 \\log 2 - 10\\log 3) / \\log 2 \\approx 0.1504\\) We can also express the parameters in terms of the L-moments: \\(\\sigma = \\lambda_{2} / \\log 2\\) \\(\\mu = \\lambda_{1} - \\sigma \\gamma\\)","title":"Gumbel (GUM) Distribution"},{"location":"probability-distributions/#normal-nor-distribution","text":"Support \\(-\\infty < x < \\infty\\) Quantiles \\(x(F) = \\mu + \\sigma \\Phi^{-1}(F)\\) Likelihood Function Its probability density function (PDF) is: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi }}e^{-z_{i}^2/2} , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, its Log-likelihood function is: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln (\\sigma \\sqrt{2\\pi }) - \\frac{z_{i}^2}{2} \\right] \\] L-Moments \\(\\lambda_{1} = \\mu\\) \\(\\lambda_{2} = \\pi^{-1/2}\\sigma \\approx 0.5642\\sigma\\) \\(\\tau_{3} = 0\\) \\(\\tau_{4} = 30\\pi^{-1}\\arctan \\sqrt{2} - 9 \\approx 0.1226\\) We can also express the parameters in terms of the L-moments: \\(\\mu = \\lambda_{1}\\) \\(\\sigma = \\pi^{1/2}\\lambda_{2}\\)","title":"Normal (NOR) Distribution"},{"location":"probability-distributions/#log-normal-lno-distribution","text":"Support \\(0 < x < \\infty\\) Quantiles \\(x(F) = \\exp(\\mu + \\sigma \\Phi^{-1}(F))\\) Likelihood Function To derive its likelihood, we use the fact that: \\[ \\text{Data} \\sim \\text{LNO} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{NOR} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LNO}}(x ; \\mu, \\sigma) = \\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma)}{x} \\] L-Moments See Normal Distribution .","title":"Log-Normal (LNO) Distribution"},{"location":"probability-distributions/#generalized-extreme-value-gev-distribution","text":"Support \\[ \\begin{cases} \\mu + (\\sigma /\\kappa) \\leq x < \\infty & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] -\\infty < x \\leq \\mu + (\\sigma/\\kappa ) &\\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{dcases} \\mu + \\sigma (1 - (-\\log F)^{\\kappa })/\\kappa &\\kappa \\neq 0\\\\[5pt] \\mu - \\sigma \\log (-\\log F) &\\kappa = 0 \\end{dcases} \\] Likelihood Function Its probability density function (PDF) is (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] Therefore, its Log-likelihood is: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\] L-Moments The L-moments are defined for \\(\\kappa > -1\\) : \\(\\lambda_{1} = \\mu + \\sigma (1 - \\Gamma (1 + \\kappa)) / \\kappa\\) \\(\\lambda_{2} = \\sigma (1 - 2^{-\\kappa })\\Gamma (1 + \\kappa) / \\kappa\\) \\(\\tau_{3} = 2(1 - 3^{-\\kappa})/(1 - 2^{-\\kappa}) - 3\\) \\(\\tau_{4} = [5(1 - 4^{-\\kappa })-10(1-3^{-\\kappa}) + 6(1-2^{-\\kappa })]/(1 - 2^{-\\kappa })\\) To compute the parameters from the L-moments, we first compute \\(c\\) : \\[ c = \\frac{2}{3 + \\tau_{3}} - \\frac{\\log 2}{\\log 3} \\] Then, we use the following approximation 2 : \\[ \\begin{cases} \\kappa \\approx 7.8590c + 2.9554c^2 \\\\[5pt] \\sigma \\approx \\lambda_{2}\\kappa / (1 - 2^{-\\kappa })\\Gamma (1 + \\kappa) \\\\[5pt] \\mu \\approx \\lambda_{1} - \\sigma (1 - \\Gamma (1 + \\kappa )) / \\kappa \\end{cases} \\] Note : Other sources often use a different notation for the GEV distribution in which the sign of the shape parameter \\(\\kappa\\) is flipped.","title":"Generalized Extreme Value (GEV) Distribution"},{"location":"probability-distributions/#generalized-logistic-glo-distribution","text":"Support \\[ \\begin{cases} -\\infty < x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] \\mu + (\\sigma /\\kappa ) \\leq x < \\infty & \\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu +\\sigma [1 - ((1 - F) / F)^{\\kappa}] / \\kappa &\\kappa \\neq 0 \\\\[5pt] \\mu - \\sigma \\log ((1 - F) / F) & k = 0 \\end{cases} \\] Likelihood Function Its probability density function (PDF) is (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{1}{\\sigma }t_{i}^{(1/\\kappa) - 1} \\left[1 + t_{i}^{1/\\kappa}\\right]^{-2}, \\quad t_{i} = 1 - \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) \\] Therefore, its Log-likelihood function is: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma + \\left(\\frac{1}{\\kappa }-1\\right) \\ln t_{i} - 2 \\ln \\left(1 + t_{i}^{1/\\kappa }\\right) \\right] \\] L-Moments The L-moments are defined for \\(-1 < \\kappa < 1\\) : \\(\\lambda_{1} = \\mu +\\sigma [(1 / \\kappa) - (\\pi / \\sin (\\kappa\\pi))]\\) \\(\\lambda_{2} = \\sigma \\kappa \\pi / \\sin (\\kappa \\pi)\\) \\(\\tau_{3} = -\\kappa\\) \\(\\tau_{4} = (1 + 5\\kappa ^2) / 6\\) We can also express the parameters in terms of the L-moments: \\(\\kappa = -\\tau_{3}\\) \\(\\sigma = \\lambda_{2}\\sin (\\kappa \\pi ) / \\kappa \\pi\\) \\(\\mu = \\lambda_{1} - \\sigma [(1 / \\kappa) - (\\pi / \\sin (\\kappa\\pi))]\\)","title":"Generalized Logistic (GLO) Distribution"},{"location":"probability-distributions/#generalized-normal-gno-distribution","text":"Support \\[ \\begin{cases} -\\infty < x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] \\mu + (\\sigma /\\kappa ) \\leq x < \\infty & \\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu + \\sigma [1 - \\exp(-\\kappa \\Phi^{-1}(F))] / \\kappa &\\kappa \\neq 0 \\\\[5pt] \\mu + \\sigma \\Phi^{-1}(F) &\\kappa = 0 \\end{cases} \\] Likelihood Function L-Moments The L-moments are defined for all values of \\(\\kappa\\) . \\(\\lambda_{1} = \\mu + \\sigma (1 - e^{\\kappa ^2/2}) / \\kappa\\) \\(\\lambda_{2} = \\sigma e^{-\\kappa ^2/ 2}[1 - 2\\Phi (-\\kappa / \\sqrt{2})] / \\kappa\\) To compute \\(\\tau_{3}\\) and \\(\\tau_{4}\\) we use the following approximation: \\[ \\begin{aligned} \\tau_{3} &\\approx -\\kappa \\left(\\frac{A_{0} + A_{1}\\kappa ^2 + A_{2}\\kappa ^{4} + A_{3}\\kappa ^{6}}{1 + B_{1}\\kappa ^2 + B_{2}\\kappa ^{4} + B_{3}\\kappa ^{6}}\\right) \\\\[5pt] \\tau_{4} &\\approx \\tau_{4}^{0} + \\kappa ^2 \\left(\\frac{C_{0} + C_{1}\\kappa ^2 + C_{2}\\kappa ^{4} + C_{3}\\kappa ^{6}}{1 + D_{1}\\kappa ^2 + D_{2}\\kappa ^{4} + D_{3}\\kappa ^{6}}\\right) \\end{aligned} \\] To determine the parameters from the L-moments we also use a rational approximation: \\[ \\kappa \\approx -\\tau_{3} \\left(\\frac{E_{0} + E_{1}\\tau_{3}^2 + E_{2}\\tau_{3}^{4} + E_{3}\\tau _{3}^{6}}{1 + F_{1}\\tau _{3}^2 + F_{2}\\tau _{3}^{4} + F_{3}\\tau _{3}^{6}}\\right) \\] Then, we can find \\(\\mu\\) and \\(\\sigma\\) as a function of \\(\\kappa\\) : \\[ \\sigma \\approx \\frac{\\lambda_{2}\\kappa e^{-\\kappa ^2 / 2}}{1 - 2\\Phi (-\\kappa / \\sqrt{2})}, \\quad \\mu \\approx \\lambda_{1} - \\frac{\\sigma }{\\kappa }\\left(1 - e^{-\\kappa ^2 / 2 }\\right) \\] The coefficients ( \\(A_{i}\\) , \\(B_{i}\\) , \\(C_{i}\\) , \\(D_{i}\\) , \\(E_{i}\\) , \\(F_{i}\\) , and \\(\\tau_{4}^{0}\\) ) are defined in Appendix A.8 of Hosking, 1997 1 . Although this appendix covers the 3-parameter log-normal distribution, the L-moments of the generalized normal distribution are the same.","title":"Generalized Normal (GNO) Distribution"},{"location":"probability-distributions/#pearson-type-iii-pe3-distribution","text":"The Pearson Type III distribution is typically reparameterized as follows for \\(\\kappa \\neq 0\\) : \\[ \\begin{aligned} \\alpha &= 4 / \\kappa^2 \\\\[5pt] \\beta &= \\sigma |\\kappa | / 2 \\\\[5pt] \\xi &= \\mu - 2\\sigma /\\kappa \\end{aligned} \\] Support \\[ \\begin{cases} \\xi \\leq x < \\infty &\\kappa > 0 \\\\[5pt] -\\infty < x < \\infty &\\kappa =0 \\\\[5pt] -\\infty < x \\leq \\xi &\\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu - \\alpha \\beta + q(F, \\alpha, \\beta) &\\kappa > 0\\\\[5pt] \\mu + \\sigma \\Phi^{-1}(F) &\\kappa = 0\\\\[5pt] \\mu + \\alpha \\beta - q(1 - F, \\alpha, \\beta) &\\kappa < 0 \\end{cases} \\] In the equations above, \\(q\\) is the quantile function of the Gamma distribution with shape \\(\\alpha\\) and scale \\(\\beta\\) . \\(q\\) is defined below, where \\(\\gamma\\) is the lower incomplete Gamma function . \\[q(F, \\alpha, \\beta) = \\beta \\gamma ^{-1}(\\alpha, p \\Gamma (\\alpha))\\] Likelihood Function The probability density function (PDF) of the PE3 distribution is given below: \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{(x_{i} - \\xi)^{\\alpha - 1}e^{-(x_{i} - \\xi )/\\beta }}{\\beta ^{\\alpha } \\Gamma (\\alpha )} \\] Therefore, its Log-likelihood function is: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[(\\alpha - 1) \\ln |x_{i} - \\xi | - \\frac{|x_{i} - \\xi |}{\\beta } - \\alpha \\ln\\beta - \\ln \\Gamma (\\alpha )\\right] \\] L-Moments All subsequent definitions assume that \\(\\kappa > 0\\) . If \\(\\kappa < 0\\) , the L-moments can be obtained by changing the signs of \\(\\lambda_{1}\\) , \\(\\tau_{3}\\) , and \\(\\xi\\) whenever they appear. If \\(\\kappa = 0\\) , the L-moments are the same as the Normal Distribution . The first two L-moments are defined as follows: \\(\\lambda_{1} = \\xi + \\alpha \\beta\\) \\(\\lambda_{2} = \\pi ^{-1/2} \\beta \\Gamma (\\alpha + 0.5) / \\Gamma (\\alpha )\\) Rational approximation is necessary to determine \\(\\tau_{3}\\) and \\(\\tau_{4}\\) . If \\(\\alpha \\geq 1\\) : \\[ \\begin{aligned} \\tau_{3} &\\approx \\alpha^{-1/2} \\left(\\frac{A_{0} + A_{1}\\alpha^{-1} + A_{2}\\alpha^{-2} + A_{3}\\alpha^{-3}}{1 + B_{1}\\alpha^{-1} + B_{2}\\alpha ^{-2}}\\right) \\\\[5pt] \\tau_{4} &\\approx \\frac{C_{0} + C_{1}\\alpha^{-1} + C_{2}\\alpha ^{-2} +C_{3}\\alpha ^{-3}}{1 + D_{1}\\alpha ^{-1} + D_{2}\\alpha ^{-2}} \\end{aligned} \\] If \\(\\alpha < 1\\) , we use a different set of coefficients: \\[ \\begin{aligned} \\tau_{3} &\\approx \\frac{1 + E_{1}\\alpha + E_{2}\\alpha ^2 + E_{3}\\alpha ^3}{1 + F_{1}\\alpha + F_{2}\\alpha ^2 + F_{3}\\alpha ^3} \\\\[5pt] \\tau_{4} &\\approx \\frac{1 + G_{1}\\alpha + G_{2}\\alpha ^2 + G_{3}\\alpha ^3}{1 + H_{1}\\alpha + H_{2}\\alpha ^2 + H_{3}\\alpha ^3} \\end{aligned} \\] Coefficients are given in Appendix A.9 of Hosking, 1997 1 . To estimate parameters from the L-moments, we use one of two approximations for \\(\\alpha\\) depending on the value of \\(\\tau_{3}\\) : \\[ \\alpha \\approx \\begin{dcases} \\frac{1 + 0.2906z}{z + 0.1882z^2 + 0.0442z^3}, &z = 3\\pi \\tau_{3}^2, &0 < |\\tau_{3}| < \\frac{1}{3} \\\\[5pt] \\frac{0.36067z - 0.59567z^2 + 0.25361z^3}{1 - 2.78861z + 2.56096z^2 - 0.77045z^3}, &z = 1 - |\\tau_{3}|, &\\frac{1}{3} \\leq |\\tau_{3}| < 1 \\end{dcases} \\] Then, we can determine the parameters from the approximated \\(\\alpha\\) : \\[ \\begin{aligned} \\kappa &= 2\\alpha ^{-1/2} \\text{sign} (\\tau_{3}) \\\\[5pt] \\sigma &= \\lambda_{2} \\pi^{1/2}\\alpha ^{1/2} \\Gamma (\\alpha )/\\Gamma (\\alpha + 0.5)\\\\[5pt] \\mu &= \\lambda_{1 } \\end{aligned} \\]","title":"Pearson Type III (PE3) Distribution"},{"location":"probability-distributions/#log-pearson-type-iii-lp3-distribution","text":"The LP3 distribution uses the same reparameterization as the PE3 distribution . Support \\[ \\begin{cases} \\max(0, \\xi) \\leq x < \\infty &\\kappa > 0 \\\\[5pt] 0 < x < \\infty &\\kappa =0 \\\\[5pt] 0 < x \\leq \\max(0, \\xi) &\\kappa < 0 \\end{cases} \\] Quantiles \\(x(F) = \\exp(x_{\\text{PE3}}(F ))\\) , where \\(x_{\\text{PE3}}(F)\\) is the quantile function of the PE3 distribution . Likelihood Function To derive the likelihood of the LP3 distribution, we use the fact that: \\[ \\text{Data} \\sim \\text{LP3} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{PE3} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LP3}}(x ; \\mu, \\sigma, \\kappa) = \\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa ) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa )}{x} \\] L-Moments Same as the PE3 distribution .","title":"Log-Pearson Type III (LP3) Distribution"},{"location":"probability-distributions/#weibull-wei-distribution","text":"The Weibull distribution is implemented as a reparameterized version of the generalized extreme value distribution: \\[ \\begin{aligned} \\kappa &= 1 / \\kappa_{\\text{GEV}} \\\\[5pt] \\sigma &= \\kappa \\sigma_{\\text{GEV} } \\\\[5pt] \\mu &= \\sigma + \\mu_{\\text{GEV} } \\end{aligned} \\] Under this reparameterization, it is required that \\(\\sigma > 0\\) and \\(\\kappa > 0\\) . Support \\(\\mu \\leq x < \\infty\\) Quantiles \\(x(F) = \\mu + \\sigma (-\\log (1 - F))^{1/\\kappa}\\) Likelihood Function Its probability density function (PDF) is given below for \\(x_{i} > \\mu\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{\\kappa}{\\sigma }\\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa -1} \\exp \\left( - \\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa } \\right) \\] Therefore, its Log-likelihood function is: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[\\ln \\kappa - \\kappa \\ln \\sigma +(\\kappa -1)\\ln (x_{i}-\\mu ) - \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) ^{\\kappa } \\right] \\] L-Moments First, reparameterize the Weibull distribution to recover the GEV parameters: \\[ \\begin{aligned} \\kappa_{\\text{GEV}} &= 1 / \\kappa \\\\[5pt] \\sigma_{\\text{GEV}} &= \\sigma / \\kappa \\\\[5pt] \\end{aligned} \\] Next, compute the L-moments for the GEV distribution with \\(\\mu_{\\text{GEV}} = 0\\) . Then, \\(\\lambda_{1} = \\mu + \\sigma - \\lambda_{1, \\text{GEV}}\\) \\(\\lambda_{2} = \\lambda_{2, \\text{GEV}}\\) \\(\\tau_{3} = -\\tau_{3, \\text{GEV}}\\) \\(\\tau_{4} = \\tau_{4, \\text{GEV} }\\) To compute the parameters from the L-moments, first flip the sign of \\(\\lambda_{1}\\) and \\(\\tau_{3}\\) . Then, estimate the parameters of the GEV distribution to get \\(\\hat{\\mu}_{\\text{GEV}}\\) , \\(\\hat{\\sigma}_{\\text{GEV}}\\) , and \\(\\hat{\\kappa}_{\\text{GEV}}\\) . Finally, reparameterize the GEV parameters as shown here and then flip the sign of \\(\\mu\\) .","title":"Weibull (WEI) Distribution"},{"location":"probability-distributions/#kappa-kap-distribution","text":"The Kappa distribution has location \\(\\mu\\) , scale \\(\\sigma\\) , and two shape parameters \\(\\kappa\\) and \\(h\\) . Support \\[ \\begin{cases} \\mu + \\sigma (1 - h^{-\\kappa}) \\leq x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0, h > 0 \\\\[5pt] -\\infty < x \\leq \\mu + (\\sigma /\\kappa) & \\kappa > 0, h \\leq 0 \\\\[5pt] \\mu + \\sigma (1 - h^{-\\kappa}) \\leq x < \\infty &\\kappa \\leq 0, h > 0 \\\\[5pt] \\mu + (\\sigma / \\kappa ) \\leq x <\\infty &\\kappa \\leq 0, h \\leq 0 \\end{cases} \\] Quantiles \\[ x(F) = \\mu + \\frac{\\sigma }{\\kappa }\\left[1 - \\left(\\frac{1 - F^{h}}{h}\\right)^{\\kappa }\\right] \\] L-Moments The L-moments are defined if \\(h \\geq 0\\) and \\(k > -1\\) or if \\(h < 0\\) and \\(-1 < k < -1/h\\) . \\(\\lambda_{1} = \\mu + \\sigma (1 - g_{1})/\\kappa\\) \\(\\lambda_{2} = \\sigma(g_{1} - g_{2})/\\kappa\\) \\(\\tau_{3} = (-g_{1} + 3g_{2} - 2g_{3}) / (g_{1} - g_{2})\\) \\(\\tau_{4} = (-g_{1} + 6g_{2} - 10g_{3} + 5g_{4}) / (g_{1} - g_{2})\\) In the expression above, \\(g_{r}\\) is defined as follows: \\[ g_{r} = \\begin{dcases} \\frac{r\\Gamma (1 + \\kappa )\\Gamma (r / h)}{h^{1 + \\kappa }\\Gamma (1 + \\kappa + r/h)} &h > 0 \\\\[5pt] \\frac{r\\Gamma (1 + \\kappa ) \\Gamma (-\\kappa - r/h)}{(-h)^{1 + \\kappa }\\Gamma (1 - r/h)} &h < 0 \\end{dcases} \\] There is no closed-form solution for the parameters in terms of the L-moments. However, \\(\\tau_{3}\\) and \\(\\tau_{4}\\) can be computed in terms of \\(\\kappa\\) and \\(h\\) using Newton-Raphson iteration.","title":"Kappa (KAP) Distribution"},{"location":"probability-distributions/#sources","text":"Hosking, J.R.M. & Wallis, J.R., 1997. Regional frequency analysis: an aproach based on L-Moments. Cambridge University Press, New York, USA. \u21a9 \u21a9 \u21a9 Hosking, J.R.M., Wallis, J.R., & Wood, E.F., 1985. Estimation of the generalized extreme-value distribution by the method of probability-weighted moments. Technometrics, 27, 251-61. \u21a9","title":"Sources"},{"location":"r-installation-instructions/","text":"Installing the ffaframework Package Prerequisites Install Git : https://git-scm.com/downloads . Install R (v4.5.1 recommended): https://www.r-project.org/ . Install Pandoc (for report generation): https://pandoc.org/ . Verify that the installations were successful by executing the following commands in a terminal: git --version R --version pandoc --version Note for Windows Users You may need to add Git, R and Pandoc to your PATH to run them from the command line. The default paths for Git, R and Pandoc are: C:\\Program Files\\Git\\bin C:\\Program Files\\R\\R-4.5.0\\bin C:\\Program Files\\Pandoc To update your PATH , edit your system environment variables from the settings menu. Instructions Open a terminal and navigate to the directory where you want to install the package. Clone the Github repository by running the following command: git clone https://github.com/rileywheadon/ffa-package.git Open the R console. Then install and load the devtools package (if not already installed): install.packages(\"devtools\") library(devtools) Install the ffaframework package: devtools::install(\"~/path/to/ffa-package\") Replace ~/path/to/ffa-package with the path to the cloned directory from step 2. Exit the R console with q() . The ffaframework package is now installed and ready to use.","title":"Installation instructions"},{"location":"r-installation-instructions/#installing-the-ffaframework-package","text":"","title":"Installing the ffaframework Package"},{"location":"r-installation-instructions/#prerequisites","text":"Install Git : https://git-scm.com/downloads . Install R (v4.5.1 recommended): https://www.r-project.org/ . Install Pandoc (for report generation): https://pandoc.org/ . Verify that the installations were successful by executing the following commands in a terminal: git --version R --version pandoc --version","title":"Prerequisites"},{"location":"r-installation-instructions/#note-for-windows-users","text":"You may need to add Git, R and Pandoc to your PATH to run them from the command line. The default paths for Git, R and Pandoc are: C:\\Program Files\\Git\\bin C:\\Program Files\\R\\R-4.5.0\\bin C:\\Program Files\\Pandoc To update your PATH , edit your system environment variables from the settings menu.","title":"Note for Windows Users"},{"location":"r-installation-instructions/#instructions","text":"Open a terminal and navigate to the directory where you want to install the package. Clone the Github repository by running the following command: git clone https://github.com/rileywheadon/ffa-package.git Open the R console. Then install and load the devtools package (if not already installed): install.packages(\"devtools\") library(devtools) Install the ffaframework package: devtools::install(\"~/path/to/ffa-package\") Replace ~/path/to/ffa-package with the path to the cloned directory from step 2. Exit the R console with q() . The ffaframework package is now installed and ready to use.","title":"Instructions"},{"location":"release-notes/","text":"Changelog v0.2.1 July 17th, 2025 Changes to the CLI: Fix load_data to better handle missing entries. Add Mission Creek ( Application_5.csv ) and Lake Louise ( Application_4.csv ) datasets. Add station information to the top of each CSV file. Add support for manual trend selection. Add support for batch processing of multiple datasets. Add support for three more report types (markdown, PDF, and JSON) Changes to the package: Report error producing unreasonably wide confidence interval in uncertainty_bootstrap . Report numerical stability errors in uncertainty_rfpl . Add function plot_ams_data for plotting raw AMS data. Add an option to plot a trend in the variance when using plot_sens_trend . Modify select_lkurtosis to only recommend 3-parameter distributions. Changes to the vignettes: Provide information about each station and a plot of the data at the start of each vignette. Split the trend identification vignette into two (one for mean, one for variability). Replace the Okanagan River dataset with Mission Creek for the trend in variability vignette. Changes to the documentation website: Add a cheat sheet for the R package. Small changes to formatting and clarity. v0.2.0 July 14th, 2025 The first version of the CLI is here! Perform stationary and nonstationary flood frequency analysis Configure the framework using the config.yml file Generate HTML reports using knitr and rmarkdown v0.1.0 July 9th, 2025 The first version of the R Package is here! Implements all features from the MATLAB version (both EDA and FFA). Generate PDF user manual using roxygen2 . Achieve 100% code coverage using the covr library. A full list of features is described in the \"Concepts\" sidebar. v0.0.3 May 22nd, 2025 Return information about nonstationary structure(s) at the end of EDA. Use the /data and /reports directories as defaults in config.yml . Refactor code for batching EDA in stats.R , eda.R . Implement support for PDF reports using a custom \\(\\LaTeX\\) template. v0.0.2 May 21st, 2025 Host documentation at rileywheadon.github.io/ffa-docs . Implement splitting (for change points) in stats.R and eda.R . Run EDA on multiple files by setting csv_files to a list in config.yml . v0.0.1 May 16th, 2025 Execute individual statistical tests using stats.R . Run a suite of unit tests using tests.R . Run the entire EDA pipeline (without data splitting) using eda.R .","title":"Release Notes"},{"location":"release-notes/#changelog","text":"","title":"Changelog"},{"location":"release-notes/#v021","text":"July 17th, 2025 Changes to the CLI: Fix load_data to better handle missing entries. Add Mission Creek ( Application_5.csv ) and Lake Louise ( Application_4.csv ) datasets. Add station information to the top of each CSV file. Add support for manual trend selection. Add support for batch processing of multiple datasets. Add support for three more report types (markdown, PDF, and JSON) Changes to the package: Report error producing unreasonably wide confidence interval in uncertainty_bootstrap . Report numerical stability errors in uncertainty_rfpl . Add function plot_ams_data for plotting raw AMS data. Add an option to plot a trend in the variance when using plot_sens_trend . Modify select_lkurtosis to only recommend 3-parameter distributions. Changes to the vignettes: Provide information about each station and a plot of the data at the start of each vignette. Split the trend identification vignette into two (one for mean, one for variability). Replace the Okanagan River dataset with Mission Creek for the trend in variability vignette. Changes to the documentation website: Add a cheat sheet for the R package. Small changes to formatting and clarity.","title":"v0.2.1"},{"location":"release-notes/#v020","text":"July 14th, 2025 The first version of the CLI is here! Perform stationary and nonstationary flood frequency analysis Configure the framework using the config.yml file Generate HTML reports using knitr and rmarkdown","title":"v0.2.0"},{"location":"release-notes/#v010","text":"July 9th, 2025 The first version of the R Package is here! Implements all features from the MATLAB version (both EDA and FFA). Generate PDF user manual using roxygen2 . Achieve 100% code coverage using the covr library. A full list of features is described in the \"Concepts\" sidebar.","title":"v0.1.0"},{"location":"release-notes/#v003","text":"May 22nd, 2025 Return information about nonstationary structure(s) at the end of EDA. Use the /data and /reports directories as defaults in config.yml . Refactor code for batching EDA in stats.R , eda.R . Implement support for PDF reports using a custom \\(\\LaTeX\\) template.","title":"v0.0.3"},{"location":"release-notes/#v002","text":"May 21st, 2025 Host documentation at rileywheadon.github.io/ffa-docs . Implement splitting (for change points) in stats.R and eda.R . Run EDA on multiple files by setting csv_files to a list in config.yml .","title":"v0.0.2"},{"location":"release-notes/#v001","text":"May 16th, 2025 Execute individual statistical tests using stats.R . Run a suite of unit tests using tests.R . Run the entire EDA pipeline (without data splitting) using eda.R .","title":"v0.0.1"},{"location":"roadmap/","text":"Roadmap CRAN Package In Development Developing workflow functions: module_eda for exploratory data analysis. module_ffa for flood frequency analysis. ffaframework for the complete framework. Testing the package in preparation for submission to CRAN. Known Issues Sample bootstrap yields unusually large CIs for nonstationary models. RFPL breaks on some stations (i.e. Lake Louise, station ID 05BA001 ). Web Interface We hope to finish a web interface for the FFA framework by September 2025.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"roadmap/#cran-package","text":"","title":"CRAN Package"},{"location":"roadmap/#in-development","text":"Developing workflow functions: module_eda for exploratory data analysis. module_ffa for flood frequency analysis. ffaframework for the complete framework. Testing the package in preparation for submission to CRAN.","title":"In Development"},{"location":"roadmap/#known-issues","text":"Sample bootstrap yields unusually large CIs for nonstationary models. RFPL breaks on some stations (i.e. Lake Louise, station ID 05BA001 ).","title":"Known Issues"},{"location":"roadmap/#web-interface","text":"We hope to finish a web interface for the FFA framework by September 2025.","title":"Web Interface"},{"location":"uncertainty-quantification/","text":"Uncertainty Quantification The FFA framework implements three methods for uncertainty quantification: Parametric bootstrap Regula-falsi profile likelihood (RFPL) Regula-falsi generalized profile likelihood (RFGPL) Parametric Bootstrap The parametric bootstrap is a flexible method for uncertainty quantification that works with all probability models and parameter estimation methods. Let \\(n\\) be the size of the original dataset. Draw \\(N_{\\text{sim}}\\) bootstrap samples of size \\(n\\) from the selected probability distribution. Fit a probability distribution to each bootstrap sample using the same model selection method and parameter estimation method that was used to generate the original distribution. Compute the quantiles for each of the bootstrapped distributions. Generate confidence intervals using the mean and variance of the bootstrapped quantiles. Warning : The parametric bootstrap is known to give unreasonably wide confidence intervals for small datasets. If the FFA framework detects a confidence interval that is 5+ times wider than the return levels themselves, it will return an error and recommend RFPL uncertainty quantification 1 . Regula-Falsi Profile Likelihood (RFPL) Consider a statistical model with parameters \\((\\theta, \\psi_{1}, \\dots, \\psi_{n})\\) . The Profile Likelihood for the scalar parameter \\(\\theta\\) and vector of nuisance parameters \\(\\psi\\) is defined as: \\[ \\ell_{p}(\\theta) = \\max_{\\psi } \\ell(\\theta , \\psi) \\] Let \\(\\hat{\\theta}\\) be MLE of \\(\\theta\\) . To find a confidence interval with significance \\(1-\\alpha\\) , we find the two solutions to the following equation (where \\(\\chi_{1;1-\\alpha}^2\\) is the \\(1-\\alpha\\) quantile of the Chi-squared distribution ): \\[ 2[\\ell_{p}(\\hat{\\theta }) - \\ell_{p}(\\theta )] = \\chi_{1;1-\\alpha }^2 \\] This is equivalent to finding the two points \\(\\theta_{L} < \\hat{\\theta} < \\theta_{U}\\) such that the profile log-likelihood has dropped by \\(\\chi _{1;1-\\alpha }^2 / 2\\) . To find \\(\\theta_{L}\\) and \\(\\theta_{U}\\) we find the roots of \\(f(\\theta)\\) using a secant-based algorithm. \\[ f(\\theta) = \\ell_{p}(\\theta) - \\left[\\ell_{p}(\\hat{\\theta}) - \\frac{\\chi_{1;1-\\alpha }^2}{2}\\right] \\] In the FFA framework, we compute the profile likelihood of each quantile \\(y\\) by reparameterizing the location parameter \\(\\mu\\) . Let \\(q(p, \\mu, \\psi)\\) be a function that takes an exceedance probability \\(p\\) , location parameter \\(\\mu\\) and nuisance parameters \\(\\psi\\) and returns a quantile \\(y\\) . All quantile functions satisfy: \\[ y = q(p, \\mu, \\psi) = \\mu + q(p, 0, \\psi) \\] Therefore, we can define \\(\\mu\\) as a function of \\((p, y, \\psi)\\) as shown below: \\[ \\mu = y - q(p, 0, \\psi) \\] We use this relationship to find the profile likelihood \\(\\ell_{p}(y)\\) by evaluating \\(\\mu(p, y, \\psi)\\) and substituting it into the log-likelihood functions listed here . Warning : RFPL uncertainty quantification can be numerically unstable for some datasets. If the FFA framework encounters an issue, it will return an error and recommend the parametric bootstrap 1 . Handling the Weibull Distribution Due to support issues, we use a different reparameterization for the Weibull distribution: \\[ \\begin{aligned} &y = (\\mu_{0} + \\mu_{1}t) + (\\sigma_{0} + \\sigma_{1}t)(-\\log (1 - p))^{1/\\kappa} \\\\[5pt] \\Rightarrow\\,&(\\sigma_{0} + \\sigma_{1}t) = \\frac{y - (\\mu_{0} + \\mu_{1}t)}{(-\\log (1 - p))^{1/\\kappa}} \\\\[5pt] \\Rightarrow\\,&\\sigma_{0} = \\frac{y - (\\mu_{0} + \\mu_{1}t)}{(-\\log (1 - p))^{1 / \\kappa }} - \\sigma_{1}t \\end{aligned} \\] The derivation above uses the Weibull distribution with a trend in both the mean and the variability. However, the reparameterizations for other nonstationary structures can be obtained easily by setting \\(\\sigma_{1} = 0\\) and/or \\(\\mu_{1} = 0\\) . After solving for \\(\\sigma_{0}\\) in terms of the other parameters, we can use the standard log-likelihood function. Initialization Algorithm Before we can find the roots of \\(f\\) , we need to identify initial values for the regula-falsi algorithm: Let \\(a_{0}\\) be a number such that \\(a_{0} < y\\) and \\(f(a_{0}) < 0\\) . Let \\(b_{0}\\) be a number such that \\(b_{0} > y\\) and \\(f(b_{0}) < 0\\) . To find \\(a_{0}\\) , start by computing \\(f(a^{*})\\) for \\(a^{*} = 0.95y\\) . If \\(f(a^{*}) < 0\\) , then assign \\(a_{0} = a^{*}\\) . Otherwise, update \\(a^{*}\\) to \\(0.95a^{*}\\) until \\(f(a^{*}) < 0\\) . To find \\(b_{0}\\) , we use a similar process. However, instead of iteratively revising \\(b^{*}\\) down, we revise it up to \\(1.05b^{*}\\) . Iteration Algorithm At iteration \\(i\\) , compute the following: \\[ c_{i} = \\frac{a_{i-1}f(b_{i-1}) - b_{i-1}f(a_{i-1})}{f(b_{i-1}) - f(a_{i-1})} \\] Evaluate \\(\\ell_{p}(c_{i})\\) by maximizing over the nuisance parameters \\(\\psi\\) , then find \\(f(c_{i})\\) . If \\(|f(c_{i})| < \\epsilon\\) (where \\(\\epsilon\\) is small), then stop. \\(c_{i}\\) is the confidence interval bound. Otherwise, assign \\(a_{i} = c_{i}\\) if \\(f(c_{i}) < 0\\) and \\(b_{i} = c_{i}\\) if \\(f(c_{i}) > 0\\) and continue to iteration \\(i + 1\\) . Regula-Falsi Generalized Profile Likelihood (RFGPL) The regula-falsi generalized profile likelihood (RFGPL) method performs the regula-falsi algorithm shown above on the GEV distributions with a \\(\\text{Beta}(p, q)\\) prior for the shape parameter \\(\\kappa\\) . For more information about generalized parameter estimation, see here . Handling Nonstationarity If the selected probability distribution is nonstationary, the quantiles (and hence confidence intervals) for the bootstrapped distributions change in time. See here for a more detailed discussion of this idea. By default, the FFA framework anchors uncertainty analysis at the last year of the dataset. However, model assessment requires confidence intervals for every year in the dataset. Note : The parametric bootstrap algorithm is the fastest algorithm for computing confidence intervals on all years in a dataset because the probabilities used to generate the bootstrapped samples can be reused. The RFPL and RFGPL algorithms are far slower, since they must be run separately at each timestamp. For more information, please see the following references: Vidrio-Sahag\u00fan, C.T., He, J. Enhanced profile likelihood method for the nonstationary hydrological frequency analysis, Advances in Water Resources 161, 10451 (2022). https://doi.org/10.1016/j.advwatres.2022.104151 Vidrio-Sahag\u00fan, C.T., He, J. & Pietroniro, A. Multi-distribution regula-falsi profile likelihood method for nonstationary hydrological frequency analysis. Stoch Environ Res Risk Assess 38, 843\u2013867 (2024). https://doi.org/10.1007/s00477-023-02603-0 \u21a9 \u21a9","title":"Uncertainty Quantification"},{"location":"uncertainty-quantification/#uncertainty-quantification","text":"The FFA framework implements three methods for uncertainty quantification: Parametric bootstrap Regula-falsi profile likelihood (RFPL) Regula-falsi generalized profile likelihood (RFGPL)","title":"Uncertainty Quantification"},{"location":"uncertainty-quantification/#parametric-bootstrap","text":"The parametric bootstrap is a flexible method for uncertainty quantification that works with all probability models and parameter estimation methods. Let \\(n\\) be the size of the original dataset. Draw \\(N_{\\text{sim}}\\) bootstrap samples of size \\(n\\) from the selected probability distribution. Fit a probability distribution to each bootstrap sample using the same model selection method and parameter estimation method that was used to generate the original distribution. Compute the quantiles for each of the bootstrapped distributions. Generate confidence intervals using the mean and variance of the bootstrapped quantiles. Warning : The parametric bootstrap is known to give unreasonably wide confidence intervals for small datasets. If the FFA framework detects a confidence interval that is 5+ times wider than the return levels themselves, it will return an error and recommend RFPL uncertainty quantification 1 .","title":"Parametric Bootstrap"},{"location":"uncertainty-quantification/#regula-falsi-profile-likelihood-rfpl","text":"Consider a statistical model with parameters \\((\\theta, \\psi_{1}, \\dots, \\psi_{n})\\) . The Profile Likelihood for the scalar parameter \\(\\theta\\) and vector of nuisance parameters \\(\\psi\\) is defined as: \\[ \\ell_{p}(\\theta) = \\max_{\\psi } \\ell(\\theta , \\psi) \\] Let \\(\\hat{\\theta}\\) be MLE of \\(\\theta\\) . To find a confidence interval with significance \\(1-\\alpha\\) , we find the two solutions to the following equation (where \\(\\chi_{1;1-\\alpha}^2\\) is the \\(1-\\alpha\\) quantile of the Chi-squared distribution ): \\[ 2[\\ell_{p}(\\hat{\\theta }) - \\ell_{p}(\\theta )] = \\chi_{1;1-\\alpha }^2 \\] This is equivalent to finding the two points \\(\\theta_{L} < \\hat{\\theta} < \\theta_{U}\\) such that the profile log-likelihood has dropped by \\(\\chi _{1;1-\\alpha }^2 / 2\\) . To find \\(\\theta_{L}\\) and \\(\\theta_{U}\\) we find the roots of \\(f(\\theta)\\) using a secant-based algorithm. \\[ f(\\theta) = \\ell_{p}(\\theta) - \\left[\\ell_{p}(\\hat{\\theta}) - \\frac{\\chi_{1;1-\\alpha }^2}{2}\\right] \\] In the FFA framework, we compute the profile likelihood of each quantile \\(y\\) by reparameterizing the location parameter \\(\\mu\\) . Let \\(q(p, \\mu, \\psi)\\) be a function that takes an exceedance probability \\(p\\) , location parameter \\(\\mu\\) and nuisance parameters \\(\\psi\\) and returns a quantile \\(y\\) . All quantile functions satisfy: \\[ y = q(p, \\mu, \\psi) = \\mu + q(p, 0, \\psi) \\] Therefore, we can define \\(\\mu\\) as a function of \\((p, y, \\psi)\\) as shown below: \\[ \\mu = y - q(p, 0, \\psi) \\] We use this relationship to find the profile likelihood \\(\\ell_{p}(y)\\) by evaluating \\(\\mu(p, y, \\psi)\\) and substituting it into the log-likelihood functions listed here . Warning : RFPL uncertainty quantification can be numerically unstable for some datasets. If the FFA framework encounters an issue, it will return an error and recommend the parametric bootstrap 1 .","title":"Regula-Falsi Profile Likelihood (RFPL)"},{"location":"uncertainty-quantification/#handling-the-weibull-distribution","text":"Due to support issues, we use a different reparameterization for the Weibull distribution: \\[ \\begin{aligned} &y = (\\mu_{0} + \\mu_{1}t) + (\\sigma_{0} + \\sigma_{1}t)(-\\log (1 - p))^{1/\\kappa} \\\\[5pt] \\Rightarrow\\,&(\\sigma_{0} + \\sigma_{1}t) = \\frac{y - (\\mu_{0} + \\mu_{1}t)}{(-\\log (1 - p))^{1/\\kappa}} \\\\[5pt] \\Rightarrow\\,&\\sigma_{0} = \\frac{y - (\\mu_{0} + \\mu_{1}t)}{(-\\log (1 - p))^{1 / \\kappa }} - \\sigma_{1}t \\end{aligned} \\] The derivation above uses the Weibull distribution with a trend in both the mean and the variability. However, the reparameterizations for other nonstationary structures can be obtained easily by setting \\(\\sigma_{1} = 0\\) and/or \\(\\mu_{1} = 0\\) . After solving for \\(\\sigma_{0}\\) in terms of the other parameters, we can use the standard log-likelihood function.","title":"Handling the Weibull Distribution"},{"location":"uncertainty-quantification/#initialization-algorithm","text":"Before we can find the roots of \\(f\\) , we need to identify initial values for the regula-falsi algorithm: Let \\(a_{0}\\) be a number such that \\(a_{0} < y\\) and \\(f(a_{0}) < 0\\) . Let \\(b_{0}\\) be a number such that \\(b_{0} > y\\) and \\(f(b_{0}) < 0\\) . To find \\(a_{0}\\) , start by computing \\(f(a^{*})\\) for \\(a^{*} = 0.95y\\) . If \\(f(a^{*}) < 0\\) , then assign \\(a_{0} = a^{*}\\) . Otherwise, update \\(a^{*}\\) to \\(0.95a^{*}\\) until \\(f(a^{*}) < 0\\) . To find \\(b_{0}\\) , we use a similar process. However, instead of iteratively revising \\(b^{*}\\) down, we revise it up to \\(1.05b^{*}\\) .","title":"Initialization Algorithm"},{"location":"uncertainty-quantification/#iteration-algorithm","text":"At iteration \\(i\\) , compute the following: \\[ c_{i} = \\frac{a_{i-1}f(b_{i-1}) - b_{i-1}f(a_{i-1})}{f(b_{i-1}) - f(a_{i-1})} \\] Evaluate \\(\\ell_{p}(c_{i})\\) by maximizing over the nuisance parameters \\(\\psi\\) , then find \\(f(c_{i})\\) . If \\(|f(c_{i})| < \\epsilon\\) (where \\(\\epsilon\\) is small), then stop. \\(c_{i}\\) is the confidence interval bound. Otherwise, assign \\(a_{i} = c_{i}\\) if \\(f(c_{i}) < 0\\) and \\(b_{i} = c_{i}\\) if \\(f(c_{i}) > 0\\) and continue to iteration \\(i + 1\\) .","title":"Iteration Algorithm"},{"location":"uncertainty-quantification/#regula-falsi-generalized-profile-likelihood-rfgpl","text":"The regula-falsi generalized profile likelihood (RFGPL) method performs the regula-falsi algorithm shown above on the GEV distributions with a \\(\\text{Beta}(p, q)\\) prior for the shape parameter \\(\\kappa\\) . For more information about generalized parameter estimation, see here .","title":"Regula-Falsi Generalized Profile Likelihood (RFGPL)"},{"location":"uncertainty-quantification/#handling-nonstationarity","text":"If the selected probability distribution is nonstationary, the quantiles (and hence confidence intervals) for the bootstrapped distributions change in time. See here for a more detailed discussion of this idea. By default, the FFA framework anchors uncertainty analysis at the last year of the dataset. However, model assessment requires confidence intervals for every year in the dataset. Note : The parametric bootstrap algorithm is the fastest algorithm for computing confidence intervals on all years in a dataset because the probabilities used to generate the bootstrapped samples can be reused. The RFPL and RFGPL algorithms are far slower, since they must be run separately at each timestamp. For more information, please see the following references: Vidrio-Sahag\u00fan, C.T., He, J. Enhanced profile likelihood method for the nonstationary hydrological frequency analysis, Advances in Water Resources 161, 10451 (2022). https://doi.org/10.1016/j.advwatres.2022.104151 Vidrio-Sahag\u00fan, C.T., He, J. & Pietroniro, A. Multi-distribution regula-falsi profile likelihood method for nonstationary hydrological frequency analysis. Stoch Environ Res Risk Assess 38, 843\u2013867 (2024). https://doi.org/10.1007/s00477-023-02603-0 \u21a9 \u21a9","title":"Handling Nonstationarity"},{"location":"vignettes/","text":"Vignettes Warning : These vignettes are out of date. View as HTML : Introduction Change Point Detection Trend Identification (Mean) Trend Identification (Variability) Stationary FFA Nonstationary FFA Download as a .R script : Introduction Change Point Detection Trend Identification (Mean) Trend Identification (Variability) Stationary FFA Nonstationary FFA Download as a .Rmd file : Introduction Change Point Detection Trend Identification (Mean) Trend Identification (Variability) Stationary FFA Nonstationary FFA","title":"Vignettes"},{"location":"vignettes/#vignettes","text":"Warning : These vignettes are out of date. View as HTML : Introduction Change Point Detection Trend Identification (Mean) Trend Identification (Variability) Stationary FFA Nonstationary FFA Download as a .R script : Introduction Change Point Detection Trend Identification (Mean) Trend Identification (Variability) Stationary FFA Nonstationary FFA Download as a .Rmd file : Introduction Change Point Detection Trend Identification (Mean) Trend Identification (Variability) Stationary FFA Nonstationary FFA","title":"Vignettes"}]}