{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FFA Framework Wiki Welcome to the FFA Framework wiki! The FFA Framework is an open-source project for Flood Frequency Analysis (FFA) developed by researchers at the University of Calgary in Alberta, Canada. The framework is currently available as an R package and a command line interface (CLI). The original version of this software was written in MATLAB and published as A practice-oriented framework for stationary and nonstationary flood frequency analysis (Vidrio-Sahag\u00fan et al. 2024). For a list of changes from the MATLAB version, see here . To get started with the R package, see the installation instructions and documentation . To get started with the CLI, please read the installation instructions and the user manual . You may also be interested in our goals , roadmap , or release notes . Warning : The FFA Framework is currently under development. Code and documentation may change frequently and drastically. Use at your own risk.","title":"Home"},{"location":"#ffa-framework-wiki","text":"Welcome to the FFA Framework wiki! The FFA Framework is an open-source project for Flood Frequency Analysis (FFA) developed by researchers at the University of Calgary in Alberta, Canada. The framework is currently available as an R package and a command line interface (CLI). The original version of this software was written in MATLAB and published as A practice-oriented framework for stationary and nonstationary flood frequency analysis (Vidrio-Sahag\u00fan et al. 2024). For a list of changes from the MATLAB version, see here . To get started with the R package, see the installation instructions and documentation . To get started with the CLI, please read the installation instructions and the user manual . You may also be interested in our goals , roadmap , or release notes . Warning : The FFA Framework is currently under development. Code and documentation may change frequently and drastically. Use at your own risk.","title":"FFA Framework Wiki"},{"location":"cli-getting-started/","text":"Getting Started First, you will need to install the CLI. Installation instructions can be found here . The FFA Framework CLI consists of a single script, ffaframework.R . This script runs exploratory data analysis and/or flood frequency analysis on one or many datasets. It can also produce a report as a markdown, PDF, HTML, or JSON file. To configure the CLI, simply edit config.yml . See the user manual for more information. Folder Structure The /data folder contains CSV files used by the CLI. The following files are included by default: Application_1.csv : Station 07BE001 (Athabasca River at Athabasca) Application_2.csv : Station 08NH021 (Kootenai River at Porthill) Application_3_1.csv : Station 05BB001 (Bow River at Banff) Application_3_2.csv : Station 08MH016 (Chilliwack River at Chilliwack Lake) Application_3_3.csv : Station 08NM050 (Okanagan River at Penticton) The /reports folder stores reports and images generated by the CLI. Trying the Framework By default, the CLI does an analysis of Application_1.csv . Give it a try by navigating to the ffa-framework folder in a terminal and running the command Rscript ffaframework.R . What Next? We recommend reading the documents in the \"Concepts\" sidebar to familiarize yourself with the statistical techniques used in the FFA Framework. Once you're comfortable with the ideas, try running another flood frequency analysis on one of the example datasets!","title":"Getting Started"},{"location":"cli-getting-started/#getting-started","text":"First, you will need to install the CLI. Installation instructions can be found here . The FFA Framework CLI consists of a single script, ffaframework.R . This script runs exploratory data analysis and/or flood frequency analysis on one or many datasets. It can also produce a report as a markdown, PDF, HTML, or JSON file. To configure the CLI, simply edit config.yml . See the user manual for more information.","title":"Getting Started"},{"location":"cli-getting-started/#folder-structure","text":"The /data folder contains CSV files used by the CLI. The following files are included by default: Application_1.csv : Station 07BE001 (Athabasca River at Athabasca) Application_2.csv : Station 08NH021 (Kootenai River at Porthill) Application_3_1.csv : Station 05BB001 (Bow River at Banff) Application_3_2.csv : Station 08MH016 (Chilliwack River at Chilliwack Lake) Application_3_3.csv : Station 08NM050 (Okanagan River at Penticton) The /reports folder stores reports and images generated by the CLI.","title":"Folder Structure"},{"location":"cli-getting-started/#trying-the-framework","text":"By default, the CLI does an analysis of Application_1.csv . Give it a try by navigating to the ffa-framework folder in a terminal and running the command Rscript ffaframework.R .","title":"Trying the Framework"},{"location":"cli-getting-started/#what-next","text":"We recommend reading the documents in the \"Concepts\" sidebar to familiarize yourself with the statistical techniques used in the FFA Framework. Once you're comfortable with the ideas, try running another flood frequency analysis on one of the example datasets!","title":"What Next?"},{"location":"cli-installation-instructions/","text":"Installing the CLI Prerequisites Install Git from https://git-scm.com/downloads . Check that Git is installed by executing git --version in a shell. Install R from https://www.r-project.org/ . Version 4.5.0 is recommended. Check that R is installed by executing R --version in a shell. Install the latest version of Pandoc from https://pandoc.org/ . Check that Pandoc is installed by executing pandoc --version in a shell. A Note for Windows Users You may need to add Git, R and Pandoc to your path in order to run them from the command line. To do this, you will need to edit your system environment variables from the settings menu. The default paths for Git, R and Pandoc are: C:\\Program Files\\Git\\bin C:\\Program Files\\R\\R-4.5.0\\bin C:\\Program Files\\Pandoc Instructions Open a shell and navigate to the directory where you would like to install the framework. Clone the Github repository by running the following command in a shell: git clone https://github.com/rileywheadon/ffa-framework.git Navigate to the source directory. This directory contains a renv.lock file. Open a command prompt with R . The renv package should install automatically. Load libraries from the renv.lock file by executing renv::restore() . Exit the command prompt with q() . You will also need to install the R package . See here for installation instructions. This guide has been tested on Linux, Windows, and MacOS. Please submit a Github Issue if you have any problems.","title":"Installation Instructions"},{"location":"cli-installation-instructions/#installing-the-cli","text":"","title":"Installing the CLI"},{"location":"cli-installation-instructions/#prerequisites","text":"Install Git from https://git-scm.com/downloads . Check that Git is installed by executing git --version in a shell. Install R from https://www.r-project.org/ . Version 4.5.0 is recommended. Check that R is installed by executing R --version in a shell. Install the latest version of Pandoc from https://pandoc.org/ . Check that Pandoc is installed by executing pandoc --version in a shell.","title":"Prerequisites"},{"location":"cli-installation-instructions/#a-note-for-windows-users","text":"You may need to add Git, R and Pandoc to your path in order to run them from the command line. To do this, you will need to edit your system environment variables from the settings menu. The default paths for Git, R and Pandoc are: C:\\Program Files\\Git\\bin C:\\Program Files\\R\\R-4.5.0\\bin C:\\Program Files\\Pandoc","title":"A Note for Windows Users"},{"location":"cli-installation-instructions/#instructions","text":"Open a shell and navigate to the directory where you would like to install the framework. Clone the Github repository by running the following command in a shell: git clone https://github.com/rileywheadon/ffa-framework.git Navigate to the source directory. This directory contains a renv.lock file. Open a command prompt with R . The renv package should install automatically. Load libraries from the renv.lock file by executing renv::restore() . Exit the command prompt with q() . You will also need to install the R package . See here for installation instructions. This guide has been tested on Linux, Windows, and MacOS. Please submit a Github Issue if you have any problems.","title":"Instructions"},{"location":"cli-user-manual/","text":"User Manual The ffaframework.R script runs the complete EDA and FFA frameworks. The optional command line argument --config can be used to specify a custom configuration file (default is config.yml ). Example Usage (default configuration) Rscript ffaframework.R Example Usage (custom configuration) Rscript ffaframework.R --config='custom-config.yml' Configuration Reference Note : All configuration options are specified using lowercase characters . Data Preparation data_source : Character (1); method for sourcing data for the framework. Must be one of: \"local\" : Source data locally using the csv_files option. \"geomet\" : Pull data from the MSC GeoMet API using the station_ids option. csv_files : Character; CSV files located in /data used to run the framework. station_ids : Character; station IDs for hydrological monitoring stations. You can search for station IDs by name, province, drainage basin, and location here . EDA run_eda : Boolean (1); if set to true , run exploratory data analysis. split_selection : Character (1); method for determining split points. Must be one of: \"automatic\" : Identify split points and split the data automatically. \"manual\" : Confirm split points after identification. \"preset\" : Set the split points ahead of time using split_points . split_points : Integer; preset values for split points or null for no split points. significance_level : Numeric (1); significance level. Must be between 0.01 and 0.1 . bbmk_samples : Integer (1); number of bootstrap samples to use for the BB-MK Test . FFA run_ffa : Boolean (1); if set to true , run flood frequency analysis . ns_selection : Character (1); non-stationary signature selection method. Must be one of: \"automatic\" : Non-stationarity is identified and handled automatically. \"manual\" : Confirm non-stationary patterns after EDA. Requires run_eda: true . \"preset\" : Set non-stationary signature ahead of time with ns_signature . ns_signature : Character (1); stationary or non-stationary signature. Must be one of: null : No non-stationarity. \"10\" : Linear trend in the mean. \"11\" : Linear trend in the mean and variance. Distribution Selection z_samples : Integer (1); number of bootstrap samples for Z-statistic selection . distribution_selection : Character (1); distribution selection method: \"l-distance\" : Euclidian distance from (L-skewness, L-kurtosis) point. \"l-kurtosis\" : Difference between theoretical and sample L-kurtosis. \"z-statistic\" : Bootstrapped Z-statistic computed using the Kappa distribution. \"preset\" : Specify a distribution ahead of time with the distribution_name option. distribution_name : Character (1); name of the probability distributions to use. Parameter Estimation gev_prior : Numeric (2); prior parameters for the shape parameter of the GEV distribution. s_estimation : Character (1); parameter estimation method for stationary models: \"l-moments\" : Method of L-moments using formulas from Hosking (1997). \"mle\" : Maximum likelihood estimation. \"gmle\" : Generalized maximum likelihood estimation (GEV distribution only). ns_estimation : Character (1); parameter estimation method for non-stationary models: \"mle\" : Maximum likelihood estimation. \"gmle\" : Generalized maximum likelihood estimation (GEV distribution only). Uncertainty Quantification return_periods : Numeric; list of return periods for estimating return levels. sb_samples : Integer (1); number of samples for bootstrap uncertainty quantification . rfpl_tolerance : Numeric (1); log-likelihood tolerance for RFPL uncertainty quantification . s_uncertainty : Character (1); uncertainty quantification method for stationary models: \"s-bootstrap\" : Sample bootstrap method. \"rfpl\" : Regula-falsi profile likelihood (MLE estimation only). \"rfgpl\" : Generalized regula-falsi profile likelihood (GMLE estimation only). ns_uncertainty : Character (1); uncertainty quantification method for non-stationary models: \"s-bootstrap\" : Sample bootstrap method. \"rfpl\" : Regula-falsi profile likelihood (MLE estimation only). \"rfgpl\" : Generalized regula-falsi profile likelihood (GMLE estimation only). Model Assessment pp_formula : Character (1); plotting position formula for model assessment . Must be one of: \"weibull\" : \\(i / (n + 1)\\) \"blom\" : \\((i - 0.375) / (n + 0.25)\\) \"cunnane\" : \\((i - 0.4) / (n + 0.2)\\) \"gringorten\" : \\((i - 0.44) / (n + 0.12)\\) \"hazen\" : \\((i - 0.5) / n\\) Plot Generation show_trend : Boolean (1); if true , add a trend line through the AMS data where applicable. anchor_year : Integer (1); year used to generate return level plot for non-stationary models. Report Generation generate_report : Boolean (1); if true , generate a report with specified report_formats . report_formats : Character; list of report formats ( \"markdown\" , \"pdf\" , \"html\" , or \"json\" ).","title":"User Manual"},{"location":"cli-user-manual/#user-manual","text":"The ffaframework.R script runs the complete EDA and FFA frameworks. The optional command line argument --config can be used to specify a custom configuration file (default is config.yml ). Example Usage (default configuration) Rscript ffaframework.R Example Usage (custom configuration) Rscript ffaframework.R --config='custom-config.yml'","title":"User Manual"},{"location":"cli-user-manual/#configuration-reference","text":"Note : All configuration options are specified using lowercase characters .","title":"Configuration Reference"},{"location":"cli-user-manual/#data-preparation","text":"data_source : Character (1); method for sourcing data for the framework. Must be one of: \"local\" : Source data locally using the csv_files option. \"geomet\" : Pull data from the MSC GeoMet API using the station_ids option. csv_files : Character; CSV files located in /data used to run the framework. station_ids : Character; station IDs for hydrological monitoring stations. You can search for station IDs by name, province, drainage basin, and location here .","title":"Data Preparation"},{"location":"cli-user-manual/#eda","text":"run_eda : Boolean (1); if set to true , run exploratory data analysis. split_selection : Character (1); method for determining split points. Must be one of: \"automatic\" : Identify split points and split the data automatically. \"manual\" : Confirm split points after identification. \"preset\" : Set the split points ahead of time using split_points . split_points : Integer; preset values for split points or null for no split points. significance_level : Numeric (1); significance level. Must be between 0.01 and 0.1 . bbmk_samples : Integer (1); number of bootstrap samples to use for the BB-MK Test .","title":"EDA"},{"location":"cli-user-manual/#ffa","text":"run_ffa : Boolean (1); if set to true , run flood frequency analysis . ns_selection : Character (1); non-stationary signature selection method. Must be one of: \"automatic\" : Non-stationarity is identified and handled automatically. \"manual\" : Confirm non-stationary patterns after EDA. Requires run_eda: true . \"preset\" : Set non-stationary signature ahead of time with ns_signature . ns_signature : Character (1); stationary or non-stationary signature. Must be one of: null : No non-stationarity. \"10\" : Linear trend in the mean. \"11\" : Linear trend in the mean and variance.","title":"FFA"},{"location":"cli-user-manual/#distribution-selection","text":"z_samples : Integer (1); number of bootstrap samples for Z-statistic selection . distribution_selection : Character (1); distribution selection method: \"l-distance\" : Euclidian distance from (L-skewness, L-kurtosis) point. \"l-kurtosis\" : Difference between theoretical and sample L-kurtosis. \"z-statistic\" : Bootstrapped Z-statistic computed using the Kappa distribution. \"preset\" : Specify a distribution ahead of time with the distribution_name option. distribution_name : Character (1); name of the probability distributions to use.","title":"Distribution Selection"},{"location":"cli-user-manual/#parameter-estimation","text":"gev_prior : Numeric (2); prior parameters for the shape parameter of the GEV distribution. s_estimation : Character (1); parameter estimation method for stationary models: \"l-moments\" : Method of L-moments using formulas from Hosking (1997). \"mle\" : Maximum likelihood estimation. \"gmle\" : Generalized maximum likelihood estimation (GEV distribution only). ns_estimation : Character (1); parameter estimation method for non-stationary models: \"mle\" : Maximum likelihood estimation. \"gmle\" : Generalized maximum likelihood estimation (GEV distribution only).","title":"Parameter Estimation"},{"location":"cli-user-manual/#uncertainty-quantification","text":"return_periods : Numeric; list of return periods for estimating return levels. sb_samples : Integer (1); number of samples for bootstrap uncertainty quantification . rfpl_tolerance : Numeric (1); log-likelihood tolerance for RFPL uncertainty quantification . s_uncertainty : Character (1); uncertainty quantification method for stationary models: \"s-bootstrap\" : Sample bootstrap method. \"rfpl\" : Regula-falsi profile likelihood (MLE estimation only). \"rfgpl\" : Generalized regula-falsi profile likelihood (GMLE estimation only). ns_uncertainty : Character (1); uncertainty quantification method for non-stationary models: \"s-bootstrap\" : Sample bootstrap method. \"rfpl\" : Regula-falsi profile likelihood (MLE estimation only). \"rfgpl\" : Generalized regula-falsi profile likelihood (GMLE estimation only).","title":"Uncertainty Quantification"},{"location":"cli-user-manual/#model-assessment","text":"pp_formula : Character (1); plotting position formula for model assessment . Must be one of: \"weibull\" : \\(i / (n + 1)\\) \"blom\" : \\((i - 0.375) / (n + 0.25)\\) \"cunnane\" : \\((i - 0.4) / (n + 0.2)\\) \"gringorten\" : \\((i - 0.44) / (n + 0.12)\\) \"hazen\" : \\((i - 0.5) / n\\)","title":"Model Assessment"},{"location":"cli-user-manual/#plot-generation","text":"show_trend : Boolean (1); if true , add a trend line through the AMS data where applicable. anchor_year : Integer (1); year used to generate return level plot for non-stationary models.","title":"Plot Generation"},{"location":"cli-user-manual/#report-generation","text":"generate_report : Boolean (1); if true , generate a report with specified report_formats . report_formats : Character; list of report formats ( \"markdown\" , \"pdf\" , \"html\" , or \"json\" ).","title":"Report Generation"},{"location":"eda-change-points/","text":"Change Point Detection Mann-Kendall-Sneyers Test The Mann-Kendall-Sneyers (MKS) Test is used to identify the beginning of a trend in a time series: Null hypothesis: There are no change points in the time series. Alternative hypothesis: There are one or more change points in the time series. Define \\(\\mathbb{I}(y_{i} > y_{j})\\) to be \\(1\\) if \\(y_{i} > y_{j}\\) and \\(0\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , we compute the progressive series \\(S^{F}_{t}\\) : \\[ S^{F}_{t} = \\sum_{i=i}^{t} \\sum_{j=1}^{i-1} \\mathbb{I}(y_{i} > y_{j}) \\] Next, we reverse the time series \\(y\\) . This gives us a new time series \\(y'\\) such that \\(y_{i}' = y_{n+1-i}\\) . Then we compute the regressive series \\(S^{B}_{t}\\) , where \\(\\text{rev}\\) indicates that the vector is reversed: \\[ S^{B}_{t} = \\text{rev}\\left( \\sum_{i=i}^{t} \\sum_{j=1}^{i-1} \\mathbb{I}(y'_{i} > y'_{j})\\right) \\] Then, we compute the normalized progressive series \\(UF_{t}\\) and normalized regressive series \\(UB_{t}\\) : \\[ UF_{t} = \\frac{S^{F}_{t} - \\mathbb{E}[S^{F}_{t}]}{\\sqrt{\\text{Var}\\,(S^{F}_{t})}}, \\quad UB_{t} = \\frac{S^{B}_{t} - \\mathbb{E}[S^{B}_{t}]}{\\sqrt{\\text{Var}\\,(S^{B}_{t})}} \\] For both the progressive and regressive series, the expectation and variance is as follows: \\[ \\mathbb{E}[S^{F}_{t}] = \\mathbb{E}[S^{B}_{t}] = \\frac{t(t-1)}{4}, \\quad \\text{Var}(S^{F}_{t}) = \\text{Var}(S^{B}_{t}) = \\frac{t(t-1)(2t+5)}{72} \\] Finally, we plot \\(UF_{t}\\) and \\(UB_{t}\\) with confidence bounds at the \\(\\alpha/2\\) and \\(1 - (\\alpha /2))\\) quantiles of the standard normal distribution, where \\(\\alpha\\) is the chosen significance level. A crossing point between \\(UF_{t}\\) and \\(UB_{t}\\) that lies outside the confidence bounds indicates the start of the trend. Example Plot Pettitt Test The Pettitt Test is used to identify abrupt changes in the mean of a time series. Null hypothesis: There are no abrupt changes in the time series mean. Alternative hypothesis: There is one abrupt change in the time series mean. Define \\(\\text{sign}(x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , compute the following test statistic: \\[ U_{t} = \\sum_{i=1}^{t} \\sum_{j=t+1}^{n} \\text{sign} (y_{j} - y_{i}), \\quad K = \\max_{t}|U_{t}| \\] The value of \\(t\\) such that \\(U_{t} = K\\) is a potential change point . The p-value of the potential change point can be approximated using the following formula for a one-sided test: \\[ p \\approx \\exp \\left(-\\frac{6K^2}{n^3 + n^2}\\right) \\] If the p-value is less than the significance level \\(\\alpha\\) , we reject the null hypothesis and conclude that there is evidence for an abrupt change in the mean at the potential change point. Example Plot","title":"Change Points"},{"location":"eda-change-points/#change-point-detection","text":"","title":"Change Point Detection"},{"location":"eda-change-points/#mann-kendall-sneyers-test","text":"The Mann-Kendall-Sneyers (MKS) Test is used to identify the beginning of a trend in a time series: Null hypothesis: There are no change points in the time series. Alternative hypothesis: There are one or more change points in the time series. Define \\(\\mathbb{I}(y_{i} > y_{j})\\) to be \\(1\\) if \\(y_{i} > y_{j}\\) and \\(0\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , we compute the progressive series \\(S^{F}_{t}\\) : \\[ S^{F}_{t} = \\sum_{i=i}^{t} \\sum_{j=1}^{i-1} \\mathbb{I}(y_{i} > y_{j}) \\] Next, we reverse the time series \\(y\\) . This gives us a new time series \\(y'\\) such that \\(y_{i}' = y_{n+1-i}\\) . Then we compute the regressive series \\(S^{B}_{t}\\) , where \\(\\text{rev}\\) indicates that the vector is reversed: \\[ S^{B}_{t} = \\text{rev}\\left( \\sum_{i=i}^{t} \\sum_{j=1}^{i-1} \\mathbb{I}(y'_{i} > y'_{j})\\right) \\] Then, we compute the normalized progressive series \\(UF_{t}\\) and normalized regressive series \\(UB_{t}\\) : \\[ UF_{t} = \\frac{S^{F}_{t} - \\mathbb{E}[S^{F}_{t}]}{\\sqrt{\\text{Var}\\,(S^{F}_{t})}}, \\quad UB_{t} = \\frac{S^{B}_{t} - \\mathbb{E}[S^{B}_{t}]}{\\sqrt{\\text{Var}\\,(S^{B}_{t})}} \\] For both the progressive and regressive series, the expectation and variance is as follows: \\[ \\mathbb{E}[S^{F}_{t}] = \\mathbb{E}[S^{B}_{t}] = \\frac{t(t-1)}{4}, \\quad \\text{Var}(S^{F}_{t}) = \\text{Var}(S^{B}_{t}) = \\frac{t(t-1)(2t+5)}{72} \\] Finally, we plot \\(UF_{t}\\) and \\(UB_{t}\\) with confidence bounds at the \\(\\alpha/2\\) and \\(1 - (\\alpha /2))\\) quantiles of the standard normal distribution, where \\(\\alpha\\) is the chosen significance level. A crossing point between \\(UF_{t}\\) and \\(UB_{t}\\) that lies outside the confidence bounds indicates the start of the trend.","title":"Mann-Kendall-Sneyers Test"},{"location":"eda-change-points/#example-plot","text":"","title":"Example Plot"},{"location":"eda-change-points/#pettitt-test","text":"The Pettitt Test is used to identify abrupt changes in the mean of a time series. Null hypothesis: There are no abrupt changes in the time series mean. Alternative hypothesis: There is one abrupt change in the time series mean. Define \\(\\text{sign}(x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. Given a time series \\(y_{1}, \\dots, y_{n}\\) , compute the following test statistic: \\[ U_{t} = \\sum_{i=1}^{t} \\sum_{j=t+1}^{n} \\text{sign} (y_{j} - y_{i}), \\quad K = \\max_{t}|U_{t}| \\] The value of \\(t\\) such that \\(U_{t} = K\\) is a potential change point . The p-value of the potential change point can be approximated using the following formula for a one-sided test: \\[ p \\approx \\exp \\left(-\\frac{6K^2}{n^3 + n^2}\\right) \\] If the p-value is less than the significance level \\(\\alpha\\) , we reject the null hypothesis and conclude that there is evidence for an abrupt change in the mean at the potential change point.","title":"Pettitt Test"},{"location":"eda-change-points/#example-plot_1","text":"","title":"Example Plot"},{"location":"eda-introduction/","text":"Introduction to EDA The exploratory data analysis (EDA) module of the flood frequency analysis (FFA) framework contains a collection of statistical tests for annual maximum streamflow (AMS) data. These tests are performed in a specific order to accomplish the four goals listed below: Identify change points (\"jumps\" or \"kinks\") in the AMS data. Identify autocorrelation in the AMS data. Identify trends in the mean value of the AMS data. Identify trends in the variance of the AMS data. Ultimately the goal of EDA is to choose between stationary and non-stationary FFA. A diagram showing the current EDA framework is shown below:","title":"Introduction"},{"location":"eda-introduction/#introduction-to-eda","text":"The exploratory data analysis (EDA) module of the flood frequency analysis (FFA) framework contains a collection of statistical tests for annual maximum streamflow (AMS) data. These tests are performed in a specific order to accomplish the four goals listed below: Identify change points (\"jumps\" or \"kinks\") in the AMS data. Identify autocorrelation in the AMS data. Identify trends in the mean value of the AMS data. Identify trends in the variance of the AMS data. Ultimately the goal of EDA is to choose between stationary and non-stationary FFA. A diagram showing the current EDA framework is shown below:","title":"Introduction to EDA"},{"location":"eda-trend-ams-mean/","text":"Identifying Trends in the AMS Mean BB-MK Test The Block Bootstrap Mann-Kendall (BB-MK) Test is used to assess whether there is a statistically significant monotonic trend in a time series. The BB-MK test is insensitive to autocorrelation , which is known to produce false positives in the MK test . Null hypothesis: There is no monotonic trend. Alternative hypothesis: There is a monotonic upwards or downwards trend. To carry out the BB-MK test, we rely on the results of the MK test and Spearman test. Compute the MK test statistic. Find the least insignificant lag \\(k\\) using the Spearman test. Resample from the original time series in blocks of size \\(k+1\\) without replacement. Estimate the MK test statistic for each bootstrapped sample. Derive the empirical distribution of the MK test statistic from the bootstrapped statistics. Estimate the significance of the observed test statistic using the empirical distribution. Example Plot KPSS Test The KPSS Test is used to identify if an autoregressive time series has a unit root . Null hypothesis: The time series does not have a unit root and is trend-stationary . Alternative hypothesis: The time series has a unit root and is non-stationary . Precisely, the autoregressive time series shown below has unit root if \\(\\sigma_{v}^2 > 0\\) : \\[ \\begin{align} y_{t} &= \\mu_{t} + \\beta t + \\epsilon_{t} \\\\[5pt] \\mu_{t} &= \\mu_{t-1} + v_{t} \\\\[5pt] v_{t} &\\sim \\mathcal{N}(0, \\sigma_{v}^2) \\end{align} \\] Here is what each term in this formulation represents: \\(\\mu_{t}\\) is the drift , or the deviation of \\(y_{t}\\) from \\(0\\) . Under the null hypothesis, \\(\\mu_{t}\\) is constant (since \\(v_{t}\\) is constant). Under the alternative hypothesis, \\(\\mu_t\\) is a stochastic process with unit root. \\(\\beta t\\) is a linear trend , which represents deterministic non-stationarity (i.e. climate change). \\(\\epsilon_{t}\\) is stationary noise , corresponding to reversible fluctuations in \\(y_{t}\\) . In hydrology, \\(\\epsilon_{t}\\) represents fluctuations in streamflow due to random events (i.e. weather). \\(v_{t}\\) is random walk innovation , or irreversible fluctuations in \\(\\mu_{t}\\) . In hydrology, \\(v_{t}\\) could represent randomness in industrial activity causing climate change. To conduct the test, we fit a linear model to \\(y_{t}\\) and get the residuals \\(\\hat{r}_{t}\\) . Then, we compute the cumulative partial-sum statistics \\(S_{k}\\) using the following formula: \\[ S_{k} = \\sum_{t=1}^{k} \\hat{r}_{t} \\] Under the null hypothesis, \\(S_{k}\\) will behave like a random walk with finite variance. If \\(y_{t}\\) has a unit root, then the sums will \"drift\" too much. Next, we estimate the long-run variance of the time series while adjusting for autocovariance. To do this, we require the sample autocovariances \\(\\gamma_{j}\\) for up to \\(q\\) lags, where: \\[ q = \\left\\lfloor \\frac{3\\sqrt{n}}{13} \\right\\rfloor \\] The sample autocovariance \\(\\gamma_{j}\\) is a measure of the correlation between the time series \\(y_{t}\\) and the shifted time series \\(y_{t-j}\\) . Each sample autocovariance \\(\\gamma_{j}\\) for \\(j = 0, 1, \\dots, q\\) is computed as follows: \\[ \\hat{\\gamma}_{j} = \\frac{1}{n} \\sum_{t = j + 1}^{n} \\hat{r}_{t}\\hat{r}_{t-j} \\] Finally, we estimate the long-run variance \\(\\hat{\\lambda}^2\\) using a Newey-West style estimator. This estimator corrects for the additional variability in \\(\\epsilon_{t}\\) caused by autocorrelation and heteroskedasticity. \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_{0} + 2\\sum_{j=1}^{q} \\left(1 - \\frac{j}{q + 1} \\right) \\gamma_{j} \\] Then, we compute the test statistic \\(z_{\\text{KPSS}}\\) using the following formula: \\[ z_{\\text{KPSS}} = \\frac{1}{n^2\\hat{\\lambda }^2}\\sum_{k=1}^{n} S_{k}^2 \\] The test statistic \\(z_{\\text{KPSS}}\\) is not normally distributed. Instead, we compute the p-value by interpolating the table of quantiles from Hobjin et al. (2004) shown below. Probability 0.90 0.95 0.975 0.99 Quantile 0.119 0.146 0.176 0.216 Warning : The interpolation procedure discussed above only works for \\(0.01 < p < 0.10\\) . Therefore, p-values below \\(0.01\\) and above \\(0.10\\) will be truncated. It is also required that the significance level \\(\\alpha\\) is between \\(0.01\\) and \\(0.10\\) . Mann-Kendall Test The Mann-Kendall Test is used to assess whether there is a statistically significant monotonic trend in a time series. The test requires that when no trend is present, the data is independent and identically distributed. Null hypothesis: There is no monotonic trend. Alternative hypothesis: There is a monotonic upwards or downwards trend. Define \\(\\text{sign} (x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. The test statistic \\(S\\) is defined as follows: \\[ S = \\sum_{k-1}^{n-1} \\sum_{j - k + 1}^{n} \\text{sign} (y_{j} - y_{k}) \\] Next, we need to compute \\(\\text{Var}(S)\\) , which depends on the number of tied groups in the data. Let \\(g\\) be the number of tied groups and \\(t_{p}\\) be the number of observations in the \\(p\\) -th group. \\[\\text{Var}(S) = \\frac{1}{18} \\left[n(n-1)(2n + 1) - \\sum_{p-1}^{g} t_{p}(t_{p} - 1)(2t_{p} + 5) \\right]\\] Then, compute the MK test statistic, \\(Z_{MK}\\) , as follows: \\[ Z_{MK} = \\begin{cases} \\frac{S-1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S > 0 \\\\ 0 &\\text{if } S = 0 \\\\[4pt] \\frac{S+1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S < 0 \\end{cases} \\] For a two-sided test, we reject the null hypothesis if \\(|Z_{MK}| \\geq Z_{1 - (\\alpha/2) }\\) and conclude that there is a statistically significant monotonic trend in the data. For more information, see here . Phillips-Perron Test The Phillips-Perron (PP) Test is used to identify if an autoregressive time series \\(y_t\\) has a unit root . Null hypothesis: \\(y_{t}\\) has a unit root and is thus non-stationary . Alternative hypothesis: \\(y_{t}\\) does not have a unit root and is trend-stationary . Precisely, let \\(x_{t}\\) be an AR(1) model. Let \\(y_{t}\\) be a function of \\(x_{t}\\) with drift \\(\\beta_{0}\\) and trend \\(\\beta_{1} t\\) . \\[ \\begin{align} y_{t} &= \\beta_{0} + \\beta_{1} t + x_{t} \\\\[5pt] x_{t} &= \\rho x_{t-1} + \\epsilon_{t} \\end{align} \\] If \\(\\rho = 1\\) , then \\(x_t\\) and hence \\(y_t\\) has a unit root (null hypothesis). If \\(\\rho < 1\\) , then \\(y_t\\) is trend stationary (alternative hypothesis). To perform the test, we begin by fitting an autoregressive linear model to \\(y_{t}\\) . Let \\(\\hat{r}_{t}\\) be the residuals of this model. From this model, we can determine \\(\\hat{\\rho}\\) (the estimated coefficient on \\(y_{t-1}\\) ) and \\(\\text{SE}(\\hat{\\rho})\\) (its standard error). Let \\(\\hat{\\sigma}^2\\) be the variance of the residuals, computed using the following formula: \\[ \\hat{\\sigma^2} = \\frac{1}{n - 3} \\sum_{t=1}^{n} \\hat{r}_{t}^2 \\] In the equation above, \\(n\\) is the number of data points in the sample. We have \\(n-3\\) degrees of freedom since there are three parameters in the autoregressive model ( \\(\\beta_{0}\\) , \\(\\beta_{1}\\) , and \\(\\rho\\) ). Next, we compute the sample autocovariances \\(\\gamma_{j}\\) for up to \\(q\\) lags, where: \\[ q = \\left\\lfloor \\sqrt[4]{\\frac{n}{25}}\\right\\rfloor \\] The sample autocovariance \\(\\gamma_{j}\\) is a measure of the correlation between the time series \\(y_{t}\\) and the shifted time series \\(y_{t-j}\\) . Each sample autocovariance \\(\\gamma_{j}\\) for \\(j = 0, 1, \\dots, q\\) is computed as follows: \\[ \\hat{\\gamma}_{j} = \\frac{1}{n} \\sum_{t = j + 1}^{n} \\hat{r}_{t}\\hat{r}_{t-j} \\] Finally, we estimate the long-run variance \\(\\hat{\\lambda}^2\\) using a Newey-West style estimator. This estimator corrects for the additional variability in \\(\\epsilon_{t}\\) caused by autocorrelation and heteroskedasticity. \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_{0} + 2\\sum_{j=1}^{q} \\left(1 - \\frac{j}{q + 1} \\right) \\gamma_{j} \\] Then, we compute the test statistic \\(z_{\\rho}\\) using the following formula: \\[ z_{\\rho } = n(\\hat{\\rho} - 1) - \\frac{n^2 \\text{SE}(\\hat{\\rho})^2}{2 \\hat{\\sigma}^2}(\\hat{\\lambda }^2 - \\hat{\\gamma}_{0}) \\] The test statistic \\(z_{\\rho}\\) is not normally distributed. Instead, we compute the p-value by interpolating a table from Fuller, W. A. (1996). This table is shown below for sample sizes \\(n\\) and probabilities \\(p\\) : \\(n\\) \\ \\(p\\) 0.01 0.025 0.05 0.10 0.50 0.90 0.95 0.975 0.99 25 -22.5 -20.0 -17.9 -15.6 -8.49 -3.65 -2.51 -1.53 -0.46 50 -25.8 -22.4 -19.7 -16.8 -8.80 -3.71 -2.60 -1.67 -0.67 100 -27.4 -23.7 -20.6 -17.5 -8.96 -3.74 -2.63 -1.74 -0.76 250 -28.5 -24.4 -21.3 -17.9 -9.05 -3.76 -2.65 -1.79 -0.83 500 -28.9 -24.7 -21.5 -18.1 -9.08 -3.76 -2.66 -1.80 -0.86 1000 -29.4 -25.0 -21.7 -18.3 -9.11 -3.77 -2.67 -1.81 -0.88 Warning : The interpolation procedure discussed above only works for \\(0.01 < p\\) . Therefore, p-values below \\(0.01\\) will be truncated and it is required that \\(0.01 < \\alpha\\) . Runs Test After computing the regression line using Sen's trend estimator , we use the Runs Test to determine whether the residuals from the regression are random. If the Runs test identifies non-randomness in the residuals, it is a strong indication that the non-stationarity in the data is not linear. Null hypothesis: The residuals are distributed randomly. Alternative hypothesis: The residuals are not distributed randomly. Prior to applying the Runs test, the data is categorized based on whether it is above ( \\(+\\) ) or below \\((-)\\) the median. Any data points equal to the median are removed. Then, we compute the number of contiguous blocks of \\(+\\) or \\(-\\) (known as runs ) in the data. Example : Suppose that after categorization, the sequence of data is as follows: \\[ +++--+++-+- \\] This sequence has six runs with length \\((3, 2, 3, 1,1, 1)\\) . Let \\(R\\) be the number of runs in \\(N\\) data points (with category counts \\(N_{+}\\) and \\(N_{-}\\) ). Then, under the null hypothesis, \\(R\\) is asymptotically normal with: \\[ \\mathbb{E}[R] = \\frac{2N_{+}N_{-}}{N} + 1, \\quad \\text{Var}(R) = \\frac{2N_{+}N_{-}(2N_{+}N_{-} - N)}{N^2(N - 1)} \\] Example Plot Sen's Trend Estimator Sen's Trend Estimator is used to estimate the slope of a regression line. Unlike Least Squares , Sen's trend estimator uses a non-parametric approach which makes it robust to outliers. To compute Sen's trend estimator we use the following procedure: Iterate over all pairs of data points \\((x_{i}, y_{i})\\) and \\((x_{j}, y_{j})\\) . If \\(x_{i} \\neq x_{j}\\) , compute the slope \\((y_{j} - y_{i})/(x_{j} - x_{i})\\) and add it to a list \\(S\\) . Sen's trend estimator \\(\\hat{m}\\) is the median of \\(S\\) . After computing \\(\\hat{m}\\) , we can estimate the \\(y\\) -intercept \\(b\\) by the median of \\(y_{i} - \\hat{m}x_{i}\\) for all \\(i\\) . Example Plot Spearman Test The Spearman Test is used to identify autocorrelation in a time series \\(y_{t}\\) . A significant lag is a number \\(i\\) such that the correlation between \\(y_{t}\\) and \\(y_{t-i}\\) is statistically significant. The least insignificant lag is the largest \\(i\\) such that all \\(j < i\\) are significant lags. Null hypothesis: The least insignificant lag is \\(0\\) . Alternative hypothesis: The least insignificant lag is greater than \\(0\\) . To carry out the Spearman test, we use the following procedure: Compute Spearman's correlation coefficient \\(\\rho_{i}\\) for \\(y_{t}\\) and \\(y_{t-i}\\) for all \\(0 \\leq i < n\\) . Determine the \\(p\\) -value \\(p_{i}\\) for each correlation coefficient \\(\\rho _{i}\\) . Iterate through \\(p_{i}\\) to find the largest \\(i\\) such that \\(p_{j} \\leq \\alpha\\) for all \\(j \\leq i\\) . The value of \\(i\\) found in (3) is the least insignificant lag at confidence level \\(\\alpha\\) . Remark : To compute the \\(p\\) -value of a correlation coefficient \\(\\rho _{i}\\) , first compute: \\[ t_{i}= \\rho_{i} \\sqrt{\\frac{n-2}{1 - \\rho _{i}^2}} \\] Then, the test statistic \\(t_{i}\\) has the \\(t\\) -distribution with \\(n-2\\) degrees of freedom. For more information, see the Wikipedia pages on Autocorrelation and Spearman's Rho . Example Plot","title":"Trends in AMS Mean"},{"location":"eda-trend-ams-mean/#identifying-trends-in-the-ams-mean","text":"","title":"Identifying Trends in the AMS Mean"},{"location":"eda-trend-ams-mean/#bb-mk-test","text":"The Block Bootstrap Mann-Kendall (BB-MK) Test is used to assess whether there is a statistically significant monotonic trend in a time series. The BB-MK test is insensitive to autocorrelation , which is known to produce false positives in the MK test . Null hypothesis: There is no monotonic trend. Alternative hypothesis: There is a monotonic upwards or downwards trend. To carry out the BB-MK test, we rely on the results of the MK test and Spearman test. Compute the MK test statistic. Find the least insignificant lag \\(k\\) using the Spearman test. Resample from the original time series in blocks of size \\(k+1\\) without replacement. Estimate the MK test statistic for each bootstrapped sample. Derive the empirical distribution of the MK test statistic from the bootstrapped statistics. Estimate the significance of the observed test statistic using the empirical distribution.","title":"BB-MK Test"},{"location":"eda-trend-ams-mean/#example-plot","text":"","title":"Example Plot"},{"location":"eda-trend-ams-mean/#kpss-test","text":"The KPSS Test is used to identify if an autoregressive time series has a unit root . Null hypothesis: The time series does not have a unit root and is trend-stationary . Alternative hypothesis: The time series has a unit root and is non-stationary . Precisely, the autoregressive time series shown below has unit root if \\(\\sigma_{v}^2 > 0\\) : \\[ \\begin{align} y_{t} &= \\mu_{t} + \\beta t + \\epsilon_{t} \\\\[5pt] \\mu_{t} &= \\mu_{t-1} + v_{t} \\\\[5pt] v_{t} &\\sim \\mathcal{N}(0, \\sigma_{v}^2) \\end{align} \\] Here is what each term in this formulation represents: \\(\\mu_{t}\\) is the drift , or the deviation of \\(y_{t}\\) from \\(0\\) . Under the null hypothesis, \\(\\mu_{t}\\) is constant (since \\(v_{t}\\) is constant). Under the alternative hypothesis, \\(\\mu_t\\) is a stochastic process with unit root. \\(\\beta t\\) is a linear trend , which represents deterministic non-stationarity (i.e. climate change). \\(\\epsilon_{t}\\) is stationary noise , corresponding to reversible fluctuations in \\(y_{t}\\) . In hydrology, \\(\\epsilon_{t}\\) represents fluctuations in streamflow due to random events (i.e. weather). \\(v_{t}\\) is random walk innovation , or irreversible fluctuations in \\(\\mu_{t}\\) . In hydrology, \\(v_{t}\\) could represent randomness in industrial activity causing climate change. To conduct the test, we fit a linear model to \\(y_{t}\\) and get the residuals \\(\\hat{r}_{t}\\) . Then, we compute the cumulative partial-sum statistics \\(S_{k}\\) using the following formula: \\[ S_{k} = \\sum_{t=1}^{k} \\hat{r}_{t} \\] Under the null hypothesis, \\(S_{k}\\) will behave like a random walk with finite variance. If \\(y_{t}\\) has a unit root, then the sums will \"drift\" too much. Next, we estimate the long-run variance of the time series while adjusting for autocovariance. To do this, we require the sample autocovariances \\(\\gamma_{j}\\) for up to \\(q\\) lags, where: \\[ q = \\left\\lfloor \\frac{3\\sqrt{n}}{13} \\right\\rfloor \\] The sample autocovariance \\(\\gamma_{j}\\) is a measure of the correlation between the time series \\(y_{t}\\) and the shifted time series \\(y_{t-j}\\) . Each sample autocovariance \\(\\gamma_{j}\\) for \\(j = 0, 1, \\dots, q\\) is computed as follows: \\[ \\hat{\\gamma}_{j} = \\frac{1}{n} \\sum_{t = j + 1}^{n} \\hat{r}_{t}\\hat{r}_{t-j} \\] Finally, we estimate the long-run variance \\(\\hat{\\lambda}^2\\) using a Newey-West style estimator. This estimator corrects for the additional variability in \\(\\epsilon_{t}\\) caused by autocorrelation and heteroskedasticity. \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_{0} + 2\\sum_{j=1}^{q} \\left(1 - \\frac{j}{q + 1} \\right) \\gamma_{j} \\] Then, we compute the test statistic \\(z_{\\text{KPSS}}\\) using the following formula: \\[ z_{\\text{KPSS}} = \\frac{1}{n^2\\hat{\\lambda }^2}\\sum_{k=1}^{n} S_{k}^2 \\] The test statistic \\(z_{\\text{KPSS}}\\) is not normally distributed. Instead, we compute the p-value by interpolating the table of quantiles from Hobjin et al. (2004) shown below. Probability 0.90 0.95 0.975 0.99 Quantile 0.119 0.146 0.176 0.216 Warning : The interpolation procedure discussed above only works for \\(0.01 < p < 0.10\\) . Therefore, p-values below \\(0.01\\) and above \\(0.10\\) will be truncated. It is also required that the significance level \\(\\alpha\\) is between \\(0.01\\) and \\(0.10\\) .","title":"KPSS Test"},{"location":"eda-trend-ams-mean/#mann-kendall-test","text":"The Mann-Kendall Test is used to assess whether there is a statistically significant monotonic trend in a time series. The test requires that when no trend is present, the data is independent and identically distributed. Null hypothesis: There is no monotonic trend. Alternative hypothesis: There is a monotonic upwards or downwards trend. Define \\(\\text{sign} (x)\\) to be \\(1\\) if \\(x > 0\\) , \\(0\\) if \\(x = 0\\) , and \\(-1\\) otherwise. The test statistic \\(S\\) is defined as follows: \\[ S = \\sum_{k-1}^{n-1} \\sum_{j - k + 1}^{n} \\text{sign} (y_{j} - y_{k}) \\] Next, we need to compute \\(\\text{Var}(S)\\) , which depends on the number of tied groups in the data. Let \\(g\\) be the number of tied groups and \\(t_{p}\\) be the number of observations in the \\(p\\) -th group. \\[\\text{Var}(S) = \\frac{1}{18} \\left[n(n-1)(2n + 1) - \\sum_{p-1}^{g} t_{p}(t_{p} - 1)(2t_{p} + 5) \\right]\\] Then, compute the MK test statistic, \\(Z_{MK}\\) , as follows: \\[ Z_{MK} = \\begin{cases} \\frac{S-1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S > 0 \\\\ 0 &\\text{if } S = 0 \\\\[4pt] \\frac{S+1}{\\sqrt{\\text{Var}(S)}} &\\text{if } S < 0 \\end{cases} \\] For a two-sided test, we reject the null hypothesis if \\(|Z_{MK}| \\geq Z_{1 - (\\alpha/2) }\\) and conclude that there is a statistically significant monotonic trend in the data. For more information, see here .","title":"Mann-Kendall Test"},{"location":"eda-trend-ams-mean/#phillips-perron-test","text":"The Phillips-Perron (PP) Test is used to identify if an autoregressive time series \\(y_t\\) has a unit root . Null hypothesis: \\(y_{t}\\) has a unit root and is thus non-stationary . Alternative hypothesis: \\(y_{t}\\) does not have a unit root and is trend-stationary . Precisely, let \\(x_{t}\\) be an AR(1) model. Let \\(y_{t}\\) be a function of \\(x_{t}\\) with drift \\(\\beta_{0}\\) and trend \\(\\beta_{1} t\\) . \\[ \\begin{align} y_{t} &= \\beta_{0} + \\beta_{1} t + x_{t} \\\\[5pt] x_{t} &= \\rho x_{t-1} + \\epsilon_{t} \\end{align} \\] If \\(\\rho = 1\\) , then \\(x_t\\) and hence \\(y_t\\) has a unit root (null hypothesis). If \\(\\rho < 1\\) , then \\(y_t\\) is trend stationary (alternative hypothesis). To perform the test, we begin by fitting an autoregressive linear model to \\(y_{t}\\) . Let \\(\\hat{r}_{t}\\) be the residuals of this model. From this model, we can determine \\(\\hat{\\rho}\\) (the estimated coefficient on \\(y_{t-1}\\) ) and \\(\\text{SE}(\\hat{\\rho})\\) (its standard error). Let \\(\\hat{\\sigma}^2\\) be the variance of the residuals, computed using the following formula: \\[ \\hat{\\sigma^2} = \\frac{1}{n - 3} \\sum_{t=1}^{n} \\hat{r}_{t}^2 \\] In the equation above, \\(n\\) is the number of data points in the sample. We have \\(n-3\\) degrees of freedom since there are three parameters in the autoregressive model ( \\(\\beta_{0}\\) , \\(\\beta_{1}\\) , and \\(\\rho\\) ). Next, we compute the sample autocovariances \\(\\gamma_{j}\\) for up to \\(q\\) lags, where: \\[ q = \\left\\lfloor \\sqrt[4]{\\frac{n}{25}}\\right\\rfloor \\] The sample autocovariance \\(\\gamma_{j}\\) is a measure of the correlation between the time series \\(y_{t}\\) and the shifted time series \\(y_{t-j}\\) . Each sample autocovariance \\(\\gamma_{j}\\) for \\(j = 0, 1, \\dots, q\\) is computed as follows: \\[ \\hat{\\gamma}_{j} = \\frac{1}{n} \\sum_{t = j + 1}^{n} \\hat{r}_{t}\\hat{r}_{t-j} \\] Finally, we estimate the long-run variance \\(\\hat{\\lambda}^2\\) using a Newey-West style estimator. This estimator corrects for the additional variability in \\(\\epsilon_{t}\\) caused by autocorrelation and heteroskedasticity. \\[ \\hat{\\lambda}^2 = \\hat{\\gamma}_{0} + 2\\sum_{j=1}^{q} \\left(1 - \\frac{j}{q + 1} \\right) \\gamma_{j} \\] Then, we compute the test statistic \\(z_{\\rho}\\) using the following formula: \\[ z_{\\rho } = n(\\hat{\\rho} - 1) - \\frac{n^2 \\text{SE}(\\hat{\\rho})^2}{2 \\hat{\\sigma}^2}(\\hat{\\lambda }^2 - \\hat{\\gamma}_{0}) \\] The test statistic \\(z_{\\rho}\\) is not normally distributed. Instead, we compute the p-value by interpolating a table from Fuller, W. A. (1996). This table is shown below for sample sizes \\(n\\) and probabilities \\(p\\) : \\(n\\) \\ \\(p\\) 0.01 0.025 0.05 0.10 0.50 0.90 0.95 0.975 0.99 25 -22.5 -20.0 -17.9 -15.6 -8.49 -3.65 -2.51 -1.53 -0.46 50 -25.8 -22.4 -19.7 -16.8 -8.80 -3.71 -2.60 -1.67 -0.67 100 -27.4 -23.7 -20.6 -17.5 -8.96 -3.74 -2.63 -1.74 -0.76 250 -28.5 -24.4 -21.3 -17.9 -9.05 -3.76 -2.65 -1.79 -0.83 500 -28.9 -24.7 -21.5 -18.1 -9.08 -3.76 -2.66 -1.80 -0.86 1000 -29.4 -25.0 -21.7 -18.3 -9.11 -3.77 -2.67 -1.81 -0.88 Warning : The interpolation procedure discussed above only works for \\(0.01 < p\\) . Therefore, p-values below \\(0.01\\) will be truncated and it is required that \\(0.01 < \\alpha\\) .","title":"Phillips-Perron Test"},{"location":"eda-trend-ams-mean/#runs-test","text":"After computing the regression line using Sen's trend estimator , we use the Runs Test to determine whether the residuals from the regression are random. If the Runs test identifies non-randomness in the residuals, it is a strong indication that the non-stationarity in the data is not linear. Null hypothesis: The residuals are distributed randomly. Alternative hypothesis: The residuals are not distributed randomly. Prior to applying the Runs test, the data is categorized based on whether it is above ( \\(+\\) ) or below \\((-)\\) the median. Any data points equal to the median are removed. Then, we compute the number of contiguous blocks of \\(+\\) or \\(-\\) (known as runs ) in the data. Example : Suppose that after categorization, the sequence of data is as follows: \\[ +++--+++-+- \\] This sequence has six runs with length \\((3, 2, 3, 1,1, 1)\\) . Let \\(R\\) be the number of runs in \\(N\\) data points (with category counts \\(N_{+}\\) and \\(N_{-}\\) ). Then, under the null hypothesis, \\(R\\) is asymptotically normal with: \\[ \\mathbb{E}[R] = \\frac{2N_{+}N_{-}}{N} + 1, \\quad \\text{Var}(R) = \\frac{2N_{+}N_{-}(2N_{+}N_{-} - N)}{N^2(N - 1)} \\]","title":"Runs Test"},{"location":"eda-trend-ams-mean/#example-plot_1","text":"","title":"Example Plot"},{"location":"eda-trend-ams-mean/#sens-trend-estimator","text":"Sen's Trend Estimator is used to estimate the slope of a regression line. Unlike Least Squares , Sen's trend estimator uses a non-parametric approach which makes it robust to outliers. To compute Sen's trend estimator we use the following procedure: Iterate over all pairs of data points \\((x_{i}, y_{i})\\) and \\((x_{j}, y_{j})\\) . If \\(x_{i} \\neq x_{j}\\) , compute the slope \\((y_{j} - y_{i})/(x_{j} - x_{i})\\) and add it to a list \\(S\\) . Sen's trend estimator \\(\\hat{m}\\) is the median of \\(S\\) . After computing \\(\\hat{m}\\) , we can estimate the \\(y\\) -intercept \\(b\\) by the median of \\(y_{i} - \\hat{m}x_{i}\\) for all \\(i\\) .","title":"Sen's Trend Estimator"},{"location":"eda-trend-ams-mean/#example-plot_2","text":"","title":"Example Plot"},{"location":"eda-trend-ams-mean/#spearman-test","text":"The Spearman Test is used to identify autocorrelation in a time series \\(y_{t}\\) . A significant lag is a number \\(i\\) such that the correlation between \\(y_{t}\\) and \\(y_{t-i}\\) is statistically significant. The least insignificant lag is the largest \\(i\\) such that all \\(j < i\\) are significant lags. Null hypothesis: The least insignificant lag is \\(0\\) . Alternative hypothesis: The least insignificant lag is greater than \\(0\\) . To carry out the Spearman test, we use the following procedure: Compute Spearman's correlation coefficient \\(\\rho_{i}\\) for \\(y_{t}\\) and \\(y_{t-i}\\) for all \\(0 \\leq i < n\\) . Determine the \\(p\\) -value \\(p_{i}\\) for each correlation coefficient \\(\\rho _{i}\\) . Iterate through \\(p_{i}\\) to find the largest \\(i\\) such that \\(p_{j} \\leq \\alpha\\) for all \\(j \\leq i\\) . The value of \\(i\\) found in (3) is the least insignificant lag at confidence level \\(\\alpha\\) . Remark : To compute the \\(p\\) -value of a correlation coefficient \\(\\rho _{i}\\) , first compute: \\[ t_{i}= \\rho_{i} \\sqrt{\\frac{n-2}{1 - \\rho _{i}^2}} \\] Then, the test statistic \\(t_{i}\\) has the \\(t\\) -distribution with \\(n-2\\) degrees of freedom. For more information, see the Wikipedia pages on Autocorrelation and Spearman's Rho .","title":"Spearman Test"},{"location":"eda-trend-ams-mean/#example-plot_3","text":"","title":"Example Plot"},{"location":"eda-trend-ams-variability/","text":"Identifying Trends in the AMS Variability MW-MK Test The Moving Window Mann-Kendall (MW-MK) Test is used to identify a statistically significant monotonic trend in the variances of an AMS time series. Null hypothesis: There is no significant trend in the variance of the AMS. Alternative hypothesis: There is a significant trend in the variance of the AMS. To compute the AMS variances we use a moving window algorithm: Let \\(w\\) be the length of the moving window and \\(s\\) be the step size. Initialize the moving window at indices \\([1, w]\\) . Compute the sample standard deviation within the moving window. Move the window forward by \\(s\\) indices. Check if last index in the window is greater than the length of the data. If it is, all moving window variances have been computed. Otherwise, go to step (3). Then, we perform the Mann-Kendall Test on the time series of variances. For more information about the Mann-Kendall test, see here . Sen's Trend Estimator See here . Runs Test See here . White Test The White Test is used to detect changes in the variance of a time series. Null hypothesis: The variance of the time series is constant (homoskedasticity). Alternative hypothesis: The variance of the time series is time-dependent (heteroskedasticity). Consider a simple linear regression model: \\[y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i}\\] Use ordinary least squares to fit the model. Then compute the squared residuals: \\[{\\hat{r}}_{i}^{2} = (y_{i} - \\hat{y}_{i})^{2}\\] Next, fit an auxillary regression model to the squared residuals. This model includes each regressor, the square of each regressor, and the cross products between all regressors. Since \\(x\\) is the only regressor, the regression model is simply: \\[{\\hat{r}}_{i}^{2} = \\alpha_{0} + \\alpha_{1}x_{i} + \\alpha_{2}x_{i}^{2} + u_{i}\\] Next, we compute the coefficient of determination \\(R^2\\) for the auxillary model. The test statistic is \\(nR^2 \\sim \\chi_{d}^2\\) , where \\(n\\) is the number of observations and \\(d = 2\\) is the number of regressors, excluding the intercept. If \\(nR^2 > \\chi^2_{1-\\alpha, d}\\) , we reject the null hypothesis and conclude that the time series exhibits heteroskedasticity.","title":"Trends in AMS Variability"},{"location":"eda-trend-ams-variability/#identifying-trends-in-the-ams-variability","text":"","title":"Identifying Trends in the AMS Variability"},{"location":"eda-trend-ams-variability/#mw-mk-test","text":"The Moving Window Mann-Kendall (MW-MK) Test is used to identify a statistically significant monotonic trend in the variances of an AMS time series. Null hypothesis: There is no significant trend in the variance of the AMS. Alternative hypothesis: There is a significant trend in the variance of the AMS. To compute the AMS variances we use a moving window algorithm: Let \\(w\\) be the length of the moving window and \\(s\\) be the step size. Initialize the moving window at indices \\([1, w]\\) . Compute the sample standard deviation within the moving window. Move the window forward by \\(s\\) indices. Check if last index in the window is greater than the length of the data. If it is, all moving window variances have been computed. Otherwise, go to step (3). Then, we perform the Mann-Kendall Test on the time series of variances. For more information about the Mann-Kendall test, see here .","title":"MW-MK Test"},{"location":"eda-trend-ams-variability/#sens-trend-estimator","text":"See here .","title":"Sen's Trend Estimator"},{"location":"eda-trend-ams-variability/#runs-test","text":"See here .","title":"Runs Test"},{"location":"eda-trend-ams-variability/#white-test","text":"The White Test is used to detect changes in the variance of a time series. Null hypothesis: The variance of the time series is constant (homoskedasticity). Alternative hypothesis: The variance of the time series is time-dependent (heteroskedasticity). Consider a simple linear regression model: \\[y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i}\\] Use ordinary least squares to fit the model. Then compute the squared residuals: \\[{\\hat{r}}_{i}^{2} = (y_{i} - \\hat{y}_{i})^{2}\\] Next, fit an auxillary regression model to the squared residuals. This model includes each regressor, the square of each regressor, and the cross products between all regressors. Since \\(x\\) is the only regressor, the regression model is simply: \\[{\\hat{r}}_{i}^{2} = \\alpha_{0} + \\alpha_{1}x_{i} + \\alpha_{2}x_{i}^{2} + u_{i}\\] Next, we compute the coefficient of determination \\(R^2\\) for the auxillary model. The test statistic is \\(nR^2 \\sim \\chi_{d}^2\\) , where \\(n\\) is the number of observations and \\(d = 2\\) is the number of regressors, excluding the intercept. If \\(nR^2 > \\chi^2_{1-\\alpha, d}\\) , we reject the null hypothesis and conclude that the time series exhibits heteroskedasticity.","title":"White Test"},{"location":"ffa-introduction/","text":"Frequency Analysis Overview Flood Frequency Analysis (FFA) is the act of using a fitted probability distribution to make predictions about the frequency of extreme streamflow events (i.e. floods). To do FFA, we require a probability model with suitably chosen parameters based on the data. Typically, we describe the severity of floods in terms of their return period . Suppose we have a flood, which I will refer to as \\(A\\) . If we expect to see a flood at least as severe as \\(A\\) every ten years, then we say that \\(A\\) is a ten-year flood . Since our framework uses annual maximum streamflow data, a ten-year flood corresponds to an exceedance probability of \\(0.1\\) . Note that an exceedance probability of \\(0.1\\) corresponds to the \\(1 - 0.1 = 0.90\\) quantile of our probability distribution. Here is a table of the return periods, exceedance probabilities, and quantiles used in the FFA framework: Return Period Exceedance Probability Quantile \\(2\\) Years \\(0.50\\) \\(0.50\\) \\(5\\) Years \\(0.20\\) \\(0.80\\) \\(10\\) Years \\(0.10\\) \\(0.90\\) \\(20\\) Years \\(0.05\\) \\(0.95\\) \\(50\\) Years \\(0.02\\) \\(0.98\\) \\(100\\) Years \\(0.01\\) \\(0.99\\) Suppose our fitted probability distribution has cumulative distribution function \\(F(x)\\) . The function \\(F(x)\\) maps annual maximum streamflow values to probabilities. However, we want to determine the streamflow from the probabilities, so we use the inverse of the cumulative distribution \\(F^{-1}(x)\\) instead. The function \\(F^{-1}(x)\\) is also known as the Quantile Function . Example Plot We typically present the results of a flood frequency analysis as a graph with time on the \\(x\\) -axis and annual maximum streamflow on the \\(y\\) -axis. There are two ways to read a graph like this: Estimate the severity of a flood for a given time period. From the graph below, we could determine that every 50 years, we can expect a streamflow event of approximately \\(85\\text{m}^3/\\text{s}\\) . Estimate the frequency of a flood for a given severity. From the graph below, we could determine that a streamflow event of \\(50\\text{m}^3/\\text{s}\\) or higher will occur every 4 years. Note : For information about how the confidence bounds in this figure are determined, see here . Handling Non-Stationarity We say that a distribution is Non-Stationary if its mean or variance (or both) are changing over time. Under non-stationarity, the quantile function of our fitted probability distribution is also a function of time \\(F^{-1}(x, t)\\) . This means that our streamflow estimates and return periods also vary with time, so we cannot report a single set of estimates and confidence intervals as we did in the plot above. For non-stationary models, the FFA framework computes Effective Return Periods , which are streamflow estimates based on the quantile function for a specific year . Example Plot The plot below shows the effective return periods for the year 2017. Note : A flood with an effective return period of 100 years will not occur every 100 years due to the non-stationarity of the model. Instead, an effective return period of 100 years means that in the anchor year (2017), we predict there is a \\(1/100\\) probability of a flood with the given severity. An Idea for Estimating Return Periods Suppose we want to estimate the severity of a flood with return period \\(t_{0}\\) at time \\(t^{*}\\) . Generate a sample \\(x_{1}, \\dots, x_{t_{0}}\\) where \\(x_{i} \\sim \\text{Unif} [0, 1]\\) . Compute simulated streamflow events \\(y_{1}, \\dots, y_{t_{0}}\\) where \\(y_{i} = F^{-1}(x_{i}, t^{*} + i)\\) . Let \\(z = \\max (y_{1}, \\dots, y_{t_{0}})\\) be the most severe streamflow event in the sample. Repeat steps 1-3 \\(n\\) times to generate a sequence \\((z_{1}, \\dots, z_{n})\\) . The mean \\((z_{1}, \\dots, z_{n}) / n\\) is an accurate estimate of the severity.","title":"Introduction"},{"location":"ffa-introduction/#frequency-analysis","text":"","title":"Frequency Analysis"},{"location":"ffa-introduction/#overview","text":"Flood Frequency Analysis (FFA) is the act of using a fitted probability distribution to make predictions about the frequency of extreme streamflow events (i.e. floods). To do FFA, we require a probability model with suitably chosen parameters based on the data. Typically, we describe the severity of floods in terms of their return period . Suppose we have a flood, which I will refer to as \\(A\\) . If we expect to see a flood at least as severe as \\(A\\) every ten years, then we say that \\(A\\) is a ten-year flood . Since our framework uses annual maximum streamflow data, a ten-year flood corresponds to an exceedance probability of \\(0.1\\) . Note that an exceedance probability of \\(0.1\\) corresponds to the \\(1 - 0.1 = 0.90\\) quantile of our probability distribution. Here is a table of the return periods, exceedance probabilities, and quantiles used in the FFA framework: Return Period Exceedance Probability Quantile \\(2\\) Years \\(0.50\\) \\(0.50\\) \\(5\\) Years \\(0.20\\) \\(0.80\\) \\(10\\) Years \\(0.10\\) \\(0.90\\) \\(20\\) Years \\(0.05\\) \\(0.95\\) \\(50\\) Years \\(0.02\\) \\(0.98\\) \\(100\\) Years \\(0.01\\) \\(0.99\\) Suppose our fitted probability distribution has cumulative distribution function \\(F(x)\\) . The function \\(F(x)\\) maps annual maximum streamflow values to probabilities. However, we want to determine the streamflow from the probabilities, so we use the inverse of the cumulative distribution \\(F^{-1}(x)\\) instead. The function \\(F^{-1}(x)\\) is also known as the Quantile Function .","title":"Overview"},{"location":"ffa-introduction/#example-plot","text":"We typically present the results of a flood frequency analysis as a graph with time on the \\(x\\) -axis and annual maximum streamflow on the \\(y\\) -axis. There are two ways to read a graph like this: Estimate the severity of a flood for a given time period. From the graph below, we could determine that every 50 years, we can expect a streamflow event of approximately \\(85\\text{m}^3/\\text{s}\\) . Estimate the frequency of a flood for a given severity. From the graph below, we could determine that a streamflow event of \\(50\\text{m}^3/\\text{s}\\) or higher will occur every 4 years. Note : For information about how the confidence bounds in this figure are determined, see here .","title":"Example Plot"},{"location":"ffa-introduction/#handling-non-stationarity","text":"We say that a distribution is Non-Stationary if its mean or variance (or both) are changing over time. Under non-stationarity, the quantile function of our fitted probability distribution is also a function of time \\(F^{-1}(x, t)\\) . This means that our streamflow estimates and return periods also vary with time, so we cannot report a single set of estimates and confidence intervals as we did in the plot above. For non-stationary models, the FFA framework computes Effective Return Periods , which are streamflow estimates based on the quantile function for a specific year .","title":"Handling Non-Stationarity"},{"location":"ffa-introduction/#example-plot_1","text":"The plot below shows the effective return periods for the year 2017. Note : A flood with an effective return period of 100 years will not occur every 100 years due to the non-stationarity of the model. Instead, an effective return period of 100 years means that in the anchor year (2017), we predict there is a \\(1/100\\) probability of a flood with the given severity.","title":"Example Plot"},{"location":"ffa-introduction/#an-idea-for-estimating-return-periods","text":"Suppose we want to estimate the severity of a flood with return period \\(t_{0}\\) at time \\(t^{*}\\) . Generate a sample \\(x_{1}, \\dots, x_{t_{0}}\\) where \\(x_{i} \\sim \\text{Unif} [0, 1]\\) . Compute simulated streamflow events \\(y_{1}, \\dots, y_{t_{0}}\\) where \\(y_{i} = F^{-1}(x_{i}, t^{*} + i)\\) . Let \\(z = \\max (y_{1}, \\dots, y_{t_{0}})\\) be the most severe streamflow event in the sample. Repeat steps 1-3 \\(n\\) times to generate a sequence \\((z_{1}, \\dots, z_{n})\\) . The mean \\((z_{1}, \\dots, z_{n}) / n\\) is an accurate estimate of the severity.","title":"An Idea for Estimating Return Periods"},{"location":"goals/","text":"Goals Vidrio-Sahag\u00fan et al. (2024) identified a number of issues with FFA research in Canada. Our framework aims to address these issues. In particular, we hope to achieve: Standardization : The United States uses a standardized distribution (LP3) and parameter estimation method (expected moments) for FFA. However, Canada does not provide such guidance. By developing this framework, we hope to limit subjectivity in flood estimation. Reproducibility : A significant fraction of FFA studies in Canada do not provide essential information for reproduction, such as the exploratory data analysis (EDA) performed, the distribution selection mechanism, and even the probability distribution itself. Our framework will ensure all of this information is available. Statistical Rigour : Historically, many FFA studies have not performed uncertainty quantification, which makes it difficult to draw accurate conclusion from the results. Our framework automatically performs uncertainty analysis to quantify potential errors. Research-to-Practice Translation : The disorganization of FFA research in Canada makes it difficult for regulatory agencies to access cutting-edge advancements in FFA methodologies. By providing a common set of techniques for modellers to use, we hope to bridge the gap between research and practice. Development is guided by the following principles: Software Freedom : Our framework is built on free and open source software. Modularity : Users are allowed to use as much or as little of the framework as they like. Interoperability : Our framework can be seamlessly integrated with other flood models. Flexibility : Users can tailor their analysis to the nuances of individual watersheds. Clarity : The source code is easy to read and understand without confusing tricks. Robustness : The framework can handle datasets with small sample sizes or missing values. Speed : The framework is fast enough to handle batch processing of many datasets. Architecture We believe the following architecture will allow us to achieve the goals listed above: CRAN Package Our CRAN package will contain a suite of statistical tests, distribution selection mechanisms, and probability distributions for FFA. Many of the statistical tests used in our framework are sparsely documented, so providing an implementation of these tests in R will benefit the statistical community at large. Command Line Interface The command line interface (CLI) will allow technical users to programmatically generate reports and run statistical models. Additionally, the CLI will make it possible to run each component of the FFA framework separately. This will allow advanced users to design custom analysis pipelines. Plumber API An API (built with plumber.R ) will provide programmatic access to our framework. Web App Finally, we will a develop a web application on top of the Plumber API which will give new users a simple GUI interface for accessing our framework. We hope that this web application will make it easier for beginner modellers to learn about FFA. Non-Goals Warning : This list is not final and will be updated as development continues. This software will not support the following features: Explanations of FFA itself. We assume the modeller is familiar with basic hydrological concepts. Handling different time periods (i.e. monthly maximum streamflow data). Transforming the AMS data in any way (i.e. removing serial correlation, removing trends).","title":"Goals"},{"location":"goals/#goals","text":"Vidrio-Sahag\u00fan et al. (2024) identified a number of issues with FFA research in Canada. Our framework aims to address these issues. In particular, we hope to achieve: Standardization : The United States uses a standardized distribution (LP3) and parameter estimation method (expected moments) for FFA. However, Canada does not provide such guidance. By developing this framework, we hope to limit subjectivity in flood estimation. Reproducibility : A significant fraction of FFA studies in Canada do not provide essential information for reproduction, such as the exploratory data analysis (EDA) performed, the distribution selection mechanism, and even the probability distribution itself. Our framework will ensure all of this information is available. Statistical Rigour : Historically, many FFA studies have not performed uncertainty quantification, which makes it difficult to draw accurate conclusion from the results. Our framework automatically performs uncertainty analysis to quantify potential errors. Research-to-Practice Translation : The disorganization of FFA research in Canada makes it difficult for regulatory agencies to access cutting-edge advancements in FFA methodologies. By providing a common set of techniques for modellers to use, we hope to bridge the gap between research and practice. Development is guided by the following principles: Software Freedom : Our framework is built on free and open source software. Modularity : Users are allowed to use as much or as little of the framework as they like. Interoperability : Our framework can be seamlessly integrated with other flood models. Flexibility : Users can tailor their analysis to the nuances of individual watersheds. Clarity : The source code is easy to read and understand without confusing tricks. Robustness : The framework can handle datasets with small sample sizes or missing values. Speed : The framework is fast enough to handle batch processing of many datasets.","title":"Goals"},{"location":"goals/#architecture","text":"We believe the following architecture will allow us to achieve the goals listed above:","title":"Architecture"},{"location":"goals/#cran-package","text":"Our CRAN package will contain a suite of statistical tests, distribution selection mechanisms, and probability distributions for FFA. Many of the statistical tests used in our framework are sparsely documented, so providing an implementation of these tests in R will benefit the statistical community at large.","title":"CRAN Package"},{"location":"goals/#command-line-interface","text":"The command line interface (CLI) will allow technical users to programmatically generate reports and run statistical models. Additionally, the CLI will make it possible to run each component of the FFA framework separately. This will allow advanced users to design custom analysis pipelines.","title":"Command Line Interface"},{"location":"goals/#plumber-api","text":"An API (built with plumber.R ) will provide programmatic access to our framework.","title":"Plumber API"},{"location":"goals/#web-app","text":"Finally, we will a develop a web application on top of the Plumber API which will give new users a simple GUI interface for accessing our framework. We hope that this web application will make it easier for beginner modellers to learn about FFA.","title":"Web App"},{"location":"goals/#non-goals","text":"Warning : This list is not final and will be updated as development continues. This software will not support the following features: Explanations of FFA itself. We assume the modeller is familiar with basic hydrological concepts. Handling different time periods (i.e. monthly maximum streamflow data). Transforming the AMS data in any way (i.e. removing serial correlation, removing trends).","title":"Non-Goals"},{"location":"license/","text":"License FFA Framework \u00a9 2025 by Riley Wheadon, Cuauht\u00e9moc Tonatiuh Vidrio-Sahag\u00fan, and Alain Pietroniro is licensed under CC BY 4.0. To view a copy of this license, visit https://creativecommons.org/licenses/by/4.0/","title":"License"},{"location":"license/#license","text":"FFA Framework \u00a9 2025 by Riley Wheadon, Cuauht\u00e9moc Tonatiuh Vidrio-Sahag\u00fan, and Alain Pietroniro is licensed under CC BY 4.0. To view a copy of this license, visit https://creativecommons.org/licenses/by/4.0/","title":"License"},{"location":"matlab-version/","text":"This page documents changes from original MATLAB code . General Improvements Store configuration details in YAML files instead of code. Implement a suite of unit tests using the testthat library. Use knitr and rmarkdown to generate reports with text, equations, and images. Easily run individual statistical tests and model fitting functions using the R package. Export reports to .md (Pandoc), .html , and .pdf . Exploratory Data Analysis (EDA) Bug Fixes Only show statistically significant change points for the Pettitt and MKS plots. Fix bug causing the MKS test to only identify one change point, even if multiple change points were found to be above the threshold for statistical significance. Fix major bug causing the MKS test to identify change points based on the progressive series instead of the z-statistics of the crossing points. Remove unnecessary rounding in the moving window algorithm for AMS variability. Re-implement the Phillips-Perron and KPSS tests to account for drift and trend. Framework Changes If serial correlation is identified, do not run the Phillips-Perron and KPSS tests. Add the the Runs test to detect nonlinearity after fitting Sen's trend estimator. Run change point detection in multiple stages to prevent overpartitioning. Flood Frequency Analysis (FFA) Distribution Changes The generalized Pareto (GPA) distribution has been removed, since its likelihood function is not amenable to maximum likelihood estimation. Issues occur because the GPA distribution is intended for peaks over threshold modelling, which we do not use. The R version uses the three parameter Weibull distribution (with location, scale, and shape) parameters instead of the two parameter Weibull distribution (with scale and shape parameters). This ensures consistency with the other distributions, which all have location parameters. Model Selection Changes The L-distance and L-kurtosis selection methods have been improved by using an optimization algorithm to find the parameters with the closest L-moments to the data instead of using a brute force approach. This is more computationally efficient and gives more precise results. The procedure for computing the Z-statistic selection metric has been changed. If the L-moments of the dataset do not satisfy \\(\\tau_{4} \\leq (1 + 5\\tau _{3}^2)/6\\) , then the Kappa distribution will not be fitted and the candidate distributions that use the dataset will be omitted. Parameter Estimation Changes Parameterization of the PE3/LP3 distributions fails for some datasets because MATLAB is unable to handle the large numbers created by the gamma function. To manage this issue, the MATLAB version used the conventional moments (i.e. sample mean/variance/skewness) when this occurred. We avoid this problem in the R version by using the lgamma (log-gamma) function. The R implementation uses L-BFGS-B for MLE/GMLE parameter estimation instead of Nelder-Mead, since the gradient is well defined for the likelihood functions we are working with. Additionally, the L-BFGS-B method makes it possible to assign bounds to the variables. This modification produced slight improvements to the MLL/GMLL for some datasets. Uncertainty Quantification Implement RFPL uncertainty quantification for the Weibull distribution. Model Assessment Changes Use the built-in R function approx() to perform log-linear interpolation of the return periods. The MATLAB implementation uses a hard-coded algorithm which behaves unpredictably when the original and interpolated \\(x\\) -values are equal.","title":"MATLAB Version"},{"location":"matlab-version/#general-improvements","text":"Store configuration details in YAML files instead of code. Implement a suite of unit tests using the testthat library. Use knitr and rmarkdown to generate reports with text, equations, and images. Easily run individual statistical tests and model fitting functions using the R package. Export reports to .md (Pandoc), .html , and .pdf .","title":"General Improvements"},{"location":"matlab-version/#exploratory-data-analysis-eda","text":"","title":"Exploratory Data Analysis (EDA)"},{"location":"matlab-version/#bug-fixes","text":"Only show statistically significant change points for the Pettitt and MKS plots. Fix bug causing the MKS test to only identify one change point, even if multiple change points were found to be above the threshold for statistical significance. Fix major bug causing the MKS test to identify change points based on the progressive series instead of the z-statistics of the crossing points. Remove unnecessary rounding in the moving window algorithm for AMS variability. Re-implement the Phillips-Perron and KPSS tests to account for drift and trend.","title":"Bug Fixes"},{"location":"matlab-version/#framework-changes","text":"If serial correlation is identified, do not run the Phillips-Perron and KPSS tests. Add the the Runs test to detect nonlinearity after fitting Sen's trend estimator. Run change point detection in multiple stages to prevent overpartitioning.","title":"Framework Changes"},{"location":"matlab-version/#flood-frequency-analysis-ffa","text":"","title":"Flood Frequency Analysis (FFA)"},{"location":"matlab-version/#distribution-changes","text":"The generalized Pareto (GPA) distribution has been removed, since its likelihood function is not amenable to maximum likelihood estimation. Issues occur because the GPA distribution is intended for peaks over threshold modelling, which we do not use. The R version uses the three parameter Weibull distribution (with location, scale, and shape) parameters instead of the two parameter Weibull distribution (with scale and shape parameters). This ensures consistency with the other distributions, which all have location parameters.","title":"Distribution Changes"},{"location":"matlab-version/#model-selection-changes","text":"The L-distance and L-kurtosis selection methods have been improved by using an optimization algorithm to find the parameters with the closest L-moments to the data instead of using a brute force approach. This is more computationally efficient and gives more precise results. The procedure for computing the Z-statistic selection metric has been changed. If the L-moments of the dataset do not satisfy \\(\\tau_{4} \\leq (1 + 5\\tau _{3}^2)/6\\) , then the Kappa distribution will not be fitted and the candidate distributions that use the dataset will be omitted.","title":"Model Selection Changes"},{"location":"matlab-version/#parameter-estimation-changes","text":"Parameterization of the PE3/LP3 distributions fails for some datasets because MATLAB is unable to handle the large numbers created by the gamma function. To manage this issue, the MATLAB version used the conventional moments (i.e. sample mean/variance/skewness) when this occurred. We avoid this problem in the R version by using the lgamma (log-gamma) function. The R implementation uses L-BFGS-B for MLE/GMLE parameter estimation instead of Nelder-Mead, since the gradient is well defined for the likelihood functions we are working with. Additionally, the L-BFGS-B method makes it possible to assign bounds to the variables. This modification produced slight improvements to the MLL/GMLL for some datasets.","title":"Parameter Estimation Changes"},{"location":"matlab-version/#uncertainty-quantification","text":"Implement RFPL uncertainty quantification for the Weibull distribution.","title":"Uncertainty Quantification"},{"location":"matlab-version/#model-assessment-changes","text":"Use the built-in R function approx() to perform log-linear interpolation of the return periods. The MATLAB implementation uses a hard-coded algorithm which behaves unpredictably when the original and interpolated \\(x\\) -values are equal.","title":"Model Assessment Changes"},{"location":"model-assessment/","text":"Model Assessment Non-Parametric Models A Plotting Position is a non-parametric estimator used to derive empirical exceedance probabilities. By using the plotting position, we can evaluate the quality of our parametric model. To compute the plotting position, arrange the sample observations in descending order of magnitude: \\(x_{n:n} \\geq \\dots \\geq x_{1:n}\\) . Then, the empirical exceedance probabilities are given by the following formula: \\[ p_{i:n} = \\frac{i-a}{n+1 - 2a}, \\quad i \\in \\{1, \\dots , n\\} \\] The coefficient \\(a\\) depends on the plotting position formula: Formula \\(a\\) Simplified Equation Weibull \\(0\\) \\(p_{i:n} = \\frac{i}{n +1}\\) Blom \\(0.375\\) \\(p_{i:n} = \\frac{i-0.375}{n + 0.25}\\) Cunnane \\(0.4\\) \\(p_{i:n} = \\frac{i-0.4}{n+0.2}\\) Gringorten \\(0.44\\) \\(p_{i:n} = \\frac{i-0.44}{n + 0.12}\\) Hazen \\(0.5\\) \\(p_{i:n} = \\frac{i-0.5}{n}\\) By default, the FFA framework uses the Weibull formula, which is unbiased. Accuracy Statistics \\(R^2\\) - Coefficient of Determination To compute the \\(R^2\\) statistic, we perform a linear regression of the streamflow data against the predictions of the parametric model at the plotting positions. The \\(R^2\\) statistic describes how well the parametric model captures variance in the streamflow data. Higher is better. The plot below shows the deviation of the estimated quantiles (red dots), from the data (black line). RMSE - Root-Mean Squared Error The RMSE statistic describes the average squared difference between the streamflow data and the predictions of the parametric model. Lower is better. Bias The Bias statistic describes the average difference between the data and the predictions of the parametric model. A positive bias indicates that the model tends to overestimate the data while a negative bias indicates that the model tends to underestimate the data. Information Criterion The Akaike Information Criterion ( AIC ) and Bayesian Information Criterion ( BIC ) describe the quality of a model based on the error ( RMSE ) and the number of parameters ( n_theta ). Better models have a lower AIC / BIC , which indicates that they have less parameters and lower error. AIC <- (n * log(RMSE)) + (2 * n_theta) BIC <- (n * log(RMSE)) + (log(n) * n_theta) The Akaike/Bayesian information criterion can also be computed using the maximum log-likelihood from maximum likelihood estimation . These statistics are reported as AIC_MLL and BIC_MLL . AIC_MLL <- (n * log(MLL)) + (2 * n_theta) BIC_MLL <- (n * log(MLL)) + (log(n) * n_theta) Uncertainty Statistics The FFA framework uses three statistics to assess the uncertainty in flood quantile estimates: AW captures precision (narrower confidence intervals are better). POC captures reliability (higher coverage of observations is better). CWI is a composite measure balancing both precision and reliability (lower is better). We use these metrics together to evaluate the robustness of the flood frequency analysis. AW \u2013 Average Width AW is the average width of the interpolated confidence intervals across return periods of interest. A smaller AW indicates more precise quantile estimates. To compute AW , we use log-linear interpolation to estimate the confidence intervals of the empirical exceedance probabilities from the confidence intervals computed during uncertainty quantification . POC \u2013 Percent of Coverage POC is the percentage of observed quantiles that fall within their corresponding confidence intervals. A higher POC indicates greater reliability of the confidence intervals. CWI \u2013 Confidence Width Indicator CWI is a composite metric that penalizes wide and/or poorly calibrated confidence intervals. A lower CWI is better. Wide intervals and low coverage increase the penalty. Ideal confidence intervals are both narrow and well-calibrated, resulting in a low CWI . The CWI is computed using the following formula, where alpha is the significance level. CWI <- AW * exp((1 - alpha) - POC / 100)^2;","title":"Model Assessment"},{"location":"model-assessment/#model-assessment","text":"","title":"Model Assessment"},{"location":"model-assessment/#non-parametric-models","text":"A Plotting Position is a non-parametric estimator used to derive empirical exceedance probabilities. By using the plotting position, we can evaluate the quality of our parametric model. To compute the plotting position, arrange the sample observations in descending order of magnitude: \\(x_{n:n} \\geq \\dots \\geq x_{1:n}\\) . Then, the empirical exceedance probabilities are given by the following formula: \\[ p_{i:n} = \\frac{i-a}{n+1 - 2a}, \\quad i \\in \\{1, \\dots , n\\} \\] The coefficient \\(a\\) depends on the plotting position formula: Formula \\(a\\) Simplified Equation Weibull \\(0\\) \\(p_{i:n} = \\frac{i}{n +1}\\) Blom \\(0.375\\) \\(p_{i:n} = \\frac{i-0.375}{n + 0.25}\\) Cunnane \\(0.4\\) \\(p_{i:n} = \\frac{i-0.4}{n+0.2}\\) Gringorten \\(0.44\\) \\(p_{i:n} = \\frac{i-0.44}{n + 0.12}\\) Hazen \\(0.5\\) \\(p_{i:n} = \\frac{i-0.5}{n}\\) By default, the FFA framework uses the Weibull formula, which is unbiased.","title":"Non-Parametric Models"},{"location":"model-assessment/#accuracy-statistics","text":"","title":"Accuracy Statistics"},{"location":"model-assessment/#r2-coefficient-of-determination","text":"To compute the \\(R^2\\) statistic, we perform a linear regression of the streamflow data against the predictions of the parametric model at the plotting positions. The \\(R^2\\) statistic describes how well the parametric model captures variance in the streamflow data. Higher is better. The plot below shows the deviation of the estimated quantiles (red dots), from the data (black line).","title":"\\(R^2\\) - Coefficient of Determination"},{"location":"model-assessment/#rmse-root-mean-squared-error","text":"The RMSE statistic describes the average squared difference between the streamflow data and the predictions of the parametric model. Lower is better.","title":"RMSE - Root-Mean Squared Error"},{"location":"model-assessment/#bias","text":"The Bias statistic describes the average difference between the data and the predictions of the parametric model. A positive bias indicates that the model tends to overestimate the data while a negative bias indicates that the model tends to underestimate the data.","title":"Bias"},{"location":"model-assessment/#information-criterion","text":"The Akaike Information Criterion ( AIC ) and Bayesian Information Criterion ( BIC ) describe the quality of a model based on the error ( RMSE ) and the number of parameters ( n_theta ). Better models have a lower AIC / BIC , which indicates that they have less parameters and lower error. AIC <- (n * log(RMSE)) + (2 * n_theta) BIC <- (n * log(RMSE)) + (log(n) * n_theta) The Akaike/Bayesian information criterion can also be computed using the maximum log-likelihood from maximum likelihood estimation . These statistics are reported as AIC_MLL and BIC_MLL . AIC_MLL <- (n * log(MLL)) + (2 * n_theta) BIC_MLL <- (n * log(MLL)) + (log(n) * n_theta)","title":"Information Criterion"},{"location":"model-assessment/#uncertainty-statistics","text":"The FFA framework uses three statistics to assess the uncertainty in flood quantile estimates: AW captures precision (narrower confidence intervals are better). POC captures reliability (higher coverage of observations is better). CWI is a composite measure balancing both precision and reliability (lower is better). We use these metrics together to evaluate the robustness of the flood frequency analysis.","title":"Uncertainty Statistics"},{"location":"model-assessment/#aw-average-width","text":"AW is the average width of the interpolated confidence intervals across return periods of interest. A smaller AW indicates more precise quantile estimates. To compute AW , we use log-linear interpolation to estimate the confidence intervals of the empirical exceedance probabilities from the confidence intervals computed during uncertainty quantification .","title":"AW \u2013 Average Width"},{"location":"model-assessment/#poc-percent-of-coverage","text":"POC is the percentage of observed quantiles that fall within their corresponding confidence intervals. A higher POC indicates greater reliability of the confidence intervals.","title":"POC \u2013 Percent of Coverage"},{"location":"model-assessment/#cwi-confidence-width-indicator","text":"CWI is a composite metric that penalizes wide and/or poorly calibrated confidence intervals. A lower CWI is better. Wide intervals and low coverage increase the penalty. Ideal confidence intervals are both narrow and well-calibrated, resulting in a low CWI . The CWI is computed using the following formula, where alpha is the significance level. CWI <- AW * exp((1 - alpha) - POC / 100)^2;","title":"CWI \u2013 Confidence Width Indicator"},{"location":"model-selection/","text":"Model Selection Our framework uses the method of L-moment ratios to choose a suitable probability model for frequency analysis. This technique involves comparing the L-moments of the data with the known L-moments of various probability distributions. An Introduction to L-Moments Definition : The \\(k\\) -th Order Statistic of a statistical sample is its \\(k\\) -th smallest value. Definition : The \\(r\\) -th Population L-moment \\(\\lambda_{r}\\) is a linear combination of the expectation of the order statistics. Let \\(X_{k:n}\\) be the \\(k\\) -th order statistic from a sample of size \\(n\\) . Then, \\[ \\lambda_{r} = \\frac{1}{r} \\sum_{k=0}^{r-1} (-1)^{k} \\binom{r-1}{k} \\mathbb{E}[X_{r-k:r}] \\] Definition : A Probability Weighted Moment (PWM) encodes information about a value's position on the cumulative distribution function. The \\(r\\) -th PWM, denoted \\(\\beta_{r}\\) , is: \\[ \\beta_{r} = \\mathbb{E}[X \\cdot F(X)^{r}] \\] For an ordered sample \\(x_{1:n} \\leq \\dots \\leq x_{n:n}\\) , the sample PWM is often estimated as: \\[ b_{r} = \\frac{1}{n} \\sum_{i=1}^{r} x_{i:n} \\left(\\frac{i-1}{n-1}\\right) ^{r} \\] Remark : The first four sample L-moments can be computed as linear combinations of the PWMs: \\[ \\begin{aligned} l_{1} &= b_{0} \\\\ l_{2} &= 2b_{1} - b_{0} \\\\ l_{3} &= 6b_{2} - 6b_{1} + b_{0} \\\\ l_{4} &= 20b_{3} - 30b_{2} + 12b_{1} - b_{0} \\end{aligned} \\] The L-moments are used to compute the Sample L-variance \\(t_{2}\\) , Sample L-skewness \\(t_{3}\\) and the Sample L-kurtosis \\(t_{4}\\) using the following formulas: \\[ \\begin{aligned} t_{2} &= l_{2} / l_{1} \\\\ t_{3} &= l_{3} / l_{2} \\\\ t_{4} &= l_{4} / l_{2} \\end{aligned} \\] Then, we compare these statistics, specifically the L-skewness and L-kurtosis to their theoretical values (given here ) using one of three different metrics to select a distribution. Note : Probability distributions with less than three parameters have constant L-skewness \\(\\tau_{3}\\) and L-kurtosis \\(\\tau_{4}\\) regardless of their parameters. The L-skewness and L-kurtosis of probability distributions with three parameters is a function of the shape parameter \\(\\kappa\\) . The notation \\(\\tau_{3}(\\kappa)\\) and \\(\\tau_{4}(\\kappa)\\) refers to the L-skewness and L-kurtosis curves for the three parameter distributions. Example Plot Shown below are the L-moment curves of the GEV , GLO , GNO , PE3 / LP3 , and WEI distributions. The L-moment ratios of the two parameter distributions GUM and NOR / LNO . This example uses the \"L-distance\" selection metric. The zoomed-in region shows that the GEV distribution is most similar to the sample L-moments. Selection Metrics L-Distance Compare the euclidean distance between the sample L-skewness and sample L-kurtosis \\((t_{3}, t_{4})\\) and the known L-moment ratios \\((\\tau_{3}, \\tau_{4})\\) for each candidate distribution. For probability distributions with three parameters, we use the minimum distance between the L-moment ratio curve \\((\\tau _{3}(\\kappa ), \\tau _{4}(\\kappa ))\\) and the L-moment ratios of the sample \\((t_{3}, t_{4})\\) . L-Kurtosis For two-parameter probability distributions, simply compare the difference between the sample L-kurtosis and the theoretical L-kurtosis using the metric \\(|\\tau_{4} - t_{4} |\\) . For three-parameter distributions, we first identify the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) . Then, compare the difference between the sample L-kurtosis and the theoretical L-kurtosis using the metric \\(|\\tau_{4}(\\kappa ^{*}) - t_{4} |\\) . Z-statistic The Z-statistic selection metric is calculated as follows (for three parameter distributions): Fit the four-parameter Kappa (K4D) distribution to the data using \\(t_{2}\\) , \\(t_{3}\\) , and \\(t_{4}\\) . Generate \\(N_{\\text{sim}}\\) bootstrap samples from the fitted K4D distribution. Calculate the sample L-kurtosis \\(t_{4}^{[i]}\\) of each synthetic dataset. Calculate the bias and standard deviation of the bootstrap distribution: \\[ B_{4} = N_{\\text{sim} }^{-1} \\sum_{i = 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right) \\] \\[ \\sigma _{4} = \\left[(N_{\\text{sim} } - 1)^{-1} \\left\\{\\sum_{i - 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right)^2 - N_{\\text{sim} } B_{4}^2\\right\\} \\right] ^{\\frac{1}{2}} \\] Identify the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) . Use bootstrap distribution to compute the Z-statistic for each distribution: \\[ z = \\frac{\\tau_{4} (\\kappa ^{*}) - t_{4} + B_{4} }{ \\sigma _{4}} \\] Choose the distribution with the smallest Z-statistic. Handling Non-Stationarity There are three non-stationary scenarios that can be identified during EDA: 10 / 100 : Significant trend in the mean only. 01 / 010 : Significant trend in the variance only. 11 / 110 : Significant trend in both the mean and variance. To determine the best probability distribution for non-stationary data, we decompose the data into stationary and non-stationary components and then use one of the methods described above. Decomposition (Scenario 1) Use Sen's Trend Estimator to identify the slope \\(b_{1}\\) and intercept \\(b_{0}\\) of the trend. Detrend the data by subtracting the linear function \\((b_{1} \\cdot \\text{Covariate})\\) from the data, where the covariate is a value between \\([0, 1]\\) derived from the index. If necessary, enforce positivity by adding a constant such that \\(\\min(\\text{data}) = 1\\) . Decomposition (Scenario 2) Use a moving-window algorithm to compute the variance of the data. Use Sen's Trend Estimator to identify the slope \\(c_{1}\\) and intercept \\(c_{0}\\) of the trend in the variance. Normalize the data to have mean \\(0\\) , then divide out the scale factor \\(g_{t}\\) . \\[ g_{t} = \\frac{(c_{1} \\cdot \\text{Covariate} ) + c_{0}}{c_{0}} \\] Add back the long-term mean \\(\\mu\\) , and then ensure positivity as in Scenario 1. Decomposition (Scenario 3) Remove the linear (additive) trend exactly as in Scenario 1. On that detrended series, compute a rolling\u2010window STD series and fit its trend. Divide the detrended data by the time-varying scale factor \\(g_{t}\\) (as in Scenario 2). Shift to preserve the series mean and ensure positivity.","title":"Model Selection"},{"location":"model-selection/#model-selection","text":"Our framework uses the method of L-moment ratios to choose a suitable probability model for frequency analysis. This technique involves comparing the L-moments of the data with the known L-moments of various probability distributions.","title":"Model Selection"},{"location":"model-selection/#an-introduction-to-l-moments","text":"Definition : The \\(k\\) -th Order Statistic of a statistical sample is its \\(k\\) -th smallest value. Definition : The \\(r\\) -th Population L-moment \\(\\lambda_{r}\\) is a linear combination of the expectation of the order statistics. Let \\(X_{k:n}\\) be the \\(k\\) -th order statistic from a sample of size \\(n\\) . Then, \\[ \\lambda_{r} = \\frac{1}{r} \\sum_{k=0}^{r-1} (-1)^{k} \\binom{r-1}{k} \\mathbb{E}[X_{r-k:r}] \\] Definition : A Probability Weighted Moment (PWM) encodes information about a value's position on the cumulative distribution function. The \\(r\\) -th PWM, denoted \\(\\beta_{r}\\) , is: \\[ \\beta_{r} = \\mathbb{E}[X \\cdot F(X)^{r}] \\] For an ordered sample \\(x_{1:n} \\leq \\dots \\leq x_{n:n}\\) , the sample PWM is often estimated as: \\[ b_{r} = \\frac{1}{n} \\sum_{i=1}^{r} x_{i:n} \\left(\\frac{i-1}{n-1}\\right) ^{r} \\] Remark : The first four sample L-moments can be computed as linear combinations of the PWMs: \\[ \\begin{aligned} l_{1} &= b_{0} \\\\ l_{2} &= 2b_{1} - b_{0} \\\\ l_{3} &= 6b_{2} - 6b_{1} + b_{0} \\\\ l_{4} &= 20b_{3} - 30b_{2} + 12b_{1} - b_{0} \\end{aligned} \\] The L-moments are used to compute the Sample L-variance \\(t_{2}\\) , Sample L-skewness \\(t_{3}\\) and the Sample L-kurtosis \\(t_{4}\\) using the following formulas: \\[ \\begin{aligned} t_{2} &= l_{2} / l_{1} \\\\ t_{3} &= l_{3} / l_{2} \\\\ t_{4} &= l_{4} / l_{2} \\end{aligned} \\] Then, we compare these statistics, specifically the L-skewness and L-kurtosis to their theoretical values (given here ) using one of three different metrics to select a distribution. Note : Probability distributions with less than three parameters have constant L-skewness \\(\\tau_{3}\\) and L-kurtosis \\(\\tau_{4}\\) regardless of their parameters. The L-skewness and L-kurtosis of probability distributions with three parameters is a function of the shape parameter \\(\\kappa\\) . The notation \\(\\tau_{3}(\\kappa)\\) and \\(\\tau_{4}(\\kappa)\\) refers to the L-skewness and L-kurtosis curves for the three parameter distributions.","title":"An Introduction to L-Moments"},{"location":"model-selection/#example-plot","text":"Shown below are the L-moment curves of the GEV , GLO , GNO , PE3 / LP3 , and WEI distributions. The L-moment ratios of the two parameter distributions GUM and NOR / LNO . This example uses the \"L-distance\" selection metric. The zoomed-in region shows that the GEV distribution is most similar to the sample L-moments.","title":"Example Plot"},{"location":"model-selection/#selection-metrics","text":"","title":"Selection Metrics"},{"location":"model-selection/#l-distance","text":"Compare the euclidean distance between the sample L-skewness and sample L-kurtosis \\((t_{3}, t_{4})\\) and the known L-moment ratios \\((\\tau_{3}, \\tau_{4})\\) for each candidate distribution. For probability distributions with three parameters, we use the minimum distance between the L-moment ratio curve \\((\\tau _{3}(\\kappa ), \\tau _{4}(\\kappa ))\\) and the L-moment ratios of the sample \\((t_{3}, t_{4})\\) .","title":"L-Distance"},{"location":"model-selection/#l-kurtosis","text":"For two-parameter probability distributions, simply compare the difference between the sample L-kurtosis and the theoretical L-kurtosis using the metric \\(|\\tau_{4} - t_{4} |\\) . For three-parameter distributions, we first identify the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) . Then, compare the difference between the sample L-kurtosis and the theoretical L-kurtosis using the metric \\(|\\tau_{4}(\\kappa ^{*}) - t_{4} |\\) .","title":"L-Kurtosis"},{"location":"model-selection/#z-statistic","text":"The Z-statistic selection metric is calculated as follows (for three parameter distributions): Fit the four-parameter Kappa (K4D) distribution to the data using \\(t_{2}\\) , \\(t_{3}\\) , and \\(t_{4}\\) . Generate \\(N_{\\text{sim}}\\) bootstrap samples from the fitted K4D distribution. Calculate the sample L-kurtosis \\(t_{4}^{[i]}\\) of each synthetic dataset. Calculate the bias and standard deviation of the bootstrap distribution: \\[ B_{4} = N_{\\text{sim} }^{-1} \\sum_{i = 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right) \\] \\[ \\sigma _{4} = \\left[(N_{\\text{sim} } - 1)^{-1} \\left\\{\\sum_{i - 1}^{N_{\\text{sim} }} \\left(t_{4}^{[i]} - t_{4}^{s}\\right)^2 - N_{\\text{sim} } B_{4}^2\\right\\} \\right] ^{\\frac{1}{2}} \\] Identify the shape parameter \\(\\kappa^{*}\\) such that \\(t_{3} = \\tau _{3}(\\kappa ^{*})\\) . Use bootstrap distribution to compute the Z-statistic for each distribution: \\[ z = \\frac{\\tau_{4} (\\kappa ^{*}) - t_{4} + B_{4} }{ \\sigma _{4}} \\] Choose the distribution with the smallest Z-statistic.","title":"Z-statistic"},{"location":"model-selection/#handling-non-stationarity","text":"There are three non-stationary scenarios that can be identified during EDA: 10 / 100 : Significant trend in the mean only. 01 / 010 : Significant trend in the variance only. 11 / 110 : Significant trend in both the mean and variance. To determine the best probability distribution for non-stationary data, we decompose the data into stationary and non-stationary components and then use one of the methods described above.","title":"Handling Non-Stationarity"},{"location":"model-selection/#decomposition-scenario-1","text":"Use Sen's Trend Estimator to identify the slope \\(b_{1}\\) and intercept \\(b_{0}\\) of the trend. Detrend the data by subtracting the linear function \\((b_{1} \\cdot \\text{Covariate})\\) from the data, where the covariate is a value between \\([0, 1]\\) derived from the index. If necessary, enforce positivity by adding a constant such that \\(\\min(\\text{data}) = 1\\) .","title":"Decomposition (Scenario 1)"},{"location":"model-selection/#decomposition-scenario-2","text":"Use a moving-window algorithm to compute the variance of the data. Use Sen's Trend Estimator to identify the slope \\(c_{1}\\) and intercept \\(c_{0}\\) of the trend in the variance. Normalize the data to have mean \\(0\\) , then divide out the scale factor \\(g_{t}\\) . \\[ g_{t} = \\frac{(c_{1} \\cdot \\text{Covariate} ) + c_{0}}{c_{0}} \\] Add back the long-term mean \\(\\mu\\) , and then ensure positivity as in Scenario 1.","title":"Decomposition (Scenario 2)"},{"location":"model-selection/#decomposition-scenario-3","text":"Remove the linear (additive) trend exactly as in Scenario 1. On that detrended series, compute a rolling\u2010window STD series and fit its trend. Divide the detrended data by the time-varying scale factor \\(g_{t}\\) (as in Scenario 2). Shift to preserve the series mean and ensure positivity.","title":"Decomposition (Scenario 3)"},{"location":"parameter-estimation/","text":"Parameter Estimation The FFA framework implements three methods for parameter estimation: L-moments Maximum likelihood (MLE) Generalized maximum likelihood (GMLE) L-Moments The method estimates parameter values based on the sample L-moments \\(l_{1}\\) , \\(l_{2}\\) and the sample L-moment ratios \\(t_{3}\\) , \\(t_{4}\\) . For more information about L-moments, see here . For a information about distribution-specific parameter estimation with L-moments, see here . The parameter estimation methods used in the framework are based on these Fortran routines by J.R.M. Hosking. Warning : L-moments parameter estimation can yield distributions which do not have support at small values. This is generally not an issue, since we are only interested in the higher quantiles of the distribution. However, it is important to remember that the probability distributions produced by L-moments should not be used to predict future streamflow maxima in general. Maximum Likelihood (MLE) Maximum likelihood estimation aims to maximize the log-likelihood function \\(\\ell(x : \\theta)\\) of the data \\(x = x_{1}, \\dots , x_{n}\\) given the parameters \\(\\theta\\) . The log-likelihood functions for each distribution are defined here . To find the optimal parameters \\(\\hat{\\theta}\\) , we use the nlminb function from the stats library. This function implements the \"L-BFGS-B\" algorithm for constrained optimization. Generalized Maximum Likelihood (GMLE) The generalized maximum likelihood (GMLE) parameter estimation method is used to determine the parameters of the generalized extreme value (GEV) distribution given a prior distribution for the shape parameter \\(\\kappa\\) . This method uses maximum a posteriori estimation , which maximizes the product of the likelihood and the prior distribution. Suppose that \\(\\kappa\\) is drawn from a random variable \\(K \\sim \\text{Beta}(p, q)\\) where \\(p\\) and \\(q\\) are determined using prior knowledge. The prior PDF \\(f_{K}(\\kappa)\\) is shown below, where \\(B(p, q)\\) is the Beta function . \\[ f_{K}(\\kappa) = \\frac{\\kappa ^{p - 1}(1 - \\kappa)^{q-1}}{B(p, q)} \\] As in the case of regular maximum likelihood estimation, the likelihood function is: \\[ f_{X}(x : \\mu, \\sigma, \\kappa) =\\prod_{i=1}^{n} \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] As mentioned previously, we want to maximize the product \\(\\mathcal{L} = f_{K}(\\kappa)f_{X}(x:\\mu ,\\sigma ,\\kappa)\\) . To ensure numerical stability, we will maximize \\(\\ln (\\mathcal{L})\\) instead, which has the following form: \\[ \\begin{aligned} \\ln(\\mathcal{L}) &= \\ln(f_{K}(\\kappa)) + \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) \\\\[10pt] \\ln(f_{K}(\\kappa)) &= (p - 1)\\ln \\kappa + (q-1) \\ln (1 - \\kappa) - \\ln (B(p, q)) \\\\[5pt] \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) &= \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\end{aligned} \\]","title":"Parameter Estimation"},{"location":"parameter-estimation/#parameter-estimation","text":"The FFA framework implements three methods for parameter estimation: L-moments Maximum likelihood (MLE) Generalized maximum likelihood (GMLE)","title":"Parameter Estimation"},{"location":"parameter-estimation/#l-moments","text":"The method estimates parameter values based on the sample L-moments \\(l_{1}\\) , \\(l_{2}\\) and the sample L-moment ratios \\(t_{3}\\) , \\(t_{4}\\) . For more information about L-moments, see here . For a information about distribution-specific parameter estimation with L-moments, see here . The parameter estimation methods used in the framework are based on these Fortran routines by J.R.M. Hosking. Warning : L-moments parameter estimation can yield distributions which do not have support at small values. This is generally not an issue, since we are only interested in the higher quantiles of the distribution. However, it is important to remember that the probability distributions produced by L-moments should not be used to predict future streamflow maxima in general.","title":"L-Moments"},{"location":"parameter-estimation/#maximum-likelihood-mle","text":"Maximum likelihood estimation aims to maximize the log-likelihood function \\(\\ell(x : \\theta)\\) of the data \\(x = x_{1}, \\dots , x_{n}\\) given the parameters \\(\\theta\\) . The log-likelihood functions for each distribution are defined here . To find the optimal parameters \\(\\hat{\\theta}\\) , we use the nlminb function from the stats library. This function implements the \"L-BFGS-B\" algorithm for constrained optimization.","title":"Maximum Likelihood (MLE)"},{"location":"parameter-estimation/#generalized-maximum-likelihood-gmle","text":"The generalized maximum likelihood (GMLE) parameter estimation method is used to determine the parameters of the generalized extreme value (GEV) distribution given a prior distribution for the shape parameter \\(\\kappa\\) . This method uses maximum a posteriori estimation , which maximizes the product of the likelihood and the prior distribution. Suppose that \\(\\kappa\\) is drawn from a random variable \\(K \\sim \\text{Beta}(p, q)\\) where \\(p\\) and \\(q\\) are determined using prior knowledge. The prior PDF \\(f_{K}(\\kappa)\\) is shown below, where \\(B(p, q)\\) is the Beta function . \\[ f_{K}(\\kappa) = \\frac{\\kappa ^{p - 1}(1 - \\kappa)^{q-1}}{B(p, q)} \\] As in the case of regular maximum likelihood estimation, the likelihood function is: \\[ f_{X}(x : \\mu, \\sigma, \\kappa) =\\prod_{i=1}^{n} \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] As mentioned previously, we want to maximize the product \\(\\mathcal{L} = f_{K}(\\kappa)f_{X}(x:\\mu ,\\sigma ,\\kappa)\\) . To ensure numerical stability, we will maximize \\(\\ln (\\mathcal{L})\\) instead, which has the following form: \\[ \\begin{aligned} \\ln(\\mathcal{L}) &= \\ln(f_{K}(\\kappa)) + \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) \\\\[10pt] \\ln(f_{K}(\\kappa)) &= (p - 1)\\ln \\kappa + (q-1) \\ln (1 - \\kappa) - \\ln (B(p, q)) \\\\[5pt] \\ln(f_{X}(x:\\mu ,\\sigma ,\\kappa )) &= \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\end{aligned} \\]","title":"Generalized Maximum Likelihood (GMLE)"},{"location":"probability-distributions/","text":"Probability Distributions The FFA framework uses the \\(9\\) probability distributions listed below: Distribution Abbreviation Parameters Gumbel GUM \\(\\mu\\) (location), \\(\\sigma\\) (scale) Normal NOR \\(\\mu\\) (location), \\(\\sigma\\) (scale) Log-Normal LNO \\(\\mu\\) (location), \\(\\sigma\\) (scale) Generalized Extreme Value GEV \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Generalized Logistic Value GLO \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Generalized Normal GNO \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Pearson Type III PE3 \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Log-Pearson Type III LP3 \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Weibull WEI \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Each distribution also has two non-stationary variants which we denote by adding a \"signature\" to the model abbreviation. For two-parameter distributions ( GUM , NOR , LNO ): XXX is the standard, stationary model. XXX10 has a trend in the location parameter \\(\\mu\\) . XXX01 has a trend in the scale parameter \\(\\sigma\\) . XXX11 has a trend in the location \\(\\mu\\) and the scale \\(\\sigma\\) . Shown below is a table summarizing these three models: Feature XXX XXX10 XXX01 XXX11 Location \\(\\mu\\) \\(\\mu\\) (constant) \\(\\mu_0 + \\mu_1z\\) \\(\\mu\\) (constant) \\(\\mu_0 + \\mu_1z\\) Scale \\(\\sigma\\) \\(\\sigma\\) (constant) \\(\\sigma\\) (constant) \\(\\sigma_0 + \\sigma_1z\\) \\(\\sigma_0 + \\sigma_1z\\) Number of Parameters 2 3 3 4 For three-parameter distributions ( GEV , GLO , GNO , PE3 , LP3 , WEI ): XXX is the standard, stationary model. XXX100 has a trend in the location parameter \\(\\mu\\) . XXX010 has a trend in the scale parameter \\(\\sigma\\) . XXX110 has a trend in the location \\(\\mu\\) and the scale \\(\\sigma\\) . Shown below is a table summarizing these three models: Feature XXX XXX100 XXX010 XXX110 Location \\(\\mu\\) \\(\\mu\\) (constant) \\(\\mu_0 + \\mu_1z\\) \\(\\mu\\) (constant) \\(\\mu_0 + \\mu_1z\\) Scale \\(\\sigma\\) \\(\\sigma\\) (constant) \\(\\sigma\\) (constant) \\(\\sigma_0 + \\sigma_1z\\) \\(\\sigma_0 + \\sigma_1z\\) Shape \\(\\kappa\\) \\(\\kappa\\) (constant) \\(\\kappa\\) (constant) \\(\\kappa\\) (constant) \\(\\kappa\\) (constant) Number of Parameters 3 4 4 5 The FFA framework also uses the four-parameter Kappa distribution (KAP) for the Z-statistic selection metric. The Kappa distribution generalizes the \\(9\\) distributions listed above. List of Distributions 1 Gumbel (GUM) Distribution Support \\(-\\infty < x < \\infty\\) Quantiles \\(x(F) = \\mu - \\sigma \\log (-\\log F)\\) Likelihood Function The probability density function (PDF) of the Gumbel distribution is given below: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma} \\exp \\left(-z_{i} - e^{-z_{i}}\\right) , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - z_{i} - e^{-z_{i}} \\right] \\] L-Moments In the equations below, \\(\\gamma \\approx 0.5772\\) is Euler's constant . \\(\\lambda_{1} = \\mu + \\sigma \\gamma\\) \\(\\lambda_{2} = \\sigma \\log 2\\) \\(\\tau_{3} = \\log(9/8)/\\log 2 \\approx 0.1699\\) \\(\\tau_{4} = (16 \\log 2 - 10\\log 3) / \\log 2 \\approx 0.1504\\) We can also express the parameters in terms of the L-moments: \\(\\sigma = \\lambda_{2} / \\log 2\\) \\(\\mu = \\lambda_{1} - \\sigma \\gamma\\) Normal (NOR) Distribution Support \\(-\\infty < x < \\infty\\) Quantiles \\(x(F) = \\mu + \\sigma \\Phi^{-1}(F)\\) Likelihood Function The probability density function (PDF) of the Normal distribution is given below: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi }}e^{-z_{i}^2/2} , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln (\\sigma \\sqrt{2\\pi }) - \\frac{z_{i}^2}{2} \\right] \\] L-Moments \\(\\lambda_{1} = \\mu\\) \\(\\lambda_{2} = \\pi^{-1/2}\\sigma \\approx 0.5642\\sigma\\) \\(\\tau_{3} = 0\\) \\(\\tau_{4} = 30\\pi^{-1}\\arctan \\sqrt{2} - 9 \\approx 0.1226\\) We can also express the parameters in terms of the L-moments: \\(\\mu = \\lambda_{1}\\) \\(\\sigma = \\pi^{1/2}\\lambda_{2}\\) Log-Normal (LNO) Distribution Support \\(0 < x < \\infty\\) Quantiles \\(x(F) = \\exp(\\mu + \\sigma \\Phi^{-1}(F))\\) Likelihood Function To derive the likleihood of the LNO distribution, we use the fact that: \\[ \\text{Data} \\sim \\text{LNO} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{NOR} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LNO}}(x ; \\mu, \\sigma) = \\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma)}{x} \\] L-Moments See Normal Distribution . Generalized Extreme Value (GEV) Distribution Support \\[ \\begin{cases} \\mu + (\\sigma /\\kappa) \\leq x < \\infty & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] -\\infty < x \\leq \\mu + (\\sigma/\\kappa ) &\\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{dcases} \\mu + \\sigma (1 - (-\\log F)^{\\kappa })/\\kappa &\\kappa \\neq 0\\\\[5pt] \\mu - \\sigma \\log (-\\log F) &\\kappa = 0 \\end{dcases} \\] Likelihood Function The probability density function (PDF) of the GEV distribution is given below (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\] L-Moments The L-moments are defined for \\(\\kappa > -1\\) : \\(\\lambda_{1} = \\mu + \\sigma (1 - \\Gamma (1 + \\kappa)) / \\kappa\\) \\(\\lambda_{2} = \\sigma (1 - 2^{-\\kappa })\\Gamma (1 + \\kappa) / \\kappa\\) \\(\\tau_{3} = 2(1 - 3^{-\\kappa})/(1 - 2^{-\\kappa}) - 3\\) \\(\\tau_{4} = [5(1 - 4^{-\\kappa })-10(1-3^{-\\kappa}) + 6(1-2^{-\\kappa })]/(1 - 2^{-\\kappa })\\) To compute the parameters from the L-moments, we first compute \\(c\\) : \\[ c = \\frac{2}{3 + \\tau_{3}} - \\frac{\\log 2}{\\log 3} \\] Then, we use the following approximation 2 : \\[ \\begin{cases} \\kappa \\approx 7.8590c + 2.9554c^2 \\\\[5pt] \\sigma \\approx \\lambda_{2}\\kappa / (1 - 2^{-\\kappa })\\Gamma (1 + \\kappa) \\\\[5pt] \\mu \\approx \\lambda_{1} - \\sigma (1 - \\Gamma (1 + \\kappa )) / \\kappa \\end{cases} \\] Note : Other sources often use a different parameterization for the GEV distribution in which the sign of the shape parameter \\(\\kappa\\) is flipped. w Generalized Logistic (GLO) Distribution Support \\[ \\begin{cases} -\\infty < x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] \\mu + (\\sigma /\\kappa ) \\leq x < \\infty & \\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu +\\sigma [1 - ((1 - F) / F)^{\\kappa}] / \\kappa &\\kappa \\neq 0 \\\\[5pt] \\mu - \\sigma \\log ((1 - F) / F) & k = 0 \\end{cases} \\] Likelihood Function The probability density function (PDF) of the GLO distribution is given below (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{1}{\\sigma }t_{i}^{(1/\\kappa) - 1} \\left[1 + t_{i}^{1/\\kappa}\\right]^{-2}, \\quad t_{i} = 1 - \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma + \\left(\\frac{1}{\\kappa }-1\\right) \\ln t_{i} - 2 \\ln \\left(1 + t_{i}^{1/\\kappa }\\right) \\right] \\] L-Moments The L-moments are defined for \\(-1 < \\kappa < 1\\) : \\(\\lambda_{1} = \\mu +\\sigma [(1 / \\kappa) - (\\pi / \\sin (\\kappa\\pi))]\\) \\(\\lambda_{2} = \\sigma \\kappa \\pi / \\sin (\\kappa \\pi)\\) \\(\\tau_{3} = -\\kappa\\) \\(\\tau_{4} = (1 + 5\\kappa ^2) / 6\\) We can also express the parameters in terms of the L-moments: \\(\\kappa = -\\tau_{3}\\) \\(\\sigma = \\lambda_{2}\\sin (\\kappa \\pi ) / \\kappa \\pi\\) \\(\\mu = \\lambda_{1} - \\sigma [(1 / \\kappa) - (\\pi / \\sin (\\kappa\\pi))]\\) Generalized Normal (GNO) Distribution Support \\[ \\begin{cases} -\\infty < x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] \\mu + (\\sigma /\\kappa ) \\leq x < \\infty & \\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu + \\sigma [1 - \\exp(-\\kappa \\Phi^{-1}(F))] / \\kappa &\\kappa \\neq 0 \\\\[5pt] \\mu + \\sigma \\Phi^{-1}(F) &\\kappa = 0 \\end{cases} \\] Likelihood Function L-Moments The L-moments are defined for all values of \\(\\kappa\\) . \\(\\lambda_{1} = \\mu + \\sigma (1 - e^{\\kappa ^2/2}) / \\kappa\\) \\(\\lambda_{2} = \\sigma e^{-\\kappa ^2/ 2}[1 - 2\\Phi (-\\kappa / \\sqrt{2})] / \\kappa\\) To compute \\(\\tau_{3}\\) and \\(\\tau_{4}\\) we use the following approximation: \\[ \\begin{aligned} \\tau_{3} &\\approx -\\kappa \\left(\\frac{A_{0} + A_{1}\\kappa ^2 + A_{2}\\kappa ^{4} + A_{3}\\kappa ^{6}}{1 + B_{1}\\kappa ^2 + B_{2}\\kappa ^{4} + B_{3}\\kappa ^{6}}\\right) \\\\[5pt] \\tau_{4} &\\approx \\tau_{4}^{0} + \\kappa ^2 \\left(\\frac{C_{0} + C_{1}\\kappa ^2 + C_{2}\\kappa ^{4} + C_{3}\\kappa ^{6}}{1 + D_{1}\\kappa ^2 + D_{2}\\kappa ^{4} + D_{3}\\kappa ^{6}}\\right) \\end{aligned} \\] To determine the parameters from the L-moments we also use a rational approximation: \\[ \\kappa \\approx -\\tau_{3} \\left(\\frac{E_{0} + E_{1}\\tau_{3}^2 + E_{2}\\tau_{3}^{4} + E_{3}\\tau _{3}^{6}}{1 + F_{1}\\tau _{3}^2 + F_{2}\\tau _{3}^{4} + F_{3}\\tau _{3}^{6}}\\right) \\] Then, we can find \\(\\mu\\) and \\(\\sigma\\) as a function of \\(\\kappa\\) : \\[ \\sigma \\approx \\frac{\\lambda_{2}\\kappa e^{-\\kappa ^2 / 2}}{1 - 2\\Phi (-\\kappa / \\sqrt{2})}, \\quad \\mu \\approx \\lambda_{1} - \\frac{\\sigma }{\\kappa }\\left(1 - e^{-\\kappa ^2 / 2 }\\right) \\] The coefficients ( \\(A_{i}\\) , \\(B_{i}\\) , \\(C_{i}\\) , \\(D_{i}\\) , \\(E_{i}\\) , \\(F_{i}\\) , and \\(\\tau_{4}^{0}\\) ) are defined in Appendix A.8 of Hosking, 1997 1 . Although this appendix covers the 3-parameter log-normal distribution, the L-moments of the generalized normal distribution are the same. Pearson Type III (PE3) Distribution The Pearson Type III distribution is typically reparameterized as follows for \\(\\kappa \\neq 0\\) : \\[ \\begin{aligned} \\alpha &= 4 / \\kappa^2 \\\\[5pt] \\beta &= \\sigma |\\kappa | / 2 \\\\[5pt] \\xi &= \\mu - 2\\sigma /\\kappa \\end{aligned} \\] Support \\[ \\begin{cases} \\xi \\leq x < \\infty &\\kappa > 0 \\\\[5pt] -\\infty < x < \\infty &\\kappa =0 \\\\[5pt] -\\infty < x \\leq \\xi &\\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu - \\alpha \\beta + q(F, \\alpha, \\beta) &\\kappa > 0\\\\[5pt] \\mu + \\sigma \\Phi^{-1}(F) &\\kappa = 0\\\\[5pt] \\mu + \\alpha \\beta - q(1 - F, \\alpha, \\beta) &\\kappa < 0 \\end{cases} \\] In the equations above, \\(q\\) is the quantile function of the Gamma distribution with with shape \\(\\alpha\\) and scale \\(\\beta\\) . \\(q\\) is defined below, where \\(\\gamma\\) is the lower incomplete Gamma function . \\[q(F, \\alpha, \\beta) = \\beta \\gamma ^{-1}(\\alpha, p \\Gamma (\\alpha))\\] Likelihood Function The probability density function (PDF) of the PE3 distribution is given below: \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{(x_{i} - \\xi)^{\\alpha - 1}e^{-(x_{i} - \\xi )/\\beta }}{\\beta ^{\\alpha } \\Gamma (\\alpha )} \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[(\\alpha - 1) \\ln |x_{i} - \\xi | - \\frac{|x_{i} - \\xi |}{\\beta } - \\alpha \\ln\\beta - \\ln \\Gamma (\\alpha )\\right] \\] L-Moments All subsequent definitions assume that \\(\\kappa > 0\\) . If \\(\\kappa < 0\\) , the L-moments can be obtained by changing the signs of \\(\\lambda_{1}\\) , \\(\\tau_{3}\\) , and \\(\\xi\\) whenever they appear. If \\(\\kappa = 0\\) , the L-moments are the same as the Normal Distribution . The first two L-moments are defined as follows: \\(\\lambda_{1} = \\xi + \\alpha \\beta\\) \\(\\lambda_{2} = \\pi ^{-1/2} \\beta \\Gamma (\\alpha + 0.5) / \\Gamma (\\alpha )\\) Rational approximation is necessary to determine \\(\\tau_{3}\\) and \\(\\tau_{4}\\) . If \\(\\alpha \\geq 1\\) : \\[ \\begin{aligned} \\tau_{3} &\\approx \\alpha^{-1/2} \\left(\\frac{A_{0} + A_{1}\\alpha^{-1} + A_{2}\\alpha^{-2} + A_{3}\\alpha^{-3}}{1 + B_{1}\\alpha^{-1} + B_{2}\\alpha ^{-2}}\\right) \\\\[5pt] \\tau_{4} &\\approx \\frac{C_{0} + C_{1}\\alpha^{-1} + C_{2}\\alpha ^{-2} +C_{3}\\alpha ^{-3}}{1 + D_{1}\\alpha ^{-1} + D_{2}\\alpha ^{-2}} \\end{aligned} \\] If \\(\\alpha < 1\\) we use a different set of coefficients: \\[ \\begin{aligned} \\tau_{3} &\\approx \\frac{1 + E_{1}\\alpha + E_{2}\\alpha ^2 + E_{3}\\alpha ^3}{1 + F_{1}\\alpha + F_{2}\\alpha ^2 + F_{3}\\alpha ^3} \\\\[5pt] \\tau_{4} &\\approx \\frac{1 + G_{1}\\alpha + G_{2}\\alpha ^2 + G_{3}\\alpha ^3}{1 + H_{1}\\alpha + H_{2}\\alpha ^2 + H_{3}\\alpha ^3} \\end{aligned} \\] Coefficients are given in Appendix A.9 of Hosking, 1997 1 . To estimate parameters from the L-moments, we use one of two approximations for \\(\\alpha\\) depending on the value of \\(\\tau_{3}\\) : \\[ \\alpha \\approx \\begin{dcases} \\frac{1 + 0.2906z}{z + 0.1882z^2 + 0.0442z^3}, &z = 3\\pi \\tau_{3}^2, &0 < |\\tau_{3}| < \\frac{1}{3} \\\\[5pt] \\frac{0.36067z - 0.59567z^2 + 0.25361z^3}{1 - 2.78861z + 2.56096z^2 - 0.77045z^3}, &z = 1 - |\\tau_{3}|, &\\frac{1}{3} \\leq |\\tau_{3}| < 1 \\end{dcases} \\] Then, we can determine the parameters from the approximated \\(\\alpha\\) : \\[ \\begin{aligned} \\kappa &= 2\\alpha ^{-1/2} \\text{sign} (\\tau_{3}) \\\\[5pt] \\sigma &= \\lambda_{2} \\pi^{1/2}\\alpha ^{1/2} \\Gamma (\\alpha )/\\Gamma (\\alpha + 0.5)\\\\[5pt] \\mu &= \\lambda_{1 } \\end{aligned} \\] Log-Pearson Type III (LP3) Distribution The LP3 distribution uses the same reparameterization as the PE3 distribution . Support \\[ \\begin{cases} \\max(0, \\xi) \\leq x < \\infty &\\kappa > 0 \\\\[5pt] 0 < x < \\infty &\\kappa =0 \\\\[5pt] 0 < x \\leq \\max(0, \\xi) &\\kappa < 0 \\end{cases} \\] Quantiles \\(x(F) = \\exp(x_{\\text{PE3}}(F ))\\) , where \\(x_{\\text{PE3}}(F)\\) is the quantile function of the PE3 distribution . Likelihood Function To derive the likelihood of the LP3 distribution, we use the fact that: \\[ \\text{Data} \\sim \\text{LP3} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{PE3} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LP3}}(x ; \\mu, \\sigma, \\kappa) = \\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa ) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa )}{x} \\] L-Moments Same as the PE3 distribution . Weibull (WEI) Distribution The Weibull distribution is a reparameterized version of the generalized extreme value distribution: \\[ \\begin{aligned} \\kappa &= 1 / \\kappa_{\\text{GEV}} \\\\[5pt] \\sigma &= \\kappa \\sigma_{\\text{GEV} } \\\\[5pt] \\mu &= \\sigma + \\mu_{\\text{GEV} } \\end{aligned} \\] Under this reparameterization, it is required that \\(\\sigma > 0\\) and \\(\\kappa > 0\\) . Support \\(\\mu \\leq x < \\infty\\) Quantiles \\(x(F) = \\mu + \\sigma (-\\log (1 - F))^{1/\\kappa}\\) Likelihood Function The probability density function (PDF) of the Weibull distribution is given below for \\(x_{i} > \\mu\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{\\kappa}{\\sigma }\\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa -1} \\exp \\left( - \\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa } \\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[\\ln \\kappa - \\kappa \\ln \\sigma +(\\kappa -1)\\ln (x_{i}-\\mu ) - \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) ^{\\kappa } \\right] \\] L-Moments First, reparameterize the Weibull distribution and to recover the GEV parameters: \\[ \\begin{aligned} \\kappa_{\\text{GEV}} &= 1 / \\kappa \\\\[5pt] \\sigma_{\\text{GEV}} &= \\sigma / \\kappa \\\\[5pt] \\end{aligned} \\] Next, compute the L-moments for the GEV distribution with \\(\\mu_{\\text{GEV}} = 0\\) . Then, \\(\\lambda_{1} = \\mu + \\sigma - \\lambda_{1, \\text{GEV}}\\) \\(\\lambda_{2} = \\lambda_{2, \\text{GEV}}\\) \\(\\tau_{3} = -\\tau_{3, \\text{GEV}}\\) \\(\\tau_{4} = \\tau_{4, \\text{GEV} }\\) To compute the parameters from the L-moments, first flip the sign of \\(\\lambda_{1}\\) and \\(\\tau_{3}\\) . Then, estimate the parameters of the GEV distribution to get \\(\\hat{\\mu}_{\\text{GEV}}\\) , \\(\\hat{\\sigma}_{\\text{GEV}}\\) , and \\(\\hat{\\kappa}_{\\text{GEV}}\\) . Finally, reparameterize the GEV parameters as shown here and then flip the sign of \\(\\mu\\) . Kappa (KAP) Distribution The Kappa distribution has location \\(\\mu\\) , scale \\(\\sigma\\) , and two shape parameters \\(\\kappa\\) and \\(h\\) . Support \\[ \\begin{cases} \\mu + \\sigma (1 - h^{-\\kappa}) \\leq x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0, h > 0 \\\\[5pt] -\\infty < x \\leq \\mu + (\\sigma /\\kappa) & \\kappa > 0, h \\leq 0 \\\\[5pt] \\mu + \\sigma (1 - h^{-\\kappa}) \\leq x < \\infty &\\kappa \\leq 0, h > 0 \\\\[5pt] \\mu + (\\sigma / \\kappa ) \\leq x <\\infty &\\kappa \\leq 0, h \\leq 0 \\end{cases} \\] Quantiles \\[ x(F) = \\mu + \\frac{\\sigma }{\\kappa }\\left[1 - \\left(\\frac{1 - F^{h}}{h}\\right)^{\\kappa }\\right] \\] L-Moments The L-moments are defined if \\(h \\geq 0\\) and \\(k > -1\\) or if \\(h < 0\\) and \\(-1 < k < -1/h\\) . \\(\\lambda_{1} = \\mu + \\sigma (1 - g_{1})/\\kappa\\) \\(\\lambda_{2} = \\sigma(g_{1} - g_{2})/\\kappa\\) \\(\\tau_{3} = (-g_{1} + 3g_{2} - 2g_{3}) / (g_{1} - g_{2})\\) \\(\\tau_{4} = (-g_{1} + 6g_{2} - 10g_{3} + 5g_{4}) / (g_{1} - g_{2})\\) In the expression above, \\(g_{r}\\) is defined as follows: \\[ g_{r} = \\begin{dcases} \\frac{r\\Gamma (1 + \\kappa )\\Gamma (r / h)}{h^{1 + \\kappa }\\Gamma (1 + \\kappa + r/h)} &h > 0 \\\\[5pt] \\frac{r\\Gamma (1 + \\kappa ) \\Gamma (-\\kappa - r/h)}{(-h)^{1 + \\kappa }\\Gamma (1 - r/h)} &h < 0 \\end{dcases} \\] There is no closed-form solution for the parameters in terms of the L-moments. However, \\(\\tau_{3}\\) and \\(\\tau_{4}\\) can be computed in terms of \\(\\kappa\\) and \\(h\\) using Newton-Raphson iteration. Sources Hosking, J.R.M. & Wallis, J.R., 1997. Regional frequency analysis: an aproach based on L-Moments. Cambridge University Press, New York, USA. \u21a9 \u21a9 \u21a9 Hosking, J.R.M., Wallis, J.R., & Wood, E.F., 1985. Estimation of the generalized extreme-value distribution by the method of probability-weighted moments. Technometrics, 27, 251-61. \u21a9","title":"Probability Distributions"},{"location":"probability-distributions/#probability-distributions","text":"The FFA framework uses the \\(9\\) probability distributions listed below: Distribution Abbreviation Parameters Gumbel GUM \\(\\mu\\) (location), \\(\\sigma\\) (scale) Normal NOR \\(\\mu\\) (location), \\(\\sigma\\) (scale) Log-Normal LNO \\(\\mu\\) (location), \\(\\sigma\\) (scale) Generalized Extreme Value GEV \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Generalized Logistic Value GLO \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Generalized Normal GNO \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Pearson Type III PE3 \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Log-Pearson Type III LP3 \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Weibull WEI \\(\\mu\\) (location), \\(\\sigma\\) (scale), \\(\\kappa\\) (shape) Each distribution also has two non-stationary variants which we denote by adding a \"signature\" to the model abbreviation. For two-parameter distributions ( GUM , NOR , LNO ): XXX is the standard, stationary model. XXX10 has a trend in the location parameter \\(\\mu\\) . XXX01 has a trend in the scale parameter \\(\\sigma\\) . XXX11 has a trend in the location \\(\\mu\\) and the scale \\(\\sigma\\) . Shown below is a table summarizing these three models: Feature XXX XXX10 XXX01 XXX11 Location \\(\\mu\\) \\(\\mu\\) (constant) \\(\\mu_0 + \\mu_1z\\) \\(\\mu\\) (constant) \\(\\mu_0 + \\mu_1z\\) Scale \\(\\sigma\\) \\(\\sigma\\) (constant) \\(\\sigma\\) (constant) \\(\\sigma_0 + \\sigma_1z\\) \\(\\sigma_0 + \\sigma_1z\\) Number of Parameters 2 3 3 4 For three-parameter distributions ( GEV , GLO , GNO , PE3 , LP3 , WEI ): XXX is the standard, stationary model. XXX100 has a trend in the location parameter \\(\\mu\\) . XXX010 has a trend in the scale parameter \\(\\sigma\\) . XXX110 has a trend in the location \\(\\mu\\) and the scale \\(\\sigma\\) . Shown below is a table summarizing these three models: Feature XXX XXX100 XXX010 XXX110 Location \\(\\mu\\) \\(\\mu\\) (constant) \\(\\mu_0 + \\mu_1z\\) \\(\\mu\\) (constant) \\(\\mu_0 + \\mu_1z\\) Scale \\(\\sigma\\) \\(\\sigma\\) (constant) \\(\\sigma\\) (constant) \\(\\sigma_0 + \\sigma_1z\\) \\(\\sigma_0 + \\sigma_1z\\) Shape \\(\\kappa\\) \\(\\kappa\\) (constant) \\(\\kappa\\) (constant) \\(\\kappa\\) (constant) \\(\\kappa\\) (constant) Number of Parameters 3 4 4 5 The FFA framework also uses the four-parameter Kappa distribution (KAP) for the Z-statistic selection metric. The Kappa distribution generalizes the \\(9\\) distributions listed above.","title":"Probability Distributions"},{"location":"probability-distributions/#list-of-distributions","text":"","title":"List of Distributions"},{"location":"probability-distributions/#gumbel-gum-distribution","text":"Support \\(-\\infty < x < \\infty\\) Quantiles \\(x(F) = \\mu - \\sigma \\log (-\\log F)\\) Likelihood Function The probability density function (PDF) of the Gumbel distribution is given below: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma} \\exp \\left(-z_{i} - e^{-z_{i}}\\right) , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - z_{i} - e^{-z_{i}} \\right] \\] L-Moments In the equations below, \\(\\gamma \\approx 0.5772\\) is Euler's constant . \\(\\lambda_{1} = \\mu + \\sigma \\gamma\\) \\(\\lambda_{2} = \\sigma \\log 2\\) \\(\\tau_{3} = \\log(9/8)/\\log 2 \\approx 0.1699\\) \\(\\tau_{4} = (16 \\log 2 - 10\\log 3) / \\log 2 \\approx 0.1504\\) We can also express the parameters in terms of the L-moments: \\(\\sigma = \\lambda_{2} / \\log 2\\) \\(\\mu = \\lambda_{1} - \\sigma \\gamma\\)","title":"Gumbel (GUM) Distribution"},{"location":"probability-distributions/#normal-nor-distribution","text":"Support \\(-\\infty < x < \\infty\\) Quantiles \\(x(F) = \\mu + \\sigma \\Phi^{-1}(F)\\) Likelihood Function The probability density function (PDF) of the Normal distribution is given below: \\[ f(x_{i} : \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi }}e^{-z_{i}^2/2} , \\quad z_{i} = \\frac{x_{i} - \\mu}{\\sigma } \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma) = \\sum_{i=1}^{n} \\left[-\\ln (\\sigma \\sqrt{2\\pi }) - \\frac{z_{i}^2}{2} \\right] \\] L-Moments \\(\\lambda_{1} = \\mu\\) \\(\\lambda_{2} = \\pi^{-1/2}\\sigma \\approx 0.5642\\sigma\\) \\(\\tau_{3} = 0\\) \\(\\tau_{4} = 30\\pi^{-1}\\arctan \\sqrt{2} - 9 \\approx 0.1226\\) We can also express the parameters in terms of the L-moments: \\(\\mu = \\lambda_{1}\\) \\(\\sigma = \\pi^{1/2}\\lambda_{2}\\)","title":"Normal (NOR) Distribution"},{"location":"probability-distributions/#log-normal-lno-distribution","text":"Support \\(0 < x < \\infty\\) Quantiles \\(x(F) = \\exp(\\mu + \\sigma \\Phi^{-1}(F))\\) Likelihood Function To derive the likleihood of the LNO distribution, we use the fact that: \\[ \\text{Data} \\sim \\text{LNO} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{NOR} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LNO}}(x ; \\mu, \\sigma) = \\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{NOR}}(\\ln x ; \\mu , \\sigma)}{x} \\] L-Moments See Normal Distribution .","title":"Log-Normal (LNO) Distribution"},{"location":"probability-distributions/#generalized-extreme-value-gev-distribution","text":"Support \\[ \\begin{cases} \\mu + (\\sigma /\\kappa) \\leq x < \\infty & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] -\\infty < x \\leq \\mu + (\\sigma/\\kappa ) &\\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{dcases} \\mu + \\sigma (1 - (-\\log F)^{\\kappa })/\\kappa &\\kappa \\neq 0\\\\[5pt] \\mu - \\sigma \\log (-\\log F) &\\kappa = 0 \\end{dcases} \\] Likelihood Function The probability density function (PDF) of the GEV distribution is given below (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{1}{\\sigma}t_{i}^{-1 - (1/\\kappa)} \\exp (-t_{i}^{-1/\\kappa}), \\quad t_{i} = 1 + \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma } \\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma - \\left(1 + \\frac{1}{\\kappa }\\right) \\ln t_{i} - t_{i}^{-1/\\kappa}\\right] \\] L-Moments The L-moments are defined for \\(\\kappa > -1\\) : \\(\\lambda_{1} = \\mu + \\sigma (1 - \\Gamma (1 + \\kappa)) / \\kappa\\) \\(\\lambda_{2} = \\sigma (1 - 2^{-\\kappa })\\Gamma (1 + \\kappa) / \\kappa\\) \\(\\tau_{3} = 2(1 - 3^{-\\kappa})/(1 - 2^{-\\kappa}) - 3\\) \\(\\tau_{4} = [5(1 - 4^{-\\kappa })-10(1-3^{-\\kappa}) + 6(1-2^{-\\kappa })]/(1 - 2^{-\\kappa })\\) To compute the parameters from the L-moments, we first compute \\(c\\) : \\[ c = \\frac{2}{3 + \\tau_{3}} - \\frac{\\log 2}{\\log 3} \\] Then, we use the following approximation 2 : \\[ \\begin{cases} \\kappa \\approx 7.8590c + 2.9554c^2 \\\\[5pt] \\sigma \\approx \\lambda_{2}\\kappa / (1 - 2^{-\\kappa })\\Gamma (1 + \\kappa) \\\\[5pt] \\mu \\approx \\lambda_{1} - \\sigma (1 - \\Gamma (1 + \\kappa )) / \\kappa \\end{cases} \\] Note : Other sources often use a different parameterization for the GEV distribution in which the sign of the shape parameter \\(\\kappa\\) is flipped. w","title":"Generalized Extreme Value (GEV) Distribution"},{"location":"probability-distributions/#generalized-logistic-glo-distribution","text":"Support \\[ \\begin{cases} -\\infty < x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] \\mu + (\\sigma /\\kappa ) \\leq x < \\infty & \\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu +\\sigma [1 - ((1 - F) / F)^{\\kappa}] / \\kappa &\\kappa \\neq 0 \\\\[5pt] \\mu - \\sigma \\log ((1 - F) / F) & k = 0 \\end{cases} \\] Likelihood Function The probability density function (PDF) of the GLO distribution is given below (assume \\(t_{i} > 0)\\) : \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{1}{\\sigma }t_{i}^{(1/\\kappa) - 1} \\left[1 + t_{i}^{1/\\kappa}\\right]^{-2}, \\quad t_{i} = 1 - \\kappa \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[-\\ln \\sigma + \\left(\\frac{1}{\\kappa }-1\\right) \\ln t_{i} - 2 \\ln \\left(1 + t_{i}^{1/\\kappa }\\right) \\right] \\] L-Moments The L-moments are defined for \\(-1 < \\kappa < 1\\) : \\(\\lambda_{1} = \\mu +\\sigma [(1 / \\kappa) - (\\pi / \\sin (\\kappa\\pi))]\\) \\(\\lambda_{2} = \\sigma \\kappa \\pi / \\sin (\\kappa \\pi)\\) \\(\\tau_{3} = -\\kappa\\) \\(\\tau_{4} = (1 + 5\\kappa ^2) / 6\\) We can also express the parameters in terms of the L-moments: \\(\\kappa = -\\tau_{3}\\) \\(\\sigma = \\lambda_{2}\\sin (\\kappa \\pi ) / \\kappa \\pi\\) \\(\\mu = \\lambda_{1} - \\sigma [(1 / \\kappa) - (\\pi / \\sin (\\kappa\\pi))]\\)","title":"Generalized Logistic (GLO) Distribution"},{"location":"probability-distributions/#generalized-normal-gno-distribution","text":"Support \\[ \\begin{cases} -\\infty < x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0 \\\\[5pt] -\\infty < x < \\infty & \\kappa = 0 \\\\[5pt] \\mu + (\\sigma /\\kappa ) \\leq x < \\infty & \\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu + \\sigma [1 - \\exp(-\\kappa \\Phi^{-1}(F))] / \\kappa &\\kappa \\neq 0 \\\\[5pt] \\mu + \\sigma \\Phi^{-1}(F) &\\kappa = 0 \\end{cases} \\] Likelihood Function L-Moments The L-moments are defined for all values of \\(\\kappa\\) . \\(\\lambda_{1} = \\mu + \\sigma (1 - e^{\\kappa ^2/2}) / \\kappa\\) \\(\\lambda_{2} = \\sigma e^{-\\kappa ^2/ 2}[1 - 2\\Phi (-\\kappa / \\sqrt{2})] / \\kappa\\) To compute \\(\\tau_{3}\\) and \\(\\tau_{4}\\) we use the following approximation: \\[ \\begin{aligned} \\tau_{3} &\\approx -\\kappa \\left(\\frac{A_{0} + A_{1}\\kappa ^2 + A_{2}\\kappa ^{4} + A_{3}\\kappa ^{6}}{1 + B_{1}\\kappa ^2 + B_{2}\\kappa ^{4} + B_{3}\\kappa ^{6}}\\right) \\\\[5pt] \\tau_{4} &\\approx \\tau_{4}^{0} + \\kappa ^2 \\left(\\frac{C_{0} + C_{1}\\kappa ^2 + C_{2}\\kappa ^{4} + C_{3}\\kappa ^{6}}{1 + D_{1}\\kappa ^2 + D_{2}\\kappa ^{4} + D_{3}\\kappa ^{6}}\\right) \\end{aligned} \\] To determine the parameters from the L-moments we also use a rational approximation: \\[ \\kappa \\approx -\\tau_{3} \\left(\\frac{E_{0} + E_{1}\\tau_{3}^2 + E_{2}\\tau_{3}^{4} + E_{3}\\tau _{3}^{6}}{1 + F_{1}\\tau _{3}^2 + F_{2}\\tau _{3}^{4} + F_{3}\\tau _{3}^{6}}\\right) \\] Then, we can find \\(\\mu\\) and \\(\\sigma\\) as a function of \\(\\kappa\\) : \\[ \\sigma \\approx \\frac{\\lambda_{2}\\kappa e^{-\\kappa ^2 / 2}}{1 - 2\\Phi (-\\kappa / \\sqrt{2})}, \\quad \\mu \\approx \\lambda_{1} - \\frac{\\sigma }{\\kappa }\\left(1 - e^{-\\kappa ^2 / 2 }\\right) \\] The coefficients ( \\(A_{i}\\) , \\(B_{i}\\) , \\(C_{i}\\) , \\(D_{i}\\) , \\(E_{i}\\) , \\(F_{i}\\) , and \\(\\tau_{4}^{0}\\) ) are defined in Appendix A.8 of Hosking, 1997 1 . Although this appendix covers the 3-parameter log-normal distribution, the L-moments of the generalized normal distribution are the same.","title":"Generalized Normal (GNO) Distribution"},{"location":"probability-distributions/#pearson-type-iii-pe3-distribution","text":"The Pearson Type III distribution is typically reparameterized as follows for \\(\\kappa \\neq 0\\) : \\[ \\begin{aligned} \\alpha &= 4 / \\kappa^2 \\\\[5pt] \\beta &= \\sigma |\\kappa | / 2 \\\\[5pt] \\xi &= \\mu - 2\\sigma /\\kappa \\end{aligned} \\] Support \\[ \\begin{cases} \\xi \\leq x < \\infty &\\kappa > 0 \\\\[5pt] -\\infty < x < \\infty &\\kappa =0 \\\\[5pt] -\\infty < x \\leq \\xi &\\kappa < 0 \\end{cases} \\] Quantiles \\[ x(F) = \\begin{cases} \\mu - \\alpha \\beta + q(F, \\alpha, \\beta) &\\kappa > 0\\\\[5pt] \\mu + \\sigma \\Phi^{-1}(F) &\\kappa = 0\\\\[5pt] \\mu + \\alpha \\beta - q(1 - F, \\alpha, \\beta) &\\kappa < 0 \\end{cases} \\] In the equations above, \\(q\\) is the quantile function of the Gamma distribution with with shape \\(\\alpha\\) and scale \\(\\beta\\) . \\(q\\) is defined below, where \\(\\gamma\\) is the lower incomplete Gamma function . \\[q(F, \\alpha, \\beta) = \\beta \\gamma ^{-1}(\\alpha, p \\Gamma (\\alpha))\\] Likelihood Function The probability density function (PDF) of the PE3 distribution is given below: \\[ f(x_{i} : \\mu , \\sigma , \\kappa ) = \\frac{(x_{i} - \\xi)^{\\alpha - 1}e^{-(x_{i} - \\xi )/\\beta }}{\\beta ^{\\alpha } \\Gamma (\\alpha )} \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[(\\alpha - 1) \\ln |x_{i} - \\xi | - \\frac{|x_{i} - \\xi |}{\\beta } - \\alpha \\ln\\beta - \\ln \\Gamma (\\alpha )\\right] \\] L-Moments All subsequent definitions assume that \\(\\kappa > 0\\) . If \\(\\kappa < 0\\) , the L-moments can be obtained by changing the signs of \\(\\lambda_{1}\\) , \\(\\tau_{3}\\) , and \\(\\xi\\) whenever they appear. If \\(\\kappa = 0\\) , the L-moments are the same as the Normal Distribution . The first two L-moments are defined as follows: \\(\\lambda_{1} = \\xi + \\alpha \\beta\\) \\(\\lambda_{2} = \\pi ^{-1/2} \\beta \\Gamma (\\alpha + 0.5) / \\Gamma (\\alpha )\\) Rational approximation is necessary to determine \\(\\tau_{3}\\) and \\(\\tau_{4}\\) . If \\(\\alpha \\geq 1\\) : \\[ \\begin{aligned} \\tau_{3} &\\approx \\alpha^{-1/2} \\left(\\frac{A_{0} + A_{1}\\alpha^{-1} + A_{2}\\alpha^{-2} + A_{3}\\alpha^{-3}}{1 + B_{1}\\alpha^{-1} + B_{2}\\alpha ^{-2}}\\right) \\\\[5pt] \\tau_{4} &\\approx \\frac{C_{0} + C_{1}\\alpha^{-1} + C_{2}\\alpha ^{-2} +C_{3}\\alpha ^{-3}}{1 + D_{1}\\alpha ^{-1} + D_{2}\\alpha ^{-2}} \\end{aligned} \\] If \\(\\alpha < 1\\) we use a different set of coefficients: \\[ \\begin{aligned} \\tau_{3} &\\approx \\frac{1 + E_{1}\\alpha + E_{2}\\alpha ^2 + E_{3}\\alpha ^3}{1 + F_{1}\\alpha + F_{2}\\alpha ^2 + F_{3}\\alpha ^3} \\\\[5pt] \\tau_{4} &\\approx \\frac{1 + G_{1}\\alpha + G_{2}\\alpha ^2 + G_{3}\\alpha ^3}{1 + H_{1}\\alpha + H_{2}\\alpha ^2 + H_{3}\\alpha ^3} \\end{aligned} \\] Coefficients are given in Appendix A.9 of Hosking, 1997 1 . To estimate parameters from the L-moments, we use one of two approximations for \\(\\alpha\\) depending on the value of \\(\\tau_{3}\\) : \\[ \\alpha \\approx \\begin{dcases} \\frac{1 + 0.2906z}{z + 0.1882z^2 + 0.0442z^3}, &z = 3\\pi \\tau_{3}^2, &0 < |\\tau_{3}| < \\frac{1}{3} \\\\[5pt] \\frac{0.36067z - 0.59567z^2 + 0.25361z^3}{1 - 2.78861z + 2.56096z^2 - 0.77045z^3}, &z = 1 - |\\tau_{3}|, &\\frac{1}{3} \\leq |\\tau_{3}| < 1 \\end{dcases} \\] Then, we can determine the parameters from the approximated \\(\\alpha\\) : \\[ \\begin{aligned} \\kappa &= 2\\alpha ^{-1/2} \\text{sign} (\\tau_{3}) \\\\[5pt] \\sigma &= \\lambda_{2} \\pi^{1/2}\\alpha ^{1/2} \\Gamma (\\alpha )/\\Gamma (\\alpha + 0.5)\\\\[5pt] \\mu &= \\lambda_{1 } \\end{aligned} \\]","title":"Pearson Type III (PE3) Distribution"},{"location":"probability-distributions/#log-pearson-type-iii-lp3-distribution","text":"The LP3 distribution uses the same reparameterization as the PE3 distribution . Support \\[ \\begin{cases} \\max(0, \\xi) \\leq x < \\infty &\\kappa > 0 \\\\[5pt] 0 < x < \\infty &\\kappa =0 \\\\[5pt] 0 < x \\leq \\max(0, \\xi) &\\kappa < 0 \\end{cases} \\] Quantiles \\(x(F) = \\exp(x_{\\text{PE3}}(F ))\\) , where \\(x_{\\text{PE3}}(F)\\) is the quantile function of the PE3 distribution . Likelihood Function To derive the likelihood of the LP3 distribution, we use the fact that: \\[ \\text{Data} \\sim \\text{LP3} \\Leftrightarrow \\ln (\\text{Data}) \\sim \\text{PE3} \\] Precisely, we require the change of variables formula, which states that: \\[ \\ell_{\\text{LP3}}(x ; \\mu, \\sigma, \\kappa) = \\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa ) \\left|\\frac{d}{dx} \\ln x\\right| = \\frac{\\ell_{\\text{PE3}}(\\ln x ; \\mu , \\sigma, \\kappa )}{x} \\] L-Moments Same as the PE3 distribution .","title":"Log-Pearson Type III (LP3) Distribution"},{"location":"probability-distributions/#weibull-wei-distribution","text":"The Weibull distribution is a reparameterized version of the generalized extreme value distribution: \\[ \\begin{aligned} \\kappa &= 1 / \\kappa_{\\text{GEV}} \\\\[5pt] \\sigma &= \\kappa \\sigma_{\\text{GEV} } \\\\[5pt] \\mu &= \\sigma + \\mu_{\\text{GEV} } \\end{aligned} \\] Under this reparameterization, it is required that \\(\\sigma > 0\\) and \\(\\kappa > 0\\) . Support \\(\\mu \\leq x < \\infty\\) Quantiles \\(x(F) = \\mu + \\sigma (-\\log (1 - F))^{1/\\kappa}\\) Likelihood Function The probability density function (PDF) of the Weibull distribution is given below for \\(x_{i} > \\mu\\) : \\[ f(x_{i} : \\mu, \\sigma, \\kappa) = \\frac{\\kappa}{\\sigma }\\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa -1} \\exp \\left( - \\left(\\frac{x_{i} - \\mu}{\\sigma }\\right)^{\\kappa } \\right) \\] Therefore, the Log-likelihood function is defined as follows: \\[ \\ell(x:\\mu, \\sigma, \\kappa) = \\sum_{i=1}^{n} \\left[\\ln \\kappa - \\kappa \\ln \\sigma +(\\kappa -1)\\ln (x_{i}-\\mu ) - \\left(\\frac{x_{i} - \\mu }{\\sigma }\\right) ^{\\kappa } \\right] \\] L-Moments First, reparameterize the Weibull distribution and to recover the GEV parameters: \\[ \\begin{aligned} \\kappa_{\\text{GEV}} &= 1 / \\kappa \\\\[5pt] \\sigma_{\\text{GEV}} &= \\sigma / \\kappa \\\\[5pt] \\end{aligned} \\] Next, compute the L-moments for the GEV distribution with \\(\\mu_{\\text{GEV}} = 0\\) . Then, \\(\\lambda_{1} = \\mu + \\sigma - \\lambda_{1, \\text{GEV}}\\) \\(\\lambda_{2} = \\lambda_{2, \\text{GEV}}\\) \\(\\tau_{3} = -\\tau_{3, \\text{GEV}}\\) \\(\\tau_{4} = \\tau_{4, \\text{GEV} }\\) To compute the parameters from the L-moments, first flip the sign of \\(\\lambda_{1}\\) and \\(\\tau_{3}\\) . Then, estimate the parameters of the GEV distribution to get \\(\\hat{\\mu}_{\\text{GEV}}\\) , \\(\\hat{\\sigma}_{\\text{GEV}}\\) , and \\(\\hat{\\kappa}_{\\text{GEV}}\\) . Finally, reparameterize the GEV parameters as shown here and then flip the sign of \\(\\mu\\) .","title":"Weibull (WEI) Distribution"},{"location":"probability-distributions/#kappa-kap-distribution","text":"The Kappa distribution has location \\(\\mu\\) , scale \\(\\sigma\\) , and two shape parameters \\(\\kappa\\) and \\(h\\) . Support \\[ \\begin{cases} \\mu + \\sigma (1 - h^{-\\kappa}) \\leq x \\leq \\mu + (\\sigma /\\kappa ) & \\kappa > 0, h > 0 \\\\[5pt] -\\infty < x \\leq \\mu + (\\sigma /\\kappa) & \\kappa > 0, h \\leq 0 \\\\[5pt] \\mu + \\sigma (1 - h^{-\\kappa}) \\leq x < \\infty &\\kappa \\leq 0, h > 0 \\\\[5pt] \\mu + (\\sigma / \\kappa ) \\leq x <\\infty &\\kappa \\leq 0, h \\leq 0 \\end{cases} \\] Quantiles \\[ x(F) = \\mu + \\frac{\\sigma }{\\kappa }\\left[1 - \\left(\\frac{1 - F^{h}}{h}\\right)^{\\kappa }\\right] \\] L-Moments The L-moments are defined if \\(h \\geq 0\\) and \\(k > -1\\) or if \\(h < 0\\) and \\(-1 < k < -1/h\\) . \\(\\lambda_{1} = \\mu + \\sigma (1 - g_{1})/\\kappa\\) \\(\\lambda_{2} = \\sigma(g_{1} - g_{2})/\\kappa\\) \\(\\tau_{3} = (-g_{1} + 3g_{2} - 2g_{3}) / (g_{1} - g_{2})\\) \\(\\tau_{4} = (-g_{1} + 6g_{2} - 10g_{3} + 5g_{4}) / (g_{1} - g_{2})\\) In the expression above, \\(g_{r}\\) is defined as follows: \\[ g_{r} = \\begin{dcases} \\frac{r\\Gamma (1 + \\kappa )\\Gamma (r / h)}{h^{1 + \\kappa }\\Gamma (1 + \\kappa + r/h)} &h > 0 \\\\[5pt] \\frac{r\\Gamma (1 + \\kappa ) \\Gamma (-\\kappa - r/h)}{(-h)^{1 + \\kappa }\\Gamma (1 - r/h)} &h < 0 \\end{dcases} \\] There is no closed-form solution for the parameters in terms of the L-moments. However, \\(\\tau_{3}\\) and \\(\\tau_{4}\\) can be computed in terms of \\(\\kappa\\) and \\(h\\) using Newton-Raphson iteration.","title":"Kappa (KAP) Distribution"},{"location":"probability-distributions/#sources","text":"Hosking, J.R.M. & Wallis, J.R., 1997. Regional frequency analysis: an aproach based on L-Moments. Cambridge University Press, New York, USA. \u21a9 \u21a9 \u21a9 Hosking, J.R.M., Wallis, J.R., & Wood, E.F., 1985. Estimation of the generalized extreme-value distribution by the method of probability-weighted moments. Technometrics, 27, 251-61. \u21a9","title":"Sources"},{"location":"r-installation-instructions/","text":"Installing the ffaframework Package Open a shell and navigate to the directory where you would like to install the package. Clone the Github repository by running the following command in a shell: git clone https://github.com/rileywheadon/ffa-package.git Open a command prompt with R . Install the devtools package: install.packages(\"devtools\") library(devtools) Run the following command, changing ~/path/to/ffa-package as needed. devtools::install(\"~/path/to/ffa-package\") Exit the command prompt with q() . The installation is complete.","title":"Installation instructions"},{"location":"r-installation-instructions/#installing-the-ffaframework-package","text":"Open a shell and navigate to the directory where you would like to install the package. Clone the Github repository by running the following command in a shell: git clone https://github.com/rileywheadon/ffa-package.git Open a command prompt with R . Install the devtools package: install.packages(\"devtools\") library(devtools) Run the following command, changing ~/path/to/ffa-package as needed. devtools::install(\"~/path/to/ffa-package\") Exit the command prompt with q() . The installation is complete.","title":"Installing the ffaframework Package"},{"location":"release-notes/","text":"Changelog v0.1.0 July 9th, 2025 The first version of the R Package is here! Implements all features from the MATLAB version (both EDA and FFA). Generate PDF user manual using roxygen2 . Achieve 100% code coverage using the covr library. A full list of features is described in the \"Concepts\" sidebar. v0.0.3 May 22nd, 2025 Return information about non-stationary structure(s) at the end of EDA. Use the /data and /reports directories as defaults in config.yml . Refactor code for batching EDA in stats.R , eda.R . Implement support for PDF reports using a custom \\(\\LaTeX\\) template. v0.0.2 May 21st, 2025 Host documentation at rileywheadon.github.io/ffa-docs . Implement splitting (for change points) in stats.R and eda.R . Run EDA on multiple files by setting csv_files to a list in config.yml . v0.0.1 May 16th, 2025 Execute individual statistical tests using stats.R . Run a suite of unit tests using tests.R . Run the entire EDA pipeline (without data spliting) using eda.R .","title":"Release Notes"},{"location":"release-notes/#changelog","text":"","title":"Changelog"},{"location":"release-notes/#v010","text":"July 9th, 2025 The first version of the R Package is here! Implements all features from the MATLAB version (both EDA and FFA). Generate PDF user manual using roxygen2 . Achieve 100% code coverage using the covr library. A full list of features is described in the \"Concepts\" sidebar.","title":"v0.1.0"},{"location":"release-notes/#v003","text":"May 22nd, 2025 Return information about non-stationary structure(s) at the end of EDA. Use the /data and /reports directories as defaults in config.yml . Refactor code for batching EDA in stats.R , eda.R . Implement support for PDF reports using a custom \\(\\LaTeX\\) template.","title":"v0.0.3"},{"location":"release-notes/#v002","text":"May 21st, 2025 Host documentation at rileywheadon.github.io/ffa-docs . Implement splitting (for change points) in stats.R and eda.R . Run EDA on multiple files by setting csv_files to a list in config.yml .","title":"v0.0.2"},{"location":"release-notes/#v001","text":"May 16th, 2025 Execute individual statistical tests using stats.R . Run a suite of unit tests using tests.R . Run the entire EDA pipeline (without data spliting) using eda.R .","title":"v0.0.1"},{"location":"roadmap/","text":"Roadmap CRAN Package Known Issues : model_diagnostics has argument pp.formula (should be pp_formula ) Bootstrap functions cannot accept custom return periods Command Line Interface Status Function Implemented Covered validate_config x x load_data x x change_points x x trend_detection x x frequency_analysis x ffaframework Known Issues : Cross-field checks are not thorough enough in validate_config Documentation is not up to date with current config.yml API Development Starting soon. Web App Starting soon.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"roadmap/#cran-package","text":"Known Issues : model_diagnostics has argument pp.formula (should be pp_formula ) Bootstrap functions cannot accept custom return periods","title":"CRAN Package"},{"location":"roadmap/#command-line-interface","text":"Status Function Implemented Covered validate_config x x load_data x x change_points x x trend_detection x x frequency_analysis x ffaframework Known Issues : Cross-field checks are not thorough enough in validate_config Documentation is not up to date with current config.yml","title":"Command Line Interface"},{"location":"roadmap/#api-development","text":"Starting soon.","title":"API Development"},{"location":"roadmap/#web-app","text":"Starting soon.","title":"Web App"},{"location":"uncertainty-quantification/","text":"Uncertainty Quantification The FFA framework implements three methods for uncertainty quantification: Sample bootstrap Regula-falsi profile likelihood (RFPL) Regula-falsi generalized profile likelihood (RFGPL) Sample Bootstrap The sample bootstrap is a flexible method for uncertainty quantification that works with all probability models and parameter estimation methods. Let \\(n\\) be the size of the original dataset. Draw \\(N_{\\text{sim}}\\) bootstrap samples of size \\(n\\) from the selected probability distribution. Fit a probability distribution to each bootstrap sample using the same model selection method and parameter estimation method that was used to generate the original distribution. Compute the quantiles for each of the bootstrapped distributions. Generate confidence intervals using the mean and variance of the bootstrapped quantiles. Regula-Falsi Profile Likelihood (RFPL) Consider a statistical model with parameters \\((\\theta, \\psi_{1}, \\dots, \\psi_{n})\\) . The Profile Likelihood for the scalar parameter \\(\\theta\\) and vector of nuisance parameters \\(\\psi\\) is defined as: \\[ \\ell_{p}(\\theta) = \\max_{\\psi } \\ell(\\theta , \\psi) \\] Let \\(\\hat{\\theta}\\) be MLE of \\(\\theta\\) . To find a confidence interval with significance \\(1-\\alpha\\) , we find the two solutions to the following equation (where \\(\\chi_{1;1-\\alpha}^2\\) is the \\(1-\\alpha\\) quantile of the Chi-squared distribution ): \\[ 2[\\ell_{p}(\\hat{\\theta }) - \\ell_{p}(\\theta )] = \\chi_{1;1-\\alpha }^2 \\] This is equivalent to finding the two points \\(\\theta_{L} < \\hat{\\theta} < \\theta_{U}\\) such that the profile log-likelihood has dropped by \\(\\chi _{1;1-\\alpha }^2 / 2\\) . To find \\(\\theta_{L}\\) and \\(\\theta_{U}\\) we find the roots of \\(f(\\theta)\\) using a secant-based algorithm. \\[ f(\\theta) = \\ell_{p}(\\theta) - \\left[\\ell_{p}(\\hat{\\theta}) - \\frac{\\chi_{1;1-\\alpha }^2}{2}\\right] \\] In the FFA framework, we compute the profile likelihood of each quantile \\(y\\) by reparameterizing the location parameter \\(\\mu\\) . Let \\(q(p, \\mu, \\psi)\\) be a function that takes an exceedance probability \\(p\\) , location parameter \\(\\mu\\) and nuisance parameters \\(\\psi\\) and returns a quantile \\(y\\) . All quantile functions[^1] satisfy: \\[ y = q(p, \\mu, \\psi) = \\mu + q(p, 0, \\psi) \\] Therefore, we can define \\(\\mu\\) as a function of \\((p, y, \\psi)\\) as shown below: \\[ \\mu = y - q(p, 0, \\psi) \\] We use this relationship to find the profile likelihood \\(\\ell_{p}(y)\\) by evaluating \\(\\mu(p, y, \\psi)\\) and substituting it into the log-likelihood functions listed here . Handling the Weibull Distribution Due to support issues, we use a different reparameterization for the Weibull distribution: \\[ \\begin{aligned} &y = (\\mu_{0} + \\mu_{1}t) + (\\sigma_{0} + \\sigma_{1}t)(-\\log (1 - p))^{1/\\kappa} \\\\[5pt] \\Rightarrow\\,&(\\sigma_{0} + \\sigma_{1}t) = \\frac{y - (\\mu_{0} + \\mu_{1}t)}{(-\\log (1 - p))^{1/\\kappa}} \\\\[5pt] \\Rightarrow\\,&\\sigma_{0} = \\frac{y - (\\mu_{0} + \\mu_{1}t)}{(-\\log (1 - p))^{1 / \\kappa }} - \\sigma_{1}t \\end{aligned} \\] The derivation above uses the WEI110 model, although the WEI100 and WEI reparameterizations can be obtained easily by setting \\(\\sigma_{1} = 0\\) and \\(\\sigma_{1} = \\mu_{1} = 0\\) respectively. After solving for \\(\\sigma_{0}\\) in terms of the other parameters, we can use the standard log-likelihood function. Initialization Algorithm Before we can find the roots of \\(f\\) , we need to identify initial values for the regula-falsi algorithm: Let \\(a_{0}\\) be a number such that \\(a_{0} < y\\) and \\(f(a_{0}) < 0\\) . Let \\(b_{0}\\) be a number such that \\(b_{0} > y\\) and \\(f(b_{0}) < 0\\) . To find \\(a_{0}\\) , start by computing \\(f(a^{*})\\) for \\(a^{*} = 0.95y\\) . If \\(f(a^{*}) < 0\\) , then assign \\(a_{0} = a^{*}\\) . Otherwise, update \\(a^{*}\\) to \\(0.95a^{*}\\) until \\(f(a^{*}) < 0\\) . To find \\(b_{0}\\) , we use a similar process. However, instead of iteratively revising \\(b^{*}\\) down, we revise it up to \\(1.05b^{*}\\) . Iteration Algorithm At iteration \\(i\\) , compute the following: \\[ c_{i} = \\frac{a_{i-1}f(b_{i-1}) - b_{i-1}f(a_{i-1})}{f(b_{i-1}) - f(a_{i-1})} \\] Evaluate \\(\\ell_{p}(c_{i})\\) by maximizing over the nuisance parameters \\(\\psi\\) , then find \\(f(c_{i})\\) . If \\(|f(c_{i})| < \\epsilon\\) (where \\(\\epsilon\\) is small), then stop. \\(c_{i}\\) is the confidence interval bound. Otherwise, assign \\(a_{i} = c_{i}\\) if \\(f(c_{i}) < 0\\) and \\(b_{i} = c_{i}\\) if \\(f(c_{i}) > 0\\) and continue to iteration \\(i + 1\\) . Regula-Falsi Generalized Profile Likelihood (RFGPL) The regula-falsi generalized profile likelihood (RFGPL) method performs the regula-falsi algorithm shown above on the GEV distributions with a \\(\\text{Beta}(p, q)\\) prior for the shape parameter \\(\\kappa\\) . For more information about generalized parameter estimation, see here . Handling Non-Stationarity If the selected probability distribution is non-stationary, the quantiles (and hence confidence intervals) for the bootstrapped distributions change in time. See here for a more detailed discussion of this idea. By default, the FFA framework anchors uncertainty analysis at the last year of the dataset. However, model assessment requires confidence intervals for every year in the dataset. Note : The sample bootstrap algorithm is the fastest algorithm for computing confidence intervals on all years in a dataset because the probabilities used to generate the bootstrapped samples can be reused. The RFPL and RFGPL algorithms are far slower, since they must be run separately at each timestamp.","title":"Uncertainty Quantification"},{"location":"uncertainty-quantification/#uncertainty-quantification","text":"The FFA framework implements three methods for uncertainty quantification: Sample bootstrap Regula-falsi profile likelihood (RFPL) Regula-falsi generalized profile likelihood (RFGPL)","title":"Uncertainty Quantification"},{"location":"uncertainty-quantification/#sample-bootstrap","text":"The sample bootstrap is a flexible method for uncertainty quantification that works with all probability models and parameter estimation methods. Let \\(n\\) be the size of the original dataset. Draw \\(N_{\\text{sim}}\\) bootstrap samples of size \\(n\\) from the selected probability distribution. Fit a probability distribution to each bootstrap sample using the same model selection method and parameter estimation method that was used to generate the original distribution. Compute the quantiles for each of the bootstrapped distributions. Generate confidence intervals using the mean and variance of the bootstrapped quantiles.","title":"Sample Bootstrap"},{"location":"uncertainty-quantification/#regula-falsi-profile-likelihood-rfpl","text":"Consider a statistical model with parameters \\((\\theta, \\psi_{1}, \\dots, \\psi_{n})\\) . The Profile Likelihood for the scalar parameter \\(\\theta\\) and vector of nuisance parameters \\(\\psi\\) is defined as: \\[ \\ell_{p}(\\theta) = \\max_{\\psi } \\ell(\\theta , \\psi) \\] Let \\(\\hat{\\theta}\\) be MLE of \\(\\theta\\) . To find a confidence interval with significance \\(1-\\alpha\\) , we find the two solutions to the following equation (where \\(\\chi_{1;1-\\alpha}^2\\) is the \\(1-\\alpha\\) quantile of the Chi-squared distribution ): \\[ 2[\\ell_{p}(\\hat{\\theta }) - \\ell_{p}(\\theta )] = \\chi_{1;1-\\alpha }^2 \\] This is equivalent to finding the two points \\(\\theta_{L} < \\hat{\\theta} < \\theta_{U}\\) such that the profile log-likelihood has dropped by \\(\\chi _{1;1-\\alpha }^2 / 2\\) . To find \\(\\theta_{L}\\) and \\(\\theta_{U}\\) we find the roots of \\(f(\\theta)\\) using a secant-based algorithm. \\[ f(\\theta) = \\ell_{p}(\\theta) - \\left[\\ell_{p}(\\hat{\\theta}) - \\frac{\\chi_{1;1-\\alpha }^2}{2}\\right] \\] In the FFA framework, we compute the profile likelihood of each quantile \\(y\\) by reparameterizing the location parameter \\(\\mu\\) . Let \\(q(p, \\mu, \\psi)\\) be a function that takes an exceedance probability \\(p\\) , location parameter \\(\\mu\\) and nuisance parameters \\(\\psi\\) and returns a quantile \\(y\\) . All quantile functions[^1] satisfy: \\[ y = q(p, \\mu, \\psi) = \\mu + q(p, 0, \\psi) \\] Therefore, we can define \\(\\mu\\) as a function of \\((p, y, \\psi)\\) as shown below: \\[ \\mu = y - q(p, 0, \\psi) \\] We use this relationship to find the profile likelihood \\(\\ell_{p}(y)\\) by evaluating \\(\\mu(p, y, \\psi)\\) and substituting it into the log-likelihood functions listed here .","title":"Regula-Falsi Profile Likelihood (RFPL)"},{"location":"uncertainty-quantification/#handling-the-weibull-distribution","text":"Due to support issues, we use a different reparameterization for the Weibull distribution: \\[ \\begin{aligned} &y = (\\mu_{0} + \\mu_{1}t) + (\\sigma_{0} + \\sigma_{1}t)(-\\log (1 - p))^{1/\\kappa} \\\\[5pt] \\Rightarrow\\,&(\\sigma_{0} + \\sigma_{1}t) = \\frac{y - (\\mu_{0} + \\mu_{1}t)}{(-\\log (1 - p))^{1/\\kappa}} \\\\[5pt] \\Rightarrow\\,&\\sigma_{0} = \\frac{y - (\\mu_{0} + \\mu_{1}t)}{(-\\log (1 - p))^{1 / \\kappa }} - \\sigma_{1}t \\end{aligned} \\] The derivation above uses the WEI110 model, although the WEI100 and WEI reparameterizations can be obtained easily by setting \\(\\sigma_{1} = 0\\) and \\(\\sigma_{1} = \\mu_{1} = 0\\) respectively. After solving for \\(\\sigma_{0}\\) in terms of the other parameters, we can use the standard log-likelihood function.","title":"Handling the Weibull Distribution"},{"location":"uncertainty-quantification/#initialization-algorithm","text":"Before we can find the roots of \\(f\\) , we need to identify initial values for the regula-falsi algorithm: Let \\(a_{0}\\) be a number such that \\(a_{0} < y\\) and \\(f(a_{0}) < 0\\) . Let \\(b_{0}\\) be a number such that \\(b_{0} > y\\) and \\(f(b_{0}) < 0\\) . To find \\(a_{0}\\) , start by computing \\(f(a^{*})\\) for \\(a^{*} = 0.95y\\) . If \\(f(a^{*}) < 0\\) , then assign \\(a_{0} = a^{*}\\) . Otherwise, update \\(a^{*}\\) to \\(0.95a^{*}\\) until \\(f(a^{*}) < 0\\) . To find \\(b_{0}\\) , we use a similar process. However, instead of iteratively revising \\(b^{*}\\) down, we revise it up to \\(1.05b^{*}\\) .","title":"Initialization Algorithm"},{"location":"uncertainty-quantification/#iteration-algorithm","text":"At iteration \\(i\\) , compute the following: \\[ c_{i} = \\frac{a_{i-1}f(b_{i-1}) - b_{i-1}f(a_{i-1})}{f(b_{i-1}) - f(a_{i-1})} \\] Evaluate \\(\\ell_{p}(c_{i})\\) by maximizing over the nuisance parameters \\(\\psi\\) , then find \\(f(c_{i})\\) . If \\(|f(c_{i})| < \\epsilon\\) (where \\(\\epsilon\\) is small), then stop. \\(c_{i}\\) is the confidence interval bound. Otherwise, assign \\(a_{i} = c_{i}\\) if \\(f(c_{i}) < 0\\) and \\(b_{i} = c_{i}\\) if \\(f(c_{i}) > 0\\) and continue to iteration \\(i + 1\\) .","title":"Iteration Algorithm"},{"location":"uncertainty-quantification/#regula-falsi-generalized-profile-likelihood-rfgpl","text":"The regula-falsi generalized profile likelihood (RFGPL) method performs the regula-falsi algorithm shown above on the GEV distributions with a \\(\\text{Beta}(p, q)\\) prior for the shape parameter \\(\\kappa\\) . For more information about generalized parameter estimation, see here .","title":"Regula-Falsi Generalized Profile Likelihood (RFGPL)"},{"location":"uncertainty-quantification/#handling-non-stationarity","text":"If the selected probability distribution is non-stationary, the quantiles (and hence confidence intervals) for the bootstrapped distributions change in time. See here for a more detailed discussion of this idea. By default, the FFA framework anchors uncertainty analysis at the last year of the dataset. However, model assessment requires confidence intervals for every year in the dataset. Note : The sample bootstrap algorithm is the fastest algorithm for computing confidence intervals on all years in a dataset because the probabilities used to generate the bootstrapped samples can be reused. The RFPL and RFGPL algorithms are far slower, since they must be run separately at each timestamp.","title":"Handling Non-Stationarity"}]}